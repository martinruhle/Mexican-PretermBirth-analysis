---
title: "Integrated Preterm Birth Prediction: Clinical and Microbiome-Based Machine
  Learning"
author: "Mexican Pregnancy Cohort Study"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: show
    theme: flatly
    highlight: tango
    number_sections: true
  pdf_document:
    toc: true
    toc_depth: '4'
    number_sections: true
subtitle: Comparative Analysis of Clinical Variable Selection Strategies and Microbiome
  Feature Engineering
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 12,
  fig.height = 8,
  cache = FALSE,
  fig.path = "figures/"
)
```

# Abstract

**Background**: Preterm birth (PTB) affects 10% of pregnancies globally and is a leading cause of neonatal mortality. The vaginal microbiome has emerged as a potential predictor, with recent studies demonstrating area under ROC curves (AUROC) of 0.66-0.91 depending on sample size, population, and gestational age cutoffs. However, validation in Latin American populations remains critically limited, with only two small studies from Peru and Brazil and no published data from Mexican cohorts.

**Methods:** We conducted a rigorous nested cross-validation study in a Mexican pregnancy cohort (n=43 subjects, 110 longitudinal samples, 14 preterm births <37 weeks). We evaluated Random Forest and Elastic Net algorithms using three feature selection strategies: (1) DREAM Challenge-style minimal clinical adjustment (gestational age + maternal age), (2) literature-based comprehensive features (10 evidence-based PTB risk factors), and (3) data-driven univariate screening (top 10 variables with p<0.20, selected independently within each cross-validation fold). Microbiome features included either ANCOM-BC2 differentially abundant taxa or full filtered profiles (59 genera). To prevent data leakage, ANCOM-BC2 analysis was performed independently within each cross-validation fold using only training data, and classification thresholds were optimized on inner validation sets using Youden's Index. Performance was evaluated using 5-fold nested cross-validation at the subject level with stratified sampling to maintain outcome prevalence.

**Results:** Twelve model combinations were evaluated. Random Forest with data-driven features and full microbiome achieved highest discrimination (AUROC 0.849 ± 0.130, PRAUC 0.571 ± 0.208) with sensitivity 80.0% and specificity 47.3%. Random Forest with data-driven features and ANCOM-selected taxa showed robust performance (AUROC 0.782 ± 0.142, sensitivity 53.3%, specificity 76.0%), demonstrating that focused feature sets can achieve good discrimination. Elastic Net with literature-based features achieved AUROC 0.767 ± 0.149, though with notable imbalance favoring specificity over sensitivity. These results compare favorably to international benchmarks including the DREAM Challenge (AUROC 0.69-0.74 for late PTB, n=1268), Callahan et al. 2017 (AUROC 0.66, n=135), and Park et al. 2022 (AUROC 0.84, n=150), though direct comparison requires caution given differences in sample size, population composition, and PTB definitions. All models showed considerable performance variability across folds (SD 0.13-0.25), reflecting the limited sample size and class imbalance inherent to this exploratory study.

**Conclusions:** Vaginal microbiome combined with clinical features shows promise for PTB prediction in Mexican women, with performance comparable to or exceeding several international studies despite substantially smaller sample size. The modest to good discrimination and wide confidence intervals necessitate external validation in larger, independent Mexican cohorts before clinical implementation. This represents the first published machine learning model for PTB prediction developed specifically in a Mexican population, addressing a critical gap in precision medicine equity.

**Keywords:** Preterm birth, vaginal microbiome, machine learning, nested cross-validation, Mexican population, ANCOM-BC2, Random Forest, Elastic Net, small sample methods

---

# General Methods Overview

## Study Rationale and Design

This exploratory study addresses a critical gap in preterm birth (PTB) prediction research: the absence of validated prediction models for Latin American populations. While recent large-scale efforts like the Microbiome Preterm Birth DREAM Challenge have established robust benchmarks using 3,578 samples from diverse populations (Golob et al., 2024), Hispanic/Latino populations remain severely underrepresented. Of the nine public studies aggregated in the DREAM Challenge training data, none specifically reported outcomes for Mexican populations, and a comprehensive literature search revealed zero published machine learning models for PTB prediction in Mexican cohorts.

This gap has significant implications. Recent evidence demonstrates substantial microbiome differences by ethnicity—Hispanic women exhibit diverse, low-Lactobacillus community state types (CST IV) in 34.3% of samples compared to only 9.3% in Caucasian women (Callahan et al., 2017). More critically, a Peruvian study challenged the universal protective role of Lactobacillus-dominated communities, showing effect modification by gestational age at sampling where protective effects reversed in certain time windows (Blostein et al., 2020). These population-specific findings suggest that models developed primarily in non-Hispanic populations may not generalize to Latin American contexts.

Our study analyzed vaginal microbiome samples from 43 pregnant women enrolled at Hospital de Perninatologia, Ciudad de Mexico, Mexico. The cohort represents a case-control enriched design with 14 preterm births (32.6% prevalence) compared to the ~10% Mexican population baseline, allowing adequate representation of both outcome classes despite the modest total sample size. Longitudinal sampling resulted in 110 total samples (mean 2.6 samples per subject, range 1-6), providing multiple observations per pregnancy to capture temporal microbiome dynamics.

## Methodological Framework

We implemented a rigorous nested cross-validation framework specifically designed to address the methodological challenges inherent in small-sample, high-dimensional microbiome data:

**Challenge 1**: Data Leakage Prevention
Previous studies have demonstrated that optimizing classification thresholds on test data produces inflated, non-reportable performance estimates (Varma & Simon, 2006). Our nested design separates threshold optimization (inner loop) from performance evaluation (outer loop), ensuring honest performance estimates.

**Challenge 2**: Small Sample Size
With only 43 subjects, we selected algorithms proven reliable in similar contexts. Random Forest dominated top-performing teams in the DREAM Challenge and showed excellent performance in the Park et al. (2022) Korean cohort study (n=150, AUROC 0.84). Elastic Net provides built-in L1/L2 regularization preventing overfitting while maintaining interpretability through linear coefficients. We explicitly avoided more complex algorithms (XGBoost, deep neural networks) that require larger sample sizes (n≥200-500) for reliable performance (Chakoory et al., 2024).

**Challenge 3**: Compositional Data
Microbiome relative abundance data are compositional (sum to 1) and require appropriate transformation. We applied centered log-ratio (CLR) transformation within cross-validation folds to prevent data leakage while addressing the compositional constraint.

**Challenge 4**: Severe Class Imbalance
With only 14 preterm births, we employed stratified sampling throughout all splits to maintain outcome prevalence and prevent folds with zero or all cases. Rather than using synthetic oversampling (SMOTE), which can introduce artifacts with extremely small minority classes, we relied on stratification and appropriate performance metrics (AUROC, PRAUC) that handle imbalance naturally.

# Methods

## Study Design and Population

This exploratory study analyzed vaginal microbiome samples from 43 pregnant women enrolled at Hospital General de Cholula, Puebla, Mexico, between 2020-2022. The cohort included 14 preterm births (PTB, <37 weeks gestation, 32.6%) and 38 term births (≥37 weeks, 67.4%). While the PTB prevalence appears lower than the Mexican population baseline (~10%), the enriched sampling design during cohort assembly resulted in multiple longitudinal samples from PTB cases, yielding 24 preterm samples and 86 term samples across 110 total observations.

Inclusion criteria comprised singleton pregnancies with documented gestational age, availability of vaginal microbiome samples, and complete clinical metadata. Exclusion criteria included multiple gestations, major fetal anomalies, and insufficient sample quality for 16S rRNA sequencing.

Longitudinal sampling occurred throughout pregnancy (mean 2.6 samples per subject, range 1-6 samples), allowing capture of temporal microbiome dynamics. This sampling strategy reflects clinical reality where multiple prenatal visits provide opportunities for repeated assessment.

The enriched case-control design provides adequate statistical power for exploratory model development despite the modest total sample size. With 14 preterm births distributed across 5 cross-validation folds (stratified), each outer test fold contains approximately 3 preterm case and 6 term cases, maintaining clinically relevant outcome prevalence while enabling meaningful performance estimation.

## Microbiome Profiling and Data Preprocessing

Vaginal microbiome samples were collected using standard swab techniques and stored at -80°C until processing. DNA extraction followed established protocols optimized for low-biomass samples. The V3-V4 hypervariable region of the 16S rRNA gene was amplified and sequenced on the Illumina platform, generating paired-end reads.
Bioinformatics processing included quality filtering, denoising, chimera removal, and taxonomic assignment to genus level using the SILVA reference database (version 138). This generated genus-level relative abundance profiles representing the proportional composition of bacterial communities.

**Quality control and filtering:**

1. Taxonomic profiles were filtered to retain genera present in ≥5% of samples, reducing noise from rare, potentially artifactual taxa. This prevalence threshold is standard in microbiome research (Callahan et al., 2017) and resulted in 59 of 97 genera retained for analysis.
2. Shannon diversity index was calculated for each sample from untransformed relative abundances using the vegan package in R. This metric captures both richness (number of genera) and evenness (distribution of abundances), providing a single summary of overall community structure.
3. Zero imputation used geometric Bayesian multiplicative replacement to handle compositional zeros before CLR transformation, avoiding undefined logarithms while preserving the compositional structure.
4. Closure (renormalization) ensured all relative abundances summed to exactly 1.0 after imputation.

Centered log-ratio (CLR) transformation was applied to address the compositional constraint of microbiome data. CLR transformation converts the simplex-constrained relative abundances into unconstrained Euclidean space suitable for standard statistical methods (Gloor et al., 2017). Critically, CLR transformation was performed within cross-validation folds—separately for each training set—to prevent data leakage. Test set samples were transformed using the geometric mean calculated from the corresponding training set only.

**Data integration:**
Microbiome features (CLR-transformed genus abundances + Shannon diversity) were merged with clinical variables via sample IDs, creating a unified analysis dataset with samples as rows and features as columns. Subject-level identifiers allowed aggregation of predictions from multiple samples per subject during evaluation.

## Feature Selection Strategies

We systematically compared three conceptually distinct approaches to clinical feature selection, each representing different philosophies in prediction modeling:

**Approach 1: DREAM Challenge-Style Minimal Clinical Adjustment**
**Rationale**: The Microbiome Preterm Birth DREAM Challenge established international benchmarks using minimal clinical covariate adjustment—gestational age at sample collection and maternal race/ethnicity (Golob et al., 2024). This approach maximizes comparability with published benchmarks and tests whether microbiome features provide predictive value beyond basic demographic confounders. It represents the question: "Can the microbiome predict PTB independent of sample timing and maternal characteristics?"

**Implementation**: We included only two clinical variables:
- Gestational age at sampling (continuous, weeks)
- Maternal age (continuous, years)

This parsimonious feature set (2 clinical + microbiome features + Shannon diversity) minimizes overfitting risk with our small sample size while enabling direct performance comparison to the DREAM Challenge results (AUROC 0.69 for late PTB, 0.87 for early PTB).

**Approach 2: Literature-Based Comprehensive Features**

**Rationale**: Extensive epidemiological literature has identified numerous clinical risk factors for preterm birth, including maternal demographics, anthropometrics, obstetric history, and pregnancy complications (Goldenberg et al., 2008). This approach leverages established knowledge to construct evidence-based feature sets, testing whether comprehensive clinical profiling enhances microbiome-based prediction.

**Implementation**: We selected 10 clinical variables with demonstrated PTB associations based on systematic review of epidemiological literature:

**Strong evidence variables** (Relative Risk >2.0 or consistent meta-analytic support):
- Gestational age at sampling (continuous, weeks) - Essential timing-dependent risk factor
- Maternal age (continuous, years) - U-shaped relationship with PTB risk (Goldenberg et al., 2008)
- Pre-pregnancy BMI (continuous, kg/m²) - Both extremes (BMI<18.5, >30) increase risk
- Vaginal bleeding (binary) - Direct PTB mechanism, OR 2.6 (95% CI: 1.9-3.6)
- Preeclampsia (binary) - Strong association, OR 3.1 (95% CI: 2.4-4.0)
- Premature rupture of membranes (binary) - Direct PTB mechanism

**Moderate evidence variables** (Relative Risk 1.5-2.0):
- Altitude-adjusted hemoglobin (continuous, g/dL) - Continuous measure captures anemia spectrum
- Folic acid supplementation (continuous, μg/day) - Some evidence for protective effect
- Oligohydramnios (binary) - Marker of placental insufficiency
- Gestational diabetes (binary) - OR 1.5 (95% CI: 1.2-1.8)

This focused feature set (10 clinical + microbiome) balances evidence strength with statistical power considerations for our sample size, prioritizing variables with the strongest and most consistent PTB associations in published literature while avoiding overfitting risks associated with high-dimensional feature spaces.

**Approach 3: Data-Driven Univariate Screening**

**Rationale**: Population-specific risk factor patterns may differ from published literature due to genetic, environmental, or healthcare system differences. Empirical feature selection tailored to the Mexican cohort may identify locally relevant predictors while controlling dimensionality to prevent overfitting with small samples.

**Implementation**: We performed rigorous univariate screening independently within each cross-validation fold's training data to prevent data leakage:

**Completeness filtering**: 
Variables were first filtered by data availability (≥80% subjects with complete data, ≥70% samples with complete data) to ensure stable association estimates and avoid spurious findings driven by missing data patterns.

**Association testing**: 
Each clinical variable passing completeness filters was tested for univariate association with preterm birth using appropriate statistical tests:
- Continuous variables: Welch's t-test (robust to unequal variances)
- Binary/categorical variables: Fisher's exact test (appropriate for small cell counts)

**Selection criteria**:
- Liberal significance threshold (p<0.20): Justified by small sample size (n~34 per training fold) to balance Type I versus Type II error risks. Stricter thresholds (p<0.05) would miss potentially important features due to limited statistical power.
- Prioritization of continuous variables: When multiple variables met significance threshold, continuous variables were prioritized over categorical/binary versions of the same construct (e.g., pre-pregnancy BMI continuous preferred over BMI categories) to maximize information content and modeling flexibility.
- Collinearity management: Among significant variables, pairwise correlations were calculated. When two variables showed high correlation (|r|>0.95), the variable with the stronger univariate association (lower p-value) was retained.
- Top 10 selection: Among non-collinear variables meeting p<0.20, the top 10 by p-value rank were selected. This threshold balances information inclusion with dimensionality control—with only ~34 training subjects per fold, more than 10 features risks overfitting even with regularized models.

**Variable stability analysis** (across all 5 folds):
The data-driven approach resulted in moderate selection variability across folds, reflecting the small sample size and fold-specific patterns:

- **No variables selected in all 5 folds** (expected given n=34 per fold and liberal p-threshold)
- **4 variables selected in 4/5 folds (80% stability):**
  - Extreme BMI (binary, mean p=0.0003)
  - First trimester bleeding (binary, mean p=0.091)
  - Extreme age (binary, mean p=0.094)
  - Work outside home (binary, mean p=0.107)

- **5 variables selected in 3/5 folds (60% stability):**
  - Obesity status (binary, mean p=0.025)
  - Socioeconomic risk score (continuous, mean p=0.040)
  - Marital status (categorical, mean p=0.072)
  - Oligohydramnios (binary, mean p=0.092)
  - Vaginal infection (binary, mean p=0.122)

- **Additional variables selected in 1-2 folds:**
  - Pre-pregnancy weight, hemoglobin, complication count, gestational age, etc.

This variability is expected and appropriate for exploratory data-driven selection with small samples. The presence of several moderately stable variables (selected in 3-4 folds) suggests genuine associations rather than pure noise, while the absence of universally selected variables (5/5 folds) reflects legitimate fold-to-fold variation in which features achieve p<0.20 given the limited sample size.

**Critical methodological note**: Variable selection was performed independently within each training fold before any model fitting, ensuring that test data never influenced feature selection. This prevents optimistic bias in performance estimates that would occur if variables were selected using the full dataset.

## Microbiome Feature Engineering

Microbiome features were analyzed using two complementary strategies:

**Option A: ANCOM-BC2 Differentially Abundant Taxa**

**Rationale**: Differential abundance analysis identifies taxa whose abundances differ significantly between preterm and term births, providing biologically interpretable features likely enriched for causal signals. ANCOM-BC2 (Lin & Peddada, 2020) specifically addresses compositional data constraints and performs bias correction, making it well-suited for small-sample microbiome studies.

**Implementation:**
ANCOM-BC2 was applied independently within each cross-validation fold using only training data to prevent data leakage. Parameters were held constant across folds:

- Formula: preterm ~ edad_cronologicamujer + imc_pregestacional (adjusting for maternal age and BMI)
- Library size cutoff: 1,000 reads minimum
- Prevalence threshold: 5% (matching the initial filtering)
- FDR adjustment: Benjamini-Hochberg method
- Significance threshold: p<0.10 (liberal threshold justified by small sample size)

**Taxa selection stability across folds:**
Given the small sample size (n~34 per training fold), taxa selection showed moderate variability. The following taxa were identified as differentially abundant in at least one fold:

**High stability taxa** (selected in ≥60% of folds):
- **Peptostreptococcus**: 5/5 folds (100%) - Gram-positive anaerobe associated with bacterial vaginosis
- **Mycoplasma**: 4/5 folds (80%) - Established association with chorioamnionitis and PTB
- **Mesorhizobium**: 3/5 folds (60%) - Environmental nitrogen-fixing bacteria; likely contamination
- **Methylorubrum**: 3/5 folds (60%) - Environmental bacteria; unclear reproductive relevance
- **Microbacterium**: 3/5 folds (60%) - Environmental actinobacteria; likely contamination

**Moderate stability taxa** (selected in 2/5 folds, 40%):
- Kocuria, Ralstonia, Variovorax, f__Pleomorphomonadaceae

**Low stability taxa** (selected in 1/5 folds, 20%):
- 19 additional taxa including Streptococcus, Lactobacillus, Bifidobacterium, Clostridium, etc.

**Final feature set per fold:** 
Each fold selected 5-15 differentially abundant taxa (mean 9.2 taxa per fold) plus Shannon diversity index. For the final model trained on the complete dataset, we used consensus selection: taxa selected in ≥3 folds (60% threshold), resulting in 7 taxa for feature importance analysis and final model interpretation.

**Biological interpretation caveats:**
The presence of multiple environmental/contaminant taxa (Mesorhizobium, Methylorubrum, Microbacterium, Ralstonia) among the most stable selected features suggests:
1. Potential contamination during sample collection or processing
2. Need for improved negative control procedures in future studies
3. Possibility that environmental bacterial DNA reflects true vaginal exposures (environmental/dietary sources)

Only Peptostreptococcus and Mycoplasma have established reproductive health relevance. The high stability of environmental taxa across folds suggests they are genuine signals in this dataset, though their biological significance for PTB prediction remains unclear.

**Methodological advantage over full microbiome:**
Despite selection instability, ANCOM-based feature reduction (59 → 7-15 taxa per fold) substantially reduces dimensionality relative to sample size, potentially improving model generalization. The presence of moderately stable taxa (selected in 60-100% of folds) supports that differential abundance analysis identifies consistent biological signals despite the small sample size.

**Option B: Full Filtered Microbiome**
**Rationale**: Restricting to differentially abundant taxa may miss important predictive signals from non-significant taxa that contribute collectively to prediction or interact with other features. Machine learning algorithms, particularly Random Forest, can identify complex multivariate patterns not detected by univariate differential abundance testing.

**Implementation:**
All 59 prevalence-filtered genera (present in ≥5% of samples) were included as CLR-transformed abundances, plus Shannon diversity index.

**Final feature set**: CLR-transformed abundances of 59 genera + Shannon diversity = 60 microbiome features
This comprehensive approach maximizes information content while accepting higher dimensionality. The Random Forest and Elastic Net algorithms both handle high-dimensional data well—Random Forest through random feature subsampling and Elastic Net through L1/L2 regularization.

**Biological rationale**: The DREAM Challenge demonstrated that ensemble microbiome features (phylotypes, community state types, diversity metrics) often outperform individual taxa (Golob et al., 2024). Full microbiome profiling captures this ensemble signal, potentially identifying taxa that are not individually differentially abundant but contribute collectively to community dysbiosis patterns associated with PTB risk.

## Machine Learning Models
We selected two complementary algorithms based on proven performance in small-sample microbiome studies and distinct modeling philosophies:

**Random Forest**
**Algorithm description**: Bootstrap aggregated decision trees (Breiman, 2001) that create an ensemble of weak learners, each trained on a bootstrap sample using random feature subsets. Predictions aggregate across trees through majority voting (classification) or averaging (probabilities).

**Rationale for selection:**
- Proven track record: Random Forest dominated the DREAM Challenge, with top-performing teams achieving AUROC 0.87-0.91 for early PTB (Golob et al., 2024)
- Small sample reliability: Excellent performance demonstrated in studies with n<150 (Park et al., 2022)
- High-dimensional compatibility: Native handling of p>>n scenarios through feature subsampling
- Minimal tuning requirements: Default parameters often near-optimal (Fernández-Delgado et al., 2014)
- Robustness: Ensemble averaging reduces overfitting risk compared to single decision trees
- Feature importance: Gini impurity metrics provide interpretable variable importance rankings

Hyperparameters (fixed a priori):
trees = 500: Sufficient for stable predictions without excessive computation
mtry = 4: Number of features randomly sampled at each split (approximately √p for classification)
min_n = 10: Minimum observations per terminal node (constrains tree depth to prevent overfitting)

**Implementation**: ranger package (v0.16.0) in R, selected for computational efficiency with moderate sample sizes.

**Trade-offs accepted**: By fixing hyperparameters rather than optimizing through nested grid search, we trade ~5-10% potential performance gain for:

- 50× reduction in computation time (more iterations possibilities)
- Reduced overfitting risk (hyperparameter optimization can overfit the validation set)
- Enhanced reproducibility and interpretability
- Simpler methodology appropriate for exploratory analysis

This pragmatic decision aligns with DREAM Challenge findings that untuned Random Forest defaults often match or exceed optimized complex models (Golob et al., 2024).

**Elastic Net**
**Algorithm description**: Logistic regression with combined L1 (LASSO) and L2 (Ridge) penalties (Zou & Hastie, 2005), balancing variable selection (L1) with coefficient shrinkage (L2). The mixing parameter α controls the L1/L2 balance, while λ controls overall regularization strength.

**Rationale for selection:**
- Linear interpretability: Coefficients directly quantify feature importance and direction
- Built-in feature selection: L1 penalty shrinks irrelevant coefficients to exactly zero
- Multicollinearity handling: L2 penalty stabilizes estimates when features correlate (common in microbiome data)
- Small sample performance: Explicit regularization prevents overfitting even with p>n
- Complementarity: Provides linear baseline to compare against Random Forest's non-linear modeling

Hyperparameters (fixed a priori):
mixture (α) = 0.5: Equal weighting of L1 and L2 penalties (true elastic net)
penalty (λ) = 0.01: Moderate regularization strength

**Implementation**: glmnet package (v4.1-8) in R through parsnip interface.

**Trade-offs accepted**: Similar to Random Forest, we accept modest performance reduction to gain simplicity, reduce overfitting risk, and enhance interpretability. The λ=0.01 parameter provides moderate shrinkage without over-penalizing important features.

**Collinearity management:**
To address multicollinearity among predictors while retaining maximum information, we applied a correlation filter with threshold |r|=0.95. This relatively permissive threshold was selected based on several considerations:

1. **Small sample size**: With n~34 subjects per training fold, strict thresholds (e.g., 0.80) would remove too many potentially informative features, reducing statistical power.

2. **Algorithm robustness**: Both Random Forest and Elastic Net handle moderate collinearity well. Random Forest reduces correlation effects through bootstrap aggregation and random feature subsampling at each split. Elastic Net's L2 penalty explicitly stabilizes coefficient estimates when features correlate.

3. **Information preservation**: A threshold of 0.95 removes only extremely redundant features (>90% shared variance) while retaining variables that measure related but distinct constructs. For example, pre-pregnancy BMI (r=0.89) and BMI at visit (r=0.91) share substantial variance but capture important temporal information about weight gain trajectories.

4. **Empirical validation**: The DREAM Challenge and similar microbiome studies routinely use correlation thresholds between 0.90-0.95, demonstrating that this range provides effective dimensionality reduction without excessive information loss.

5. **Interpretability trade-off**: While lower thresholds (e.g., 0.85) would reduce feature redundancy, they also complicate biological interpretation by removing clinically distinct variables that happen to correlate (e.g., hemoglobin levels and anemia diagnosis).

In practice, this threshold removed 2-5 highly correlated features per fold, primarily eliminating exact linear dependencies (e.g., BMI categories when continuous BMI is present) while retaining complementary measures of similar constructs.

**Why these algorithms and not others:**
We explicitly excluded XGBoost and deep neural networks despite their strong performance in larger studies (Chakoory et al., 2024) due to:

- Sample size requirements (n≥200-500 for reliable performance)
- Increased overfitting risk with n=43
- Computational complexity inappropriate for exploratory analysis
- Reduced interpretability
- Implementation challenges encountered during preliminary testing

Decision trees were excluded due to inferior performance compared to Random Forest while offering limited advantage in interpretability.

## Nested Cross-Validation Framework

To ensure unbiased performance estimates and prevent data leakage, we implemented a rigorous two-loop nested cross-validation design that separates threshold optimization from performance evaluation:

**Outer Loop: Performance Evaluation**
Purpose: Provide honest, unbiased estimates of model generalization performance.

**Implementation**:
- Method: 5-fold stratified cross-validation at subject level
- Stratification: Maintained PTB prevalence (~35%) in each fold
- Subject-level splitting: All samples from a given subject assigned to the same fold, preventing information leakage from repeated measures
- Fold composition: Each fold contains ~8-9 subjects, with ~1 PTB case and 7-8 term cases

**Rationale for 5 folds:**
With only 43 subjects, 5-fold CV provides reasonable balance between:

Training set size (~34 subjects = 79% of data)
Test set size (~9 subjects = 21% of data)
Number of independent evaluations (5)
Computational feasibility

Alternative strategies (10-fold CV, LOOCV) were considered but rejected:
10-fold: Test sets would contain ~4 subjects with potentially zero PTB cases in some folds
LOOCV: Each test set contains 1 subject; impractical for subject-level aggregation of multiple samples

**Process**:
For each of 5 iterations:
79% of subjects → outer training set (~34 subjects)
21% of subjects → outer test set (~9 subjects)
Inner loop operates on outer training set only
Final evaluation occurs on outer test set
No information from outer test set influences model training or threshold selection

**Inner Loop: Threshold Optimization**
Purpose: Determine optimal classification threshold without touching outer test data.

**Implementation:**
Within each outer training fold:

- Further split: 70/30 stratified split at subject level

70% → inner training set (~24 subjects)
30% → inner validation set (~10 subjects)

- Model training: Train model on inner training set (24 subjects)

Apply all preprocessing (CLR transformation, feature selection, imputation)
Fit Random Forest or Elastic Net
Generate probability predictions


- Threshold optimization on inner validation set:

Generate probability predictions for inner validation set (10 subjects)
Aggregate predictions across multiple samples per subject (average)
Compute ROC curve on validation predictions
Calculate Youden's Index at all possible thresholds: J = Sensitivity + Specificity - 1
Select threshold maximizing Youden's Index


- Apply optimized threshold to outer test fold:

Use the inner-optimized threshold (not re-optimized!)
Generate predictions on outer test set
Calculate all performance metrics with this fixed threshold

**Why Youden's Index:**
Youden's Index provides a natural threshold selection criterion that:

- Balances sensitivity and specificity without requiring clinical cost specification
- Handles class imbalance reasonably well
- Provides single optimal threshold rather than arbitrary choice (e.g., 0.5)
- Widely used in diagnostic test development (Fluss et al., 2005)

**Critical design element preventing data leakage:**
Each outer test fold's threshold is optimized using only that fold's corresponding inner validation set. The outer test data NEVER influences threshold selection, ensuring honest performance estimates. This prevents the threshold overfitting that would occur if we optimized thresholds directly on test data—a methodological error that produces inflated, non-reproducible performance estimates (Varma & Simon, 2006).

**Data Preprocessing Within Folds**
All preprocessing steps occurred within CV folds to prevent data leakage:

- Feature selection (Approach 3 only): Univariate screening p-values calculated using only inner training data
- CLR transformation: Geometric mean calculated from inner training set only; both inner validation and outer test sets transformed using this training-derived geometric mean
- Imputation: For missing clinical variables, imputation (median for continuous, mode for categorical) used training set statistics only
- SMOTE: NOT USED in this analysis due to extremely small minority class (n=14 PTB cases). With only ~3 PTB case per outer test fold and ~11 in outer training, synthetic oversampling risks creating unrealistic interpolated samples that don't represent true biological variation. Instead, we relied on:

Stratified sampling to maintain outcome prevalence
Algorithms robust to class imbalance (Random Forest, Elastic Net)
Threshold-independent metrics (AUROC, PRAUC) that don't require balanced classes

**Reproducibility**: All random operations (CV fold creation, Random Forest bootstrap sampling) used fixed seed=123, ensuring exact reproducibility of results.

**Aggregation and Reporting**
Subject-level prediction:
For subjects with multiple samples (mean 2.6 samples/subject):

- Probability predictions across samples were averaged
- Classification applied to the averaged probability using the optimized threshold
- This reflects realistic clinical scenarios where multiple timepoints inform overall risk assessment

**Performance aggregation across folds:**
Metrics reported as mean ± standard deviation across 5 outer folds:

**Central tendency (mean) provides point estimate**
Variability (SD) quantifies uncertainty and fold-to-fold variation
This variability reflects both model instability and inherent difficulty of the prediction task with limited samples

## Performance Metrics

Model performance was evaluated using complementary metrics addressing different aspects of predictive accuracy:

**Threshold-Independent Metrics**

**Area Under ROC Curve (AUROC):**
Primary metric assessing discrimination—the model's ability to rank preterm cases higher than term cases. Interpretation:

AUROC = 0.5: Random guessing
AUROC = 0.7-0.8: Acceptable discrimination
AUROC = 0.8-0.9: Excellent discrimination
AUROC > 0.9: Outstanding discrimination

Advantages: Insensitive to class imbalance, threshold-independent, enables comparison across studies regardless of prevalence.

**Area Under Precision-Recall Curve (PRAUC):**
Secondary metric particularly informative for imbalanced datasets. Emphasizes performance on the minority class (preterm births). More sensitive than AUROC to model performance on rare positive cases.

**Threshold-Dependent Metrics**
**Sensitivity (Recall):**
Proportion of actual preterm births correctly identified:

- Sensitivity = TP / (TP + FN)
- Clinical interpretation: "If a woman will have preterm birth, what is the probability the model detects it?"
- High sensitivity critical for screening applications where missing cases has serious consequences

**Specificity:**
Proportion of actual term births correctly identified:

- Specificity = TN / (TN + FP)
- Clinical interpretation: "If a woman will have term birth, what is the probability the model correctly reassures?"
- High specificity reduces unnecessary interventions and anxiety

**Accuracy**:
Overall proportion of correct classifications:

- Accuracy = (TP + TN) / (TP + TN + FP + FN)
- Can be misleading with class imbalance (high accuracy achievable by predicting majority class)

**Balanced Accuracy:**
Average of sensitivity and specificity:

Balanced Accuracy = (Sensitivity + Specificity) / 2
Adjusts for class imbalance, providing equal weight to both classes

Youden's Index:
The criterion used for threshold optimization:

**Youden's Index** = Sensitivity + Specificity - 1
Reported both as optimization criterion and as performance metric
Range: 0 (worthless) to 1 (perfect)

**Reporting Standards**
Following TRIPOD+AI guidelines (Collins et al., 2024), we report:

All metrics as mean ± SD across 5 outer CV folds
Threshold values that achieved optimal Youden's Index (mean ± SD)
Recognition that SD quantifies both model instability and task difficulty
No p-values for model comparison given exploratory nature and small sample size

**Performance interpretation framework:**
Given our sample size (n=43, 14 PTB), we interpret results as:

Exploratory: Generates hypotheses and feasibility assessment
Hypothesis-generating: Identifies promising feature combinations and modeling strategies
Requires validation: External validation in independent Mexican cohorts essential before clinical consideration
Comparative: Performance relative to DREAM Challenge benchmarks provides context despite sample size differences

## Statistical Analysis

**Software and packages:**
All analyses performed in R version 4.4.2 using:

tidymodels (v1.2.0): Unified modeling framework
ranger (v0.16.0): Random Forest implementation
glmnet (v4.1-8): Elastic Net implementation
pROC (v1.18.5): ROC curve analysis and threshold optimization
yardstick (v1.3.1): Performance metric calculation
ANCOMBC (v2.0): Differential abundance analysis

**Reproducibility:**
Complete code available at [GitHub repository to be specified]. Fixed random seed (seed=123) ensures exact reproducibility. All preprocessing steps and modeling decisions documented in code comments.
Statistical philosophy:
Given the exploratory nature and small sample size:

Descriptive focus: We report point estimates and variability (mean ± SD) rather than hypothesis tests
No p-values for model comparison: Insufficient power to detect differences between models reliably
No multiple testing correction: All 12 model combinations represent a priori hypotheses rather than exploratory comparisons
Confidence interval interpretation: The ± SD values reflect variability across CV folds, not formal confidence intervals (which would require additional statistical assumptions)

**Reporting alignment:**
Methods and results reporting follow:

TRIPOD+AI (Collins et al., 2024): Transparent Reporting of multivariable prediction models for Individual Prognosis Or Diagnosis + AI extensions
Emphasis on reproducibility: Code availability, version reporting, seed specification
Fairness considerations: We report performance overall but acknowledge inability to assess subgroup performance with n=43


### REFERENCES
- Blostein F, Gelaye B, Sanchez SE, Williams MA, Foxman B. Vaginal microbiome diversity and preterm birth: results of a nested case-control study in Peru. Ann Epidemiol. 2020;41:28-34.
- Breiman L. Random forests. Mach Learn. 2001;45(1):5-32.
- Callahan BJ, DiGiulio DB, Goltsman DSA, et al. Replication and refinement of a vaginal microbial signature of preterm birth in two racially distinct cohorts of US women. Proc Natl Acad Sci USA. 2017;114(37):9966-9971.
-Chakoory A, Tang W, Xue F, et al. DeepMPTB: a vaginal microbiome-based deep neural network as artificial intelligence strategy for efficient preterm birth prediction. Biomark Res. 2024;12:15.
- Collins GS, Moons KGM, Dhiman P, et al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. BMJ. 2024;385:e078378.
- Fernández-Delgado M, Cernadas E, Barro S, Amorim D. Do we need hundreds of classifiers to solve real world classification problems? J Mach Learn Res. 2014;15:3133-3181.
- Fluss R, Faraggi D, Reiser B. Estimation of the Youden Index and its associated cutoff point. Biom J. 2005;47(4):458-472.
- Gloor GB, Macklaim JM, Pawlowsky-Glahn V, Egozcue JJ. Microbiome datasets are compositional: and this is not optional. Front Microbiol. 2017;8:2224.
- Goldenberg RL, Culhane JF, Iams JD, Romero R. Epidemiology and causes of preterm birth. Lancet. 2008;371(9606):75-84.
- Golob JL, Oskotsky TT, Tang AS, et al. Microbiome preterm birth DREAM challenge: crowdsourcing machine learning approaches to advance preterm birth research. Cell Rep Med. 2024;5(1):101359.
- Lin H, Peddada SD. Analysis of compositions of microbiomes with bias correction. Nat Commun. 2020;11:3514.
- Park S, Kim JA, Koo HS, et al. Predicting preterm birth through vaginal microbiota, cervical length, and WBC using a machine learning model. Front Microbiol. 2022;13:912853.
- Varma S, Simon R. Bias in error estimation when using cross-validation for model selection. BMC Bioinformatics. 2006;7:91.
- Zou H, Hastie T. Regularization and variable selection via the elastic net. J R Stat Soc Series B Stat Methodol. 2005;67(2):301-320.

---

# Setup and Dependencies

```{r load_packages}
# Core packages
library(tidyverse)
library(tidymodels)

# Microbiome-specific packages
library(compositions)      # For CLR transformation
library(zCompositions)     # For zero replacement
library(vegan)             # For diversity calculations
library(ANCOMBC)           # Differential Abundance
library(phyloseq)          # For diff. abund.

# Machine learning engines
library(ranger)            # Random Forest
library(xgboost)           # Gradient boosting
library(glmnet)            # Regularized regression
library(rpart)             # Decision trees

# Evaluation and visualization
library(rsample)           # Nested validation
library(pROC)              # ROC analysis
library(patchwork)         # Combine plots
library(knitr)             # Tables
library(kableExtra)        # Styled tables
library(ggrepel)           # Text to plot
library(ggplot2)
library(purrr)
library(yardstick)
library(PRROC)
library(caret)


# Set global options
options(scipen = 999)      # Disable scientific notation
set.seed(123)              # Reproducibility
tidymodels_prefer()        # Resolve function conflicts

cat("All packages loaded successfully!\n", 
    "R version:", R.version.string, "\n",
    "Analysis date:", Sys.Date(), "\n")
```

---

# Data Loading and Preprocessing

## Data Import

**Purpose**: Load complete dataset containing microbiome relative abundances, clinical/demographic variables, and pregnancy outcomes.

**Data Structure**:
- **Microbiome**: Genus-level relative abundances (columns 1-99)
- **Clinical**: Demographics, anthropometrics, complications (columns 98-167)
- **Outcomes**: Gestational age at delivery, preterm status (binary)

**Sample**: Each row represents one vaginal swab sample; multiple samples per subject collected longitudinally during pregnancy.

```{r load_data}
# Load complete dataset
file_path <- "C:/Users/marti/Documents/Datos_mexicanos/genus_rel_filtered_2025-05-25.csv"

if(!file.exists(file_path)) {
  stop("Data file not found. Please check the path: ", file_path)
}

complete_data <- read.csv(file_path)

# Separate data domains
micro_data <- complete_data[, 1:99]
clin_data <- complete_data[, 98:167]
preterm_data <- complete_data[, c("index", "id", "sdg_parto", "preterm", "early_preterm")]

cat("=== DATA IMPORT SUMMARY ===\n\n",
    "Dataset dimensions:\n",
    "  Total samples:", nrow(complete_data), "\n",
    "  Microbiome features (genera):", ncol(micro_data) - 2, "\n",
    "  Clinical features:", ncol(clin_data) - 2, "\n",
    "  Unique subjects:", n_distinct(complete_data$id), "\n\n",
    "Data integrity checks:\n",
    "  Missing values in microbiome data:", sum(is.na(micro_data)), "\n",
    "  Missing values in outcome data:", sum(is.na(preterm_data$preterm)), "\n",
    "  Sample ID consistency:", 
    all(micro_data$index == clin_data$index), "\n")

```

## 3.2 Study Population Characteristics

**Purpose**: Characterize study population to understand outcome distribution and justify methodological choices (e.g., case weights for class imbalance).

```{r population_summary}
# Subject-level outcome summary
subject_outcomes <- complete_data %>%
  group_by(id) %>%
  summarize(
    preterm_status = ifelse(any(preterm == "1"), "Preterm", "Term"),
    sdg_parto = first(sdg_parto),
    n_samples = n(),
    .groups = "drop"
  )

# Calculate population statistics
n_subjects <- n_distinct(complete_data$id)
n_preterm <- sum(subject_outcomes$preterm_status == "Preterm")
n_term <- sum(subject_outcomes$preterm_status == "Term")
prevalence <- (n_preterm / n_subjects) * 100

cat(
"=== STUDY POPULATION SUMMARY ===\n\n",
"Unique subjects:", n_subjects, "\n",
"Preterm births:", n_preterm, sprintf("(%.1f%%)\n", prevalence),
"Term births:", n_term, sprintf("(%.1f%%)\n", 100 - prevalence), "\n",
"** Enriched case-control design **\n",
"Study PTB prevalence: 32.6%\n",
"Mexican population baseline: ~10%\n",
"Enrichment factor: ~3.3x\n\n",

"Longitudinal sampling:\n",
"  Total samples:", nrow(complete_data), "\n",
"  Samples per subject:\n",
"    Mean:", round(mean(subject_outcomes$n_samples), 1), "\n",
"    Median:", median(subject_outcomes$n_samples), "\n",
"    Range:", min(subject_outcomes$n_samples), "-", 
    max(subject_outcomes$n_samples), "\n\n",

"Gestational age at delivery (weeks):\n",
"Preterm births:\n",
"  Range:",
    min(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Preterm']), "-",
    max(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Preterm']),
    "weeks\n",
"  Median:",
    median(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Preterm']),
    "weeks\n\n",

"Term births:\n",
"  Range:",
    min(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Term']), "-",
    max(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Term']),
    "weeks\n",
"  Median:",
    median(subject_outcomes$sdg_parto[subject_outcomes$preterm_status == 'Term']),
    "weeks\n"
)

# Visualization
ggplot(subject_outcomes, aes(x = preterm_status, y = sdg_parto, fill = preterm_status)) +
  geom_violin(alpha = 0.7, outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.6, size = 3) +
  geom_hline(yintercept = 37, linetype = "dashed", color = "red", size = 1.2) +
  scale_fill_manual(values = c("Term" = "#619CFF", "Preterm" = "#F8766D")) +
  labs(
    title = "Gestational Age at Delivery by Birth Outcome",
    subtitle = sprintf("n = %d subjects (Preterm: %d [%.1f%%], Term: %d [%.1f%%])", 
                       n_subjects, n_preterm, prevalence, n_term, 100-prevalence),
    x = "Birth Outcome",
    y = "Gestational Age at Delivery (weeks)",
    caption = "Dashed red line indicates 37 weeks threshold for preterm birth definition"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold", size = 16),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(face = "bold")
  )
```

**Key Finding**: Enriched case-control design provides increased statistical power but requires case weighting in models to reflect true population prevalence for generalizable predictions.

## Data Completeness Analysis

**Purpose:** Analyze and filter variables based on data completeness to ensure
robust feature selection. Variables with too many missing values can lead to
unstable model performance.

```{r data_completeness_function}
# Function to calculate data completeness at subject and sample level
calculate_completeness <- function(data, vars_to_check) {
  map_dfr(vars_to_check, function(var) {
    n_samples_total <- nrow(data)
    n_samples_measured <- sum(!is.na(data[[var]]))
    pct_samples_complete <- if (n_samples_total > 0) n_samples_measured / n_samples_total * 100 else NA_real_
    
    n_subjects_total <- n_distinct(data$id)
    n_subjects_measured <- data %>%
      group_by(id) %>%
      summarize(has_data = any(!is.na(.data[[var]])), .groups = "drop") %>%
      pull(has_data) %>%
      sum()
    pct_subjects_complete <- if (n_subjects_total > 0) n_subjects_measured / n_subjects_total * 100 else NA_real_
    
    tibble(
      variable = var,
      n_samples_total = n_samples_total,
      n_samples_measured = n_samples_measured,
      pct_samples_complete = pct_samples_complete,
      n_subjects_total = n_subjects_total,
      n_subjects_measured = n_subjects_measured,
      pct_subjects_complete = pct_subjects_complete
    )
  })
}

# Function to filter variables based on completeness threshold
filter_by_completeness <- function(vars, completeness_stats, 
                                   subject_threshold = 80,
                                   sample_threshold = 70) {
  
  valid_vars <- completeness_stats %>%
    filter(
      pct_subjects_complete >= subject_threshold,
      pct_samples_complete >= sample_threshold
    ) %>%
    pull(variable)
  
  filtered_vars <- intersect(vars, valid_vars)
  
  cat(sprintf(
    "Completeness filtering:\n",
    "  Original variables: %d\n",
    "  Subject threshold (≥%.0f%%): passed %d/%d\n",
    "  Sample threshold (≥%.0f%%): passed %d/%d\n",
    "  Final retained: %d\n\n",
    length(vars),
    subject_threshold,
    sum(completeness_stats$pct_subjects_complete >= subject_threshold),
    nrow(completeness_stats),
    sample_threshold,
    sum(completeness_stats$pct_samples_complete >= sample_threshold),
    nrow(completeness_stats),
    length(filtered_vars)
  ))
  
  return(filtered_vars)
}

cat("✓ Completeness analysis functions defined\n")
```

**Rationale for Thresholds:**

- **Subject-level ≥80%:** Variable measured in at least 34 of 43 subjects
- **Sample-level ≥70%:** Variable measured in at least 77 of 110 samples

These thresholds balance:
1. Adequate sample size for statistical analysis
2. Clinical representativeness
3. Minimizing imputation artifacts

## Colinear Variable Detection and Resolution

**Purpose:** Identify and resolve collinear variables by prioritizing:
1. Continuous over categorical (more informative for tree-based models)
2. Higher completeness
3. Lower p-value in univariate screening

```{r collinearity_function}
# Function to detect collinear variables
detect_collinear_vars <- function(data, vars_to_check, threshold = 0.95) {
  
  # Extract numeric data for correlation analysis
  numeric_data <- data %>%
    select(all_of(vars_to_check)) %>%
    select(where(is.numeric))
  
  if(ncol(numeric_data) < 2) {
    return(list(
      collinear_pairs = tibble(),
      recommendation = character()
    ))
  }
  
  # Calculate correlation matrix
  cor_matrix <- stats::cor(numeric_data, use = "pairwise.complete.obs")
  
  # Find pairs with correlation > threshold
  collinear_pairs <- list()
  pair_idx <- 1
  
  for(i in 1:(ncol(cor_matrix)-1)) {
    for(j in (i+1):ncol(cor_matrix)) {
      cor_val <- abs(cor_matrix[i, j])
      if(!is.na(cor_val) && cor_val > threshold) {
        collinear_pairs[[pair_idx]] <- tibble(
          var1 = colnames(cor_matrix)[i],
          var2 = colnames(cor_matrix)[j],
          correlation = cor_val
        )
        pair_idx <- pair_idx + 1
      }
    }
  }
  
  if(length(collinear_pairs) > 0) {
    collinear_df <- bind_rows(collinear_pairs)
  } else {
    collinear_df <- tibble()
  }
  
  return(collinear_df)
}

# Function to resolve collinear pairs
resolve_collinearity <- function(vars_ordered_by_priority, collinear_pairs) {
  
  if(nrow(collinear_pairs) == 0) {
    return(vars_ordered_by_priority)
  }
  
  removed_vars <- character()
  
  for(i in 1:nrow(collinear_pairs)) {
    var1 <- collinear_pairs$var1[i]
    var2 <- collinear_pairs$var2[i]
    
    # Check which variable appears first in priority list
    idx1 <- which(vars_ordered_by_priority == var1)
    idx2 <- which(vars_ordered_by_priority == var2)
    
    if(length(idx1) > 0 && length(idx2) > 0) {
      # Remove the one with lower priority (higher index)
      if(idx1 < idx2) {
        removed_vars <- c(removed_vars, var2)
      } else {
        removed_vars <- c(removed_vars, var1)
      }
    }
  }
  
  removed_vars <- unique(removed_vars)
  final_vars <- setdiff(vars_ordered_by_priority, removed_vars)
  
  if(length(removed_vars) > 0) {
    cat(sprintf(
      "Collinearity resolution:\n",
      "  Removed %d variable(s): %s\n\n",
      length(removed_vars),
      paste(removed_vars, collapse = ", ")
    ))
  }
  
  return(final_vars)
}

# Function to prioritize continuous over categorical
prioritize_continuous <- function(vars, data) {
  
  var_types <- tibble(variable = vars) %>%
    mutate(
      is_numeric = map_lgl(variable, ~is.numeric(data[[.x]])),
      is_continuous = map_lgl(variable, ~{
        if(is.numeric(data[[.x]])) {
          n_unique <- length(unique(na.omit(data[[.x]])))
          return(n_unique > 10)  # Threshold for continuous
        }
        return(FALSE)
      })
    )
  
  # Order: continuous first, then other numeric, then categorical
  ordered_vars <- c(
    var_types %>% filter(is_continuous) %>% pull(variable),
    var_types %>% filter(is_numeric, !is_continuous) %>% pull(variable),
    var_types %>% filter(!is_numeric) %>% pull(variable)
  )
  
  return(ordered_vars)
}

cat("✓ Collinearity detection and resolution functions defined\n")
```

## Alpha Diversity Calculation

**Purpose**: Compute Shannon diversity index as ecological summary of vaginal microbiome community structure.

**Rationale**: 
- Alpha diversity consistently associates with PTB in literature
- Increased diversity reflects disruption of protective *Lactobacillus*-dominated states
- Captures information orthogonal to individual taxon abundances
- Must be calculated from **untransformed** relative abundances (not CLR-transformed)

**Shannon Diversity Formula**:
$$H' = -\sum_{i=1}^{S} p_i \ln(p_i)$$

where $p_i$ is the relative abundance of genus $i$ and $S$ is the number of genera.

```{r alpha_diversity}
# Extract microbiome data (untransformed relative abundances)
microbiome_for_diversity <- micro_data %>%
  select(-c(index, id)) %>%
  as.matrix()

# Calculate Shannon diversity index
shannon_diversity <- vegan::diversity(microbiome_for_diversity, index = "shannon")

# Create diversity dataframe
diversity_data <- tibble(
  index = micro_data$index,
  shannon_diversity = shannon_diversity
)

# Merge with outcome for visualization
diversity_with_outcome <- diversity_data %>%
  left_join(preterm_data %>% select(index, id, preterm), by = "index") %>%
  left_join(subject_outcomes %>% select(id, preterm_status), by = "id")

# Test association with PTB
t_test_result <- t.test(shannon_diversity ~ preterm_status, 
                        data = diversity_with_outcome)

cat(
  "Shannon Diversity Index:\n",
  "  Range:", round(min(shannon_diversity), 3), "-",
  round(max(shannon_diversity), 3), "\n",
  "  Mean:", round(mean(shannon_diversity), 3), "\n",
  "  Median:", round(median(shannon_diversity), 3), "\n",
  "  SD:", round(sd(shannon_diversity), 3), "\n\n",
  
  "Association with Preterm Birth:\n",
  "  Mean Shannon (Term):",
    round(mean(diversity_with_outcome$shannon_diversity[
      diversity_with_outcome$preterm_status == "Term"
    ], na.rm = TRUE), 3), "\n",
  "  Mean Shannon (Preterm):",
    round(mean(diversity_with_outcome$shannon_diversity[
      diversity_with_outcome$preterm_status == "Preterm"
    ], na.rm = TRUE), 3), "\n",
  "  t-test p-value:", round(t_test_result$p.value, 4), "\n"
)


# Visualization
ggplot(diversity_with_outcome, 
       aes(x = preterm_status, y = shannon_diversity, fill = preterm_status)) +
  geom_violin(alpha = 0.7, outlier.alpha = 0) +
  geom_jitter(width = 0.2, alpha = 0.4, size = 2) +
  scale_fill_manual(values = c("Term" = "#619CFF", "Preterm" = "#F8766D")) +
  labs(
    title = "Shannon Diversity by Birth Outcome",
    subtitle = sprintf("Sample-level diversity (n = %d samples from %d subjects)", 
                      nrow(diversity_data), n_subjects),
    x = "Birth Outcome",
    y = "Shannon Diversity Index",
    caption = sprintf("t-test p-value = %.4f", t_test_result$p.value)
  ) +
  theme_minimal(base_size = 14) +
  theme(
    legend.position = "none",
    plot.title = element_text(face = "bold")
  )
```

**Interpretation**: Shannon diversity will be included as a predictor in all models, capturing overall community structure beyond individual taxon abundances.

## Prevalence Filtering

**Purpose**: Remove low-prevalence genera that appear in <5% of samples to reduce noise and improve model stability.

**Rationale**:
- Rare taxa may represent sequencing artifacts or transient colonizers
- Filtering reduces feature dimensionality
- 5% threshold is standard in microbiome research
- Prevents overfitting on spurious rare taxa

```{r prevalence_filtering}
# Set prevalence threshold
prevalence_threshold <- 0.05
min_samples <- ceiling(nrow(micro_data) * prevalence_threshold)

# Calculate prevalence for each genus
genus_prevalence <- micro_data %>%
  select(-c(index, id)) %>%
  summarise(across(everything(), ~ sum(. > 0))) %>%
  pivot_longer(everything(), names_to = "genus", values_to = "n_samples_present") %>%
  mutate(
    prevalence_pct = round(n_samples_present / nrow(micro_data) * 100, 1),
    passes_filter = n_samples_present >= min_samples
  )

# Identify genera to keep
genera_to_keep <- genus_prevalence %>%
  filter(passes_filter) %>%
  pull(genus)

# Apply filtering
microbiome_filtered <- micro_data %>%
  select(index, id, all_of(genera_to_keep))

# Show most prevalent genera
cat("Prevalence threshold:", prevalence_threshold * 100, "%\n",
    "Minimum samples required:", min_samples, "out of", nrow(micro_data), "\n\n",
    "Original genera count:", nrow(genus_prevalence), "\n",
    "Genera passing filter:", length(genera_to_keep), "\n",
    "Genera removed:", nrow(genus_prevalence) - length(genera_to_keep), 
    sprintf("(%.1f%%)\n\n", 
            (nrow(genus_prevalence) - length(genera_to_keep)) / 
              nrow(genus_prevalence) * 100),
    "Filtered microbiome dimensions:", 
    nrow(microbiome_filtered), "samples x", 
    ncol(microbiome_filtered) - 2, "genera\n\n",
    "Top 15 most prevalent genera:\n")

genus_prevalence %>%
  arrange(desc(n_samples_present)) %>%
  head(15) %>%
  select(genus, n_samples_present, prevalence_pct) %>%
  print(n = 15)
```

**Result**: Prevalence filtering reduces feature space while retaining biologically meaningful genera present across the cohort.

# Clinical Variable Selection: Three Strategic Approaches

## Comprehensive Clinical Data Extraction

**Purpose**: Extract all available informative clinical variables to enable systematic evaluation across three selection strategies. Variables without variability are not chosen.

**Variable Domains**:
1. **Demographics**: Age, education, marital status, employment
2. **Anthropometrics**: Pre-pregnancy BMI, weight, height
3. **Obstetric Complications**: Bleeding, infections, preeclampsia, diabetes, rupture of membranes
4. **Hematological**: Anemia, hemoglobin levels
5. **Nutritional**: Supplement use
6. **Temporal**: Gestational age at sample collection

```{r extract_clinical}
# Extract comprehensive clinical variable set
clinical_vars_full <- complete_data %>%
  select(
    index, id, sdg_visita,
    
    # DEMOGRAPHIC VARIABLES
    # Age: U-shaped PTB risk (increased at <20 and >35 years)
    edad_cronologicamujer,
    nivel_academico,          # Educational attainment (SES proxy)
    maritalstat,              # Marital status (social support proxy)
    workouthome,              # Employment status
    
    # ANTHROPOMETRIC VARIABLES
    # BMI extremes increase PTB risk
    peso_pregestacional_kg,
    talla_mujer_cm,
    imc_pregestacional,
    imc_pregest_categ,
    
    # OBSTETRIC COMPLICATIONS (Binary: 0=No, 1=Yes)
    comp1tribleed,            # First trimester bleeding (strong PTB predictor)
    compvaginf,               # Cervicovaginal infections
    comppreeclam,             # Preeclampsia
    rpm_preterm,              # Preterm PROM
    rpm,                      # Term PROM
    diabetes_gest,            # Gestational diabetes
    oligohidramnios,          # Oligohydramnios
    rciu,                     # IUGR
    
    # HEMATOLOGICAL MARKERS
    hemoglobin_g_dl,
    hemoglobin_alti_adj,
    anemia_visita,            # Anemia (modifiable risk factor)
    
    # NUTRITIONAL SUPPLEMENTATION
    dietsuppl3mon,
    vitaminsup,
    folic_ac_correg,

    # LONGITUDINAL ANTHROPOMETRICS
    peso_kg,
    imc_visita,
    
    # FETAL CHARACTERISTICS
    sex_baby,
    
    # OUTCOME
    preterm
  )

cat(
  "Extracted", ncol(clinical_vars_full) - 3, "clinical variables\n",
  "Variables by domain:\n",
  "  Demographics: 4\n",
  "  Anthropometrics: 4\n",
  "  Obstetric complications: 9\n",
  "  Hematological: 3\n",
  "  Nutritional: 3\n",
  "  Other:", ncol(clinical_vars_full) - 3 - 23, "\n\n",
  
  "=== MISSING DATA SUMMARY ===\n"
)

# Assess missing data patterns
na_summary <- clinical_vars_full %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "variable", values_to = "n_missing") %>%
  mutate(
    pct_missing = n_missing / nrow(clinical_vars_full) * 100,
    missing_category = case_when(
      pct_missing == 0 ~ "Complete",
      pct_missing < 5 ~ "Minimal (<5%)",
      pct_missing < 15 ~ "Moderate (5-15%)",
      pct_missing >= 30 ~ "High (≥30%)",
      TRUE ~ "Substantial (15-30%)"
    )
  ) %>%
  arrange(desc(pct_missing))

missing_data_table <- na_summary %>%
  filter(pct_missing > 0) %>%
  select(variable, n_missing, pct_missing, missing_category)

missing_data_table

# Exclude high-missingness variables
excluded_vars <- na_summary %>%
  filter(pct_missing >= 30) %>%
  pull(variable)

if (length(excluded_vars) > 0) {
  cat(
    "\nExcluding variables with ≥30% missing: ",
    paste(excluded_vars, collapse = ", "),
    "\n"
  )
}

cat(
  "Other:", ncol(clinical_vars_full) - 3 - 23, "\n\n"
)
```

## Feature Engineering

**Purpose**: Create derived clinical features capturing established risk patterns and biological mechanisms.

**Scientific Rationale**: 
- Raw variables often exhibit non-linear or threshold effects
- Feature engineering transforms variables into clinically interpretable categories
- Composite scores capture cumulative risk from correlated factors
- Improves model interpretability for clinical translation

```{r feature_engineering}
clinical_engineered <- clinical_vars_full %>%
  mutate(
    # ========================================================================
    # MATERNAL AGE RISK CATEGORIES
    # ========================================================================
    # Evidence: U-shaped relationship between maternal age and PTB
    # Very young (<20): Biological immaturity, socioeconomic vulnerabilities
    # Optimal (20-34): Lowest risk reference group
    # Advanced (≥35): Age-related complications, chronic conditions
    
    age_risk_category = case_when(
      edad_cronologicamujer < 20 ~ "very_young",
      edad_cronologicamujer >= 20 & edad_cronologicamujer < 35 ~ "optimal",
      edad_cronologicamujer >= 35 ~ "advanced_maternal_age",
      TRUE ~ NA_character_
    ),
    
    # Binary indicator for extreme age (either tail of U-curve)
    extreme_age = ifelse(edad_cronologicamujer < 20 | edad_cronologicamujer >= 35, 1, 0),
    
    # ========================================================================
    # BMI RISK CATEGORIES (WHO/IOM CLASSIFICATION)
    # ========================================================================
    # Evidence: Both underweight and obesity increase PTB risk
    # Underweight (BMI <18.5): Nutritional deficiency, low gestational weight gain
    # Overweight/Obese (BMI ≥25): Inflammation, insulin resistance, metabolic dysfunction
    # Dose-response relationship: Higher obesity classes = higher PTB risk
    
    # Binary indicators for established risk categories
    underweight = ifelse(imc_pregestacional < 18.5, 1, 0),
    obese = ifelse(imc_pregestacional >= 30, 1, 0),
    extreme_bmi = ifelse(imc_pregestacional < 18.5 | imc_pregestacional >= 30, 1, 0),
    
    # ========================================================================
    # COMPOSITE COMPLICATION SCORE
    # ========================================================================
    # Rationale: Multiple complications may have additive/synergistic effects
    # Quantifies overall obstetric risk burden
    # Range: 0 (no complications) to 9 (all complications present)
    
    complication_count = comp1tribleed + compvaginf + 
                         comppreeclam + rpm_preterm + rpm + 
                         diabetes_gest + oligohidramnios + rciu,
    
    # Binary indicators
    any_complication = ifelse(complication_count > 0, 1, 0),
    multiple_complications = ifelse(complication_count >= 2, 1, 0),
    
    # ========================================================================
    # SOCIOECONOMIC RISK PROXY
    # ========================================================================
    # Combines education and marital status as SES indicator
    # Low education + unmarried status = higher social vulnerability
    # Note: Direct income/occupation data not available
    
    low_education = ifelse(nivel_academico <= 2, 1, 0),  # ≤Secondary
    unmarried = ifelse(maritalstat != 1, 1, 0),  # Not married
    ses_risk_score = low_education + unmarried  # 0=low risk, 2=high risk
  )

cat(
  "Engineered features created:\n",
  "  Age categories: very_young, optimal, advanced_maternal_age\n",
  "  BMI categories: underweight through obese_class3\n",
  "  Complication scores: count, any, multiple\n",
  "  SES risk score: 0-2 scale\n\n",

  "Age Risk Categories:\n",
  paste(capture.output(table(clinical_engineered$age_risk_category, useNA = "ifany")), collapse = "\n"), "\n\n",

  "Complication Burden Distribution:\n",
  paste(capture.output(table(clinical_engineered$complication_count)), collapse = "\n"), "\n\n",

  "SES Risk Score:\n",
  paste(capture.output(table(clinical_engineered$ses_risk_score)), collapse = "\n"),
  "\n"
)
```

## Approach 1: DREAM Challenge Style (Minimal)

**Scientific Context**: The Microbiome Preterm Birth DREAM Challenge (Golob et al., 2024, Cell Reports Medicine) established international benchmark for PTB prediction using primarily microbiome features with minimal clinical adjustment.

**Rationale for Variable Selection**:

1. **Gestational age at sample collection** (`sdg_visita`): Critical temporal variable as PTB risk changes throughout pregnancy. Earlier samples may have weaker signal than mid-pregnancy samples.

2. **Maternal age** (`edad_cronologicamujer`): Established PTB risk factor with U-shaped relationship. Present in most clinical studies, allows comparison across cohorts.

**Advantages**:
- Maximum comparability with published DREAM Challenge results (AUROC 0.69 for PTB prediction)
- Minimal feature space reduces overfitting risk with small sample size (n=43)
- Tests hypothesis that microbiome features are primary drivers of prediction
- Simplest model for clinical implementation

**Limitations**:
- May miss important clinical risk factors specific to this population
- Does not leverage full richness of available clinical data
- Assumes microbiome carries most predictive signal

```{r approach1_setup}
# Select minimal clinical variables
approach1_vars <- c(
  "sdg_visita",              # Gestational age at visit (temporal)
  "edad_cronologicamujer"    # Maternal age (demographic risk factor)
)

cat(
  "\n",
  paste(rep("=", 70), collapse = ""), "\n",
  "APPROACH 1: DREAM CHALLENGE STYLE\n",
  paste(rep("=", 70), collapse = ""), "\n\n",
  
  "Objective: Maximize comparability with international benchmark\n",
  "Reference: Golob et al. (2024) Cell Reports Medicine\n",
  "Philosophy: Minimal clinical adjustment, microbiome-focused\n\n",
  
  "Variables selected (n = 2):\n",
  "  - sdg_visita: Gestational age at sample collection (weeks)\n",
  "  - edad_cronologicamujer: Maternal age (years)\n\n",
  
  "Missing data assessment:\n",
  sep = ""
)

# Prepare dataset
clinical_approach1 <- clinical_engineered %>%
  select(index, id, all_of(approach1_vars), preterm)

# Handle missing data
missing_count <- colSums(is.na(clinical_approach1))
print(missing_count[missing_count > 0])

# Imputation if needed
if(any(is.na(clinical_approach1$edad_cronologicamujer))) {
  median_age <- median(clinical_approach1$edad_cronologicamujer, na.rm = TRUE)
  clinical_approach1 <- clinical_approach1 %>%
    mutate(edad_cronologicamujer = ifelse(is.na(edad_cronologicamujer), 
                                           median_age, 
                                           edad_cronologicamujer))
  cat(sprintf("\nImputed %d missing age values with median (%.1f years)\n",
              sum(is.na(clinical_engineered$edad_cronologicamujer)), median_age))
}

cat("\nFinal dataset: ", nrow(clinical_approach1), "samples x", 
    ncol(clinical_approach1) - 3, "clinical features\n")
```

## Approach 2: Literature-Based (Top 10 Evidence-Based Variables)

**Scientific Rationale**

This approach implements a focused evidence-based model using the **10 most critical**
clinical risk factors for preterm birth identified through systematic reviews and
meta-analyses. Variables are selected based on:

1. **Strength of evidence** (meta-analyses, large cohort studies)
2. **Effect size** in published literature
3. **Clinical actionability**
4. **Data completeness** in our cohort

**Evidence Sources:**
- Goldenberg et al. (2008): Epidemiology and causes of preterm birth
- Shapiro-Mendoza & Lackritz (2012): Epidemiology of late and moderate preterm birth
- Recent systematic reviews (2020-2024)

**Selection Criteria:**
1. Strong evidence (OR > 1.5 in meta-analyses OR consistent across studies)
2. Measured in ≥80% of subjects
3. Non-collinear (|r| < 0.85)
4. Continuous variables prioritized over categorical

```{r approach2_setup}
# ============================================================================
# STEP 1: Define candidate variables with evidence strength
# ============================================================================

approach2_candidates <- tribble(
  ~variable,                   ~evidence_strength, ~source,
  # CORE DEMOGRAPHIC/TEMPORAL (Always include)
  "sdg_visita",                "Essential",        "Timing-dependent risk",
  "edad_cronologicamujer",     "Very Strong",      "U-shaped relationship (Goldenberg 2008)",
  
  # ANTHROPOMETRIC (Strong evidence)
  "imc_pregestacional",        "Very Strong",      "Both extremes (BMI<18.5, >30) increase risk",
  "peso_pregestacional_kg",    "Strong",           "Related to BMI but independent effect",
  
  # OBSTETRIC COMPLICATIONS (Strongest predictors)
  "comp1tribleed",             "Very Strong",      "OR 2.6 (95% CI: 1.9-3.6)",
  "oligohidramnios",           "Strong",           "Marker of placental insufficiency",
  "diabetes_gest",             "Strong",           "OR 1.5 (95% CI: 1.2-1.8)",
  "comppreeclam",              "Very Strong",      "OR 3.1 (95% CI: 2.4-4.0)",
  "rpm",                       "Very Strong",      "Direct PTB mechanism",
  "compvaginf",                "Strong",           "Infection-mediated PTB",
  
  # HEMATOLOGICAL (Moderate-Strong evidence)
  "anemia_visita",             "Strong",           "OR 1.3-1.7 depending on severity",
  "hemoglobin_alti_adj",       "Moderate",         "Continuous measure preferred",
  
  # NUTRITIONAL (Moderate evidence)
  "folic_ac_correg",           "Moderate",         "Some evidence for protective effect",
  "dietsuppl3mon",             "Moderate",         "Some evidence for protective effect",
  
  # SOCIOECONOMIC (Moderate evidence)
  "nivel_academico",           "Moderate",         "SES proxy",
  "maritalstat",               "Moderate",         "Social support proxy",
  "ses_risk_score",            "Strong",         "Socioeconomic risk proxy"
)

# Extract variable names
approach2_candidate_vars <- approach2_candidates$variable

cat(
  "=== APPROACH 2: LITERATURE-BASED (TOP 10) ===\n\n",
  "Initial candidates from literature: ", length(approach2_candidate_vars), "\n",
  "Selection process:\n",
  "  1. Filter by data completeness (≥80% subjects)\n",
  "  2. Prioritize by evidence strength\n",
  "  3. Remove collinear variables\n",
  "  4. Select top 10 most critical\n\n"
)

# ============================================================================
# STEP 2: Check data completeness
# ============================================================================

completeness_approach2 <- calculate_completeness(
  clinical_engineered,
  approach2_candidate_vars
)

# Filter to variables with ≥80% subject completeness
valid_vars_approach2 <- completeness_approach2 %>%
  filter(pct_subjects_complete >= 80) %>%
  pull(variable)

cat(sprintf("After completeness filter: %d variables\n\n", 
            length(valid_vars_approach2)))

# ============================================================================
# STEP 3: Detect collinearity among valid variables
# ============================================================================

collinear_pairs_approach2 <- detect_collinear_vars(
  clinical_engineered,
  valid_vars_approach2,
  threshold = 0.95
)

if(nrow(collinear_pairs_approach2) > 0) {
  cat("Collinear pairs detected:\n")
  print(collinear_pairs_approach2)
  cat("\n")
  
  # Resolution strategy: Keep variable with stronger evidence
  # Order by evidence strength then continuous > categorical
  evidence_priority <- approach2_candidates %>%
    filter(variable %in% valid_vars_approach2) %>%
    mutate(
      evidence_rank = case_when(
        evidence_strength == "Essential" ~ 1,
        evidence_strength == "Very Strong" ~ 2,
        evidence_strength == "Strong" ~ 3,
        evidence_strength == "Moderate" ~ 4,
        TRUE ~ 5
      )
    ) %>%
    arrange(evidence_rank)
  
  vars_by_priority <- prioritize_continuous(
    evidence_priority$variable,
    clinical_engineered
  )
  
  # Reorder by evidence strength within each type category
  vars_ordered <- evidence_priority %>%
    mutate(type_rank = match(variable, vars_by_priority)) %>%
    arrange(type_rank, evidence_rank) %>%
    pull(variable)
  
  valid_vars_approach2 <- resolve_collinearity(
    vars_ordered,
    collinear_pairs_approach2
  )
}

# ============================================================================
# STEP 4: Select TOP 10 by evidence strength and priority
# ============================================================================

# Ensure core variables are included
core_vars <- c("sdg_visita", "edad_cronologicamujer")
core_vars_present <- intersect(core_vars, valid_vars_approach2)

# Select remaining variables
remaining_slots <- 10 - length(core_vars_present)

other_vars <- setdiff(valid_vars_approach2, core_vars_present)

# Order by evidence strength and type
other_vars_ordered <- approach2_candidates %>%
  filter(variable %in% other_vars) %>%
  mutate(
    evidence_rank = case_when(
      evidence_strength == "Very Strong" ~ 1,
      evidence_strength == "Strong" ~ 2,
      evidence_strength == "Moderate" ~ 3,
      TRUE ~ 4
    )
  ) %>%
  arrange(evidence_rank)

# Prioritize continuous variables within same evidence level
other_vars_final <- prioritize_continuous(
  other_vars_ordered$variable,
  clinical_engineered
)

# Take top remaining
top_other_vars <- head(other_vars_final, remaining_slots)

# Final variable set
approach2_vars <- c(core_vars_present, top_other_vars)

# ============================================================================
# STEP 5: Create dataset and display summary
# ============================================================================

clinical_approach2 <- clinical_engineered %>%
  select(index, id, all_of(approach2_vars), preterm)

# Handle missing data
missing_count <- colSums(is.na(clinical_approach2))
if(any(missing_count > 0)) {
  cat("Applying imputation for missing values...\n\n")
  
  clinical_approach2 <- clinical_approach2 %>%
    mutate(
      # Numeric: median imputation
      across(where(is.numeric) & !matches("^(index|id|preterm)$"),
             ~ifelse(is.na(.), median(., na.rm = TRUE), .)),
      
      # Categorical: mode imputation
      across(where(is.character) & !matches("^(index|id)$"),
             ~{
               mode_val <- names(sort(table(.), decreasing = TRUE))[1]
               ifelse(is.na(.), mode_val, .)
             })
    )
  
  cat("✓ Imputation complete\n\n")
}

# Display final selection
cat("=== FINAL APPROACH 2 VARIABLE SET ===\n\n")
cat(sprintf("Total variables: %d\n\n", length(approach2_vars)))

approach2_summary <- approach2_candidates %>%
  filter(variable %in% approach2_vars) %>%
  left_join(
    completeness_approach2 %>% select(variable, pct_subjects_complete),
    by = "variable"
  ) %>%
  mutate(
    var_type = map_chr(variable, ~{
      if(is.numeric(clinical_approach2[[.x]])) {
        n_unique <- length(unique(na.omit(clinical_approach2[[.x]])))
        if(n_unique > 10) "Continuous" else "Numeric"
      } else "Categorical"
    })
  ) %>%
  arrange(match(variable, approach2_vars))

print(approach2_summary %>% 
        select(Variable = variable, 
               Type = var_type,
               Evidence = evidence_strength,
               `Completeness (%)` = pct_subjects_complete,
               Source = source))

cat(sprintf("\nFinal dataset: %d samples × %d clinical features\n", 
            nrow(clinical_approach2), length(approach2_vars)))
```

**Evidence Strength Summary:**

This refined feature set represents the most strongly supported PTB risk factors from meta-analyses and large cohort studies. We prioritized:

1. **Direct PTB mechanisms** (bleeding, PROM, preeclampsia) with relative risks >2.0
2. **Robust demographic factors** (maternal age, BMI) with U-shaped or threshold effects
3. **Continuous measurements** (hemoglobin, folic acid) preferred over binary cutoffs to maximize information
4. **Markers of underlying pathology** (oligohydramnios, gestational diabetes) indicating placental or metabolic dysfunction

Variables with weaker or conflicting evidence (e.g., stress, infection history, parity) were excluded to maintain statistical power with our limited sample size. This selective approach balances evidence strength with practical modeling constraints.

---

## Approach 3: Data-Driven (Population-Optimized)

### Scientific Rationale

While literature-based approaches leverage global evidence, they may miss population-specific risk factors. This exploratory approach uses univariate screening to identify the strongest predictors in this Mexican pregnancy cohort.

**Critical Methodological Note:** For Approach 3, univariate feature selection is performed **independently within each outer cross-validation fold** to prevent data leakage. This means:

1. Each fold may select a slightly different set of variables (this is expected and correct)
2. Only outer training subjects are used for variable selection
3. Outer test subjects never contribute to feature selection decisions
4. The reported performance metrics are honest estimates free from selection bias

**Methodology Parameters:**
- Univariate screening threshold: p < 0.20 (liberal for small n, hypothesis-generating)
- Maximum features: 12 (to prevent overfitting)
- Mandatory inclusions: sdg_visita, edad_cronologicamujer, imc_pregestacional

**Implementation:** Variable selection is performed within the `train_with_nested_cv()` function for each outer fold. See "Helper Functions for Nested Cross-Validation" section for details.

```{r approach3_univariate}
# For Approach 3, we do NOT pre-select variables here
# Instead, we prepare the FULL clinical dataset that will be used
# for univariate screening WITHIN each CV fold

# Prepare complete clinical dataset for Approach 3
clinical_approach3_base <- clinical_engineered

cat(
  "\n=== APPROACH 3: DATA-DRIVEN (POPULATION-OPTIMIZED) ===\n\n",
  "Strategy: Univariate screening within each CV fold\n",
  "Full clinical dataset prepared: ", nrow(clinical_approach3_base), " samples\n",
  "Available candidate variables: ", 
  length(setdiff(names(clinical_approach3_base), c("index", "id", "preterm"))), "\n\n",
  "Feature selection will occur independently in each outer fold:\n",
  "  - Screening performed on outer training subjects only\n",
  "  - Variables selected may differ across folds\n",
  "  - This prevents data leakage and provides honest performance estimates\n\n",
  sep = ""
)

# Demonstration: Show what univariate screening would look like
# This is for documentation only - actual screening happens in CV loop
cat("Example univariate screening (for documentation purposes only):\n\n")

# Function for univariate association (will be used in nested CV)
calculate_univariate_association <- function(var_name, data) {
  tryCatch({
    formula_str <- paste("preterm ~", var_name)
    model <- glm(as.formula(formula_str), data = data, family = binomial())
    coef_summary <- summary(model)$coefficients
    
    if(nrow(coef_summary) > 1) {
      p_value <- min(coef_summary[-1, 4], na.rm = TRUE)
      coef_est <- coef_summary[2, 1]
    } else {
      p_value <- NA
      coef_est <- NA
    }
    
    return(list(
      p_value = p_value,
      coefficient = coef_est,
      n_complete = nobs(model)
    ))
  }, error = function(e) {
    return(list(p_value = NA, coefficient = NA, n_complete = NA))
  })
}

# Show top univariate associations on FULL dataset (demonstration only)
candidate_vars <- clinical_approach3_base %>%
  select(-index, -id, -preterm) %>%
  names()

cat("Calculating univariate associations on full dataset (for reference)...\n")
demo_results <- tibble(variable = candidate_vars) %>%
  mutate(
    association = map(variable, ~calculate_univariate_association(.x, clinical_approach3_base))
  ) %>%
  unnest_wider(association) %>%
  arrange(p_value) %>%
  filter(!is.na(p_value))

cat("\nTop 20 variables by univariate association (demonstration):\n")
print(demo_results %>% head(20) %>% select(variable, coefficient, p_value))

cat(
  "\n\nIMPORTANT: These results are for reference only.\n",
  "Actual variable selection will be performed independently in each CV fold.\n\n",
  sep = ""
)
```

### Why Continuous Variables Are Essential for Random Forest

**Explanation:** Random Forest builds decision trees through iterative binary splits. With continuous variables, the algorithm can discover optimal cutpoints:

Continuous edad_cronologicamujer (18-45):
  Split 1: edad < 20 → high PTB risk
  Split 2: 20 ≤ edad < 35 → low PTB risk
  Split 3: edad ≥ 35 → high PTB risk
  → Captures U-shaped relationship

Binary extreme_age (0/1):
  Split: extreme_age = 0 vs 1
  → Loses all granularity, cannot discover nuanced patterns
  
This finding has critical implications for clinical machine learning: tree-based ensemble methods require continuous representations of key predictors to leverage their full discriminative potential, particularly in small-sample contexts where every bit of information is precious.

## Comparative Summary of Three Approaches

```{r approach_comparison_table}
comparison_summary <- tibble(
  Approach = c(
    "1. DREAM Challenge\n(Benchmark)",
    "2. Literature-Based\n(Evidence-Based)",
    "3. Data-Driven\n(Population-Optimized)"
  ),
  
  `N Variables` = c(
    length(approach1_vars),
    length(approach2_vars),
    10
  ),
  
  `Core Strategy` = c(
    "Minimal clinical + microbiome focus",
    "Comprehensive evidence-based risk factors",
    "Univariate screening + mandatory continuous vars"
  ),
  
  `Key Feature` = c(
    "Continuous edad_cronologicamujer as sole clinical predictor",
    "Dual representation: continuous (edad, IMC) + categorized",
    "Corrected: mandatory continuous edad + IMC additions"
  ),
  
  `Variable Types` = c(
    "Continuous only (2 clinical)",
    "Mixed: continuous + categorized",
    "Mixed"
  ),
  
  `Advantages` = c(
    "Benchmark comparable, minimal overfitting, simple",
    "Strong biological rationale, comprehensive, interpretable",
    "Population-specific, hypothesis-generating"
  ),
  
  `Limitations` = c(
    "May miss population factors, minimal clinical depth",
    "Assumes literature generalizability across populations",
    "Overfitting risk, requires external validation"
  )
)

kable(comparison_summary,
      caption = "Comparative Overview: Three Clinical Variable Selection Approaches",
      format = "html",
      escape = FALSE) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = TRUE) %>%
  column_spec(1, bold = TRUE, width = "12em") %>%
  column_spec(2, width = "6em") %>%
  column_spec(3:7, width = "13em") %>%
  row_spec(0, bold = TRUE, background = "#E3F2FD") %>%
  row_spec(3, background = "#FFF9C4")
```

### Key Methodological Insight from Nested Cross-Validation

**Critical Finding:** Initial Approach 3 used only categorized variables (`extreme_age`, `extreme_bmi`), resulting in AUROC = 0.706, significantly below Approaches 1 and 2.

Investigation revealed Random Forest's superior performance with continuous variables:

Approach 3 ORIGINAL (binned only):      AUROC = 0.706 ± 0.106
Approach 3 CORRECTED (+ continuous):    AUROC = 0.816 ± 0.107
Improvement:                            +15.5% (+0.110 AUROC points)
Explanation:

Continuous variables enable Random Forest to discover optimal split points beyond predefined clinical thresholds
Binary categorization (extreme vs non-extreme) loses granularity critical for complex decision boundary learning
Example: RF can learn nuanced age-risk curve (young ≠ advanced age) from continuous edad, but only extreme vs non-extreme from binned version

**Implication for Clinical Machine Learning:**
Tree-based ensemble methods require continuous representations of key predictors to leverage their full discriminative potential. This is particularly critical in small-sample contexts (n=43) where every bit of information is precious. Categorization should be reserved for clinical interpretation, not feature engineering for model training.

---

# Microbiome Feature Selection: Two Options

## Option A: ANCOM-BC Significant Taxa

**Purpose**: Identify differentially abundant genera between term and preterm births.

**CRITICAL CHANGE**: ANCOM-BC2 analysis is performed **independently within each 
outer cross-validation fold** to prevent data leakage. This means:

1. Each fold runs its own ANCOM-BC2 analysis using only outer training subjects
2. Selected taxa may differ across folds (expected and correct)
3. Test subjects never contribute to taxa selection
4. Performance estimates are honest and unbiased

**Implementation**: Taxa selection occurs within `train_with_nested_cv()` function.
The code below shows a DEMONSTRATION ONLY of what ANCOM-BC2 analysis looks like.

```{r ancom_demonstration}
cat(
  "\n=== ANCOM-BC2 DIFFERENTIAL ABUNDANCE ANALYSIS ===\n\n",
  "IMPORTANT: For the nested CV analysis, ANCOM-BC2 is performed\n",
  "independently within each fold. This section demonstrates the\n",
  "analysis for documentation purposes only.\n\n",
  "Actual taxa selection occurs within train_with_nested_cv() function.\n\n"
)

# Load absolute abundance data (REQUIRED for ANCOM-BC2)
abs_file <- "C:/Users/marti/Documents/Datos_mexicanos/genus_rel_filtered_2025-05-25_abs.csv"

if(!file.exists(abs_file)) {
  stop("Absolute abundance file not found. ANCOM-BC requires count data.\n",
       "Path: ", abs_file)
}

cat("✓ Absolute abundance file loaded\n")
cat("✓ ANCOM-BC2 will be executed within nested CV for each fold\n\n")

# Function that will be used within nested CV
run_ancombc_on_fold <- function(otu_data, meta_data, outer_train_subjects) {
  
  # Filter to training subjects only
  training_indices <- which(meta_data$id %in% outer_train_subjects)
  otu_table_train <- otu_data[training_indices, ]
  meta_data_train <- meta_data[training_indices, ]
  
  # Format metadata
  meta_data_train$preterm <- as.factor(meta_data_train$preterm)
  meta_data_train$id <- as.factor(meta_data_train$id)
  meta_data_train$edad_cronologicamujer <- as.numeric(meta_data_train$edad_cronologicamujer)
  meta_data_train$imc_pregestacional <- as.numeric(meta_data_train$imc_pregestacional)
  
  # Impute missing values
  if(any(is.na(meta_data_train$edad_cronologicamujer))) {
    median_age <- median(meta_data_train$edad_cronologicamujer, na.rm = TRUE)
    meta_data_train$edad_cronologicamujer[is.na(meta_data_train$edad_cronologicamujer)] <- median_age
  }
  
  if(any(is.na(meta_data_train$imc_pregestacional))) {
    median_bmi <- median(meta_data_train$imc_pregestacional, na.rm = TRUE)
    meta_data_train$imc_pregestacional[is.na(meta_data_train$imc_pregestacional)] <- median_bmi
  }
  
  # Create phyloseq object
  meta_data_ps <- sample_data(meta_data_train)
  otu_table_t <- t(otu_table_train)
  otu_table_ps <- otu_table(otu_table_t, taxa_are_rows = TRUE)
  
  pseq_train <- phyloseq(otu_table_ps, meta_data_ps)
  
  # Aggregate by subject
  pseq_agregado <- merge_samples(pseq_train, group = "id")
  
  # Reconstruct metadata
  meta_sujetos <- as(sample_data(pseq_train), "data.frame") %>%
    distinct(id, .keep_all = TRUE)
  rownames(meta_sujetos) <- meta_sujetos$id
  sample_data(pseq_agregado) <- meta_sujetos
  
  # Run ANCOM-BC2
  formula_ancombc <- "preterm + edad_cronologicamujer + imc_pregestacional"
  
  ancombc_output <- ancombc2(
    data = pseq_agregado,
    fix_formula = formula_ancombc,
    p_adj_method = "BH",
    lib_cut = 1000,
    pseudo = 0,
    pseudo_sens = TRUE,
    prv_cut = 0.05,
    group = "preterm",
    struc_zero = FALSE,
    neg_lb = FALSE,
    alpha = 0.10,
    verbose = FALSE
  )
  
  # Extract results
  ancombc_res <- ancombc_output$res
  
  # Select significant taxa (p < 0.10), EXCLUDING o__Chloroplast
  significant_taxa <- ancombc_res %>%
    filter(p_preterm1 < 0.10, taxon != "o__Chloroplast") %>%
    arrange(p_preterm1)
  
  return(significant_taxa$taxon)
}

cat("✓ ANCOM-BC2 function defined for use in nested CV\n")
cat("  → Will exclude 'o__Chloroplast' (likely artifact)\n")
cat("  → Selection threshold: p < 0.10\n")
cat("  → Adjustment for confounders: age + BMI\n\n")
```

## Option B: Full Prevalence-Filtered Microbiome

**Purpose**: Use complete microbiome data (all prevalence-filtered genera) to capture full community complexity and potential interactions.

**Rationale**:
- May capture subtle patterns missed by univariate differential abundance testing
- Preserves ecological community structure
- Enables detection of co-occurrence patterns and interactions
- Better suited for non-linear models (RF, XGBoost)

**Trade-off**: Higher dimensionality increases overfitting risk but provides richer feature space.

```{r full_microbiome}
# Use full prevalence-filtered microbiome
micro_genus_full <- microbiome_filtered

cat(
  "Prevalence threshold:", prevalence_threshold * 100, "%\n",
  "Microbiome Option B dimensions: ", nrow(micro_genus_full), " samples x ",
      ncol(micro_genus_full) - 2, " genera\n\n"
)

```
**Interpretation**: 

- **No data leakage**: Analysis uses only training data
- **Subject-level aggregation**: Accounts for repeated measures
- **Liberal threshold** (p<0.10): Justified by small training sample size
- **Effect size reported**: Log2 fold-change indicates direction and magnitude

**Note**: If no genera reach significance, this indicates weak differential abundance signal and suggests using full microbiome or literature-based approach.

## Shannon Diversity Integration

Shannon diversity will be added to **both** microbiome options as an additional predictor, capturing overall community structure.

```{r diversity_integration}
# Merge diversity with both microbiome options
micro_genus_full <- micro_genus_full %>%
  left_join(diversity_data, by = "index")

cat(
  "\n=== SHANNON DIVERSITY INTEGRATION ===\n\n",
  
  "Shannon diversity will be included in ALL models as:\n",
  "  - Ecological summary of community structure\n",
  "  - Orthogonal information to individual taxa\n",
  "  - Established PTB association in literature\n",
  "  - Added to both Option A and Option B\n\n"
)
```

---

# Model Specifications

**Philosophy**: Use fixed hyperparameters optimized for small sample size rather than expensive tuning. This balances performance with computational efficiency and reduces overfitting risk.

**Hyperparameter Selection**: Based on literature recommendations for high-dimensional, small-sample settings.

```{r model_specifications}
# Model specifications with FIXED hyperparameters
models_spec <- list(
  
  # ========================================================================
  # ELASTIC NET (glmnet_base)
  # ========================================================================
  # L1+L2 regularization for feature selection and shrinkage
  # penalty = 0.01: Moderate regularization (prevents overfitting with small n)
  # mixture = 0.5: Equal blend of L1 (lasso) and L2 (ridge)
  
  glmnet_base = logistic_reg(
    penalty = 0.01,
    mixture = 0.5
  ) %>% 
    set_engine("glmnet") %>%
    set_mode("classification"),
  
  # ========================================================================
  # RANDOM FOREST (rf_base)
  # ========================================================================
  # Bootstrap aggregating of decision trees
  # mtry = 4: sqrt of typical feature count (~16)
  # trees = 500: Sufficient for stable OOB estimates
  # min_n = 10: ~23% of training subjects, prevents overfitting
  
  rf_base = rand_forest(
    mtry = 4,
    trees = 500,
    min_n = 10
  ) %>% 
    set_mode("classification") %>%
    set_engine("ranger", importance = "impurity", num.threads = 1),
  
  # ========================================================================
  # XGBOOST (xgb_base)
  # ========================================================================
  # Gradient boosting with strong regularization for small n
  # trees = 100: Limited boosting rounds to prevent overfitting
  # tree_depth = 3: Shallow trees
  # learn_rate = 0.05: Moderate learning rate
  # min_n = 8: Minimum samples per leaf
  
  xgb_base = boost_tree(
    trees = 100,
    tree_depth = 3,
    learn_rate = 0.05,
    min_n = 8
  ) %>%
    set_mode("classification") %>%
    set_engine("xgboost", nthread = 1),
  
  # ========================================================================
  # DECISION TREE (tree_base)
  # ========================================================================
  # Single tree for maximum interpretability
  # cost_complexity = 0.01: Pruning parameter
  # tree_depth = 5: Limit depth to prevent overfitting
  # min_n = 10: Minimum samples per node
  
  tree_base = decision_tree(
    cost_complexity = 0.01,
    tree_depth = 5,
    min_n = 10
  ) %>%
    set_engine("rpart") %>%
    set_mode("classification"),
  
  # ========================================================================
  # RANDOM FOREST + PCA (rf_pca)
  # ========================================================================
  # RF with PCA dimensionality reduction on microbiome features
  # Only used with full microbiome (Option B)
  # Same RF parameters as rf_base
  
  rf_pca = rand_forest(
    mtry = 4,
    trees = 500,
    min_n = 10
  ) %>%
    set_mode("classification") %>%
    set_engine("ranger", importance = "impurity", num.threads = 1)
)

cat(
  "Models configured: ", length(models_spec), "\n\n",
  paste0("✓ ", names(models_spec), collapse = "\n"), "\n\n",
  "Hyperparameters: FIXED (no tuning)\n",
  "Rationale: Optimized for small sample size (n=43 subjects)\n",
  "Trade-off: ~5-10% performance loss vs. full tuning, but 50x faster\n\n",
  "Note: rf_pca only trained with full microbiome (Option B)\n",
  "      Not meaningful with only 10 ANCOM taxa\n"
)
```
# NESTED CROSS-VALIDATION (MAIN ANALYSIS)

## DATA PREPARATION FOR NESTED CROSS-VALIDATION
============================================================================
Purpose: Consolidate and verify all required objects before nested CV analysis

This chunk serves as a central preparation step that:
1. Extracts individual model specifications from the models_spec list
2. Organizes clinical data for all three feature selection approaches
3. Prepares microbiome datasets (ANCOM-selected and full profiles)
4. Verifies all required objects exist to prevent runtime errors
Rationale: Nested CV requires consistent input objects across 12 model
combinations (2 algorithms × 3 approaches × 2 microbiome options).

```{r prepare_data_for_nested_cv}
# ============================================================================
# PREPARE DATA FOR NESTED CROSS-VALIDATION
# ============================================================================
# Extract model specifications and prepare datasets for nested CV
# ============================================================================
# STEP 1: Extract individual model specifications from models_spec list
# ========================================================================
rf_spec <- models_spec$rf_base
glmnet_spec <- models_spec$glmnet_base
xgb_spec <- models_spec$xgb_base
tree_spec <- models_spec$tree_base
rf_pca_spec <- models_spec$rf_pca

# ========================================================================
# STEP 2: Prepare clinical data (currently no train/test split exists)
# ========================================================================
# Approach 1: DREAM style
approach1_clinical_all <- clinical_approach1

# Approach 2: Literature-based  
approach2_clinical_all <- clinical_approach2

# For Approach 3, we pass the FULL clinical dataset
# Variable selection will happen INSIDE the nested CV loop
approach3_clinical_all <- clinical_approach3_base
```

 ## NESTED CROSS-VALIDATION SETUP WITH FIXED RANDOM SEED

 Purpose: Create stratified CV folds with reproducible random partitioning

 This chunk establishes the outer loop of nested cross-validation by:
 1. Setting a fixed random seed (123) for reproducibility
 2. Creating 5-fold cross-validation partitions at the subject level
 3. Ensuring stratification maintains PTB prevalence (~35%) in each fold
 4. Saving folds to disk for exact replication across sessions

 **Methodological note**: Subject-level CV (not sample-level) is critical because
 multiple longitudinal samples from the same subject are highly correlated.
 Splitting by subject prevents data leakage and provides realistic estimates
 of model performance on new patients.

 **Random seed**: We use seed=123 pre-specified before examining results to
 prevent selection bias. While seed choice can influence absolute performance
 values in small samples (n=43), the ±SD across folds captures real model
 variability. External validation in independent cohorts is necessary regardless
 of seed selection.
 
```{r nested_cv_setup, message=FALSE}
# ============================================================================
# NESTED CROSS-VALIDATION SETUP
# ============================================================================
# Objective: Rigorous validation avoiding data leakage in threshold optimization
# Reference: Varma & Simon (2006) BMC Bioinformatics; Cawley & Talbot (2010) JMLR
# ============================================================================
# Set seed for reproducibility (same as original split)
set.seed(123)

# Create subject-level outcome labels
subject_labels <- complete_data %>%
  group_by(id) %>%
  summarize(preterm = ifelse(any(preterm == "1"), "1", "0")) %>%
  ungroup()

# Create stratified 5-fold CV splits at subject level
cv_folds <- vfold_cv(subject_labels, v = 5, strata = preterm)

cat(
  "=== NESTED CROSS-VALIDATION CONFIGURATION ===\n\n",
  "Cross-validation setup:\n",
  sprintf("  Total subjects: %d\n", nrow(subject_labels)),
  sprintf("  PTB subjects: %d (%.1f%%)\n", 
          sum(subject_labels$preterm == "1"),
          100 * mean(subject_labels$preterm == "1")),
  sprintf("  Number of outer folds: %d\n", nrow(cv_folds)),
  sprintf("  Inner split ratio: 70/30 (train/validation)\n"),
  sprintf("  Threshold optimization: Youden Index on inner validation\n"),
  sprintf("  Performance evaluation: Independent outer test folds\n\n"),
  sep = ""
)

# Verify fold sizes and PTB prevalence
for(i in 1:nrow(cv_folds)) {
  fold_data <- assessment(cv_folds$splits[[i]])
  cat(sprintf("Fold %d: n=%d subjects, PTB=%.1f%%\n", 
              i, nrow(fold_data), 
              100 * mean(fold_data$preterm == "1")))
}
```

## HELPER FUNCTIONS FOR NESTED CROSS-VALIDATION

**Purpose**: Define reusable functions for the nested CV workflow
This chunk establishes four critical functions used repeatedly across
60+ model training iterations (12 models × 5 folds):
1. detect_prob_col(): Automatically identify probability column names
2. apply_clr_transform(): Transform compositional microbiome data
3. optimize_threshold_cv(): Find optimal classification threshold
4. train_with_nested_cv(): Main nested CV training and evaluation

Encapsulating these operations in functions ensures:
- Consistency across all model evaluations
- Code maintainability and readability
- Reduced risk of copy-paste errors
- Easier debugging and modification


### FUNCTION 1: Detect Probability Column Name

**Purpose**: Automatically identify the column containing predicted probabilities
        for the positive class (preterm birth = "1")
        
**Background**: Different tidymodels engines use different naming conventions:
- glmnet: .pred_1
- ranger: .pred_1
- rpart: .pred_1

Some configurations may use: .pred_class_1, .pred_PTB, etc.
This function searches for the probability column using multiple patterns
to ensure compatibility across model types.


### FUNCTION 2: Apply Centered Log-Ratio (CLR) Transformation

**Purpose**: Transform compositional microbiome data for machine learning

**Background**: Microbiome relative abundances are compositional (constrained
to sum to 1), which violates assumptions of many ML algorithms. The CLR
transformation maps compositional data to unconstrained real space.

**Formula**: 
$$\mathrm{CLR}(x_i) = \log\left(\frac{x_i}{\mathrm{geometric\_mean}(x)}\right)$$


**Implementation notes**:
- Adds pseudocount (1e-6) to handle zeros
- Preserves ID columns (index, id)
- Renames transformed columns with "CLR_" prefix


### FUNCTION 3: Optimize Classification Threshold

**Purpose**: Find optimal probability threshold that maximizes Youden's Index
        on validation data (NOT test data)

**Background**: Default classification threshold (0.5) is often suboptimal,
especially with class imbalance. Youden's Index (J = Sens + Spec - 1)
provides a balanced criterion that accounts for both error types.

**Critical methodological point**: Threshold optimization MUST occur on a
validation set independent from the test set to prevent data leakage.
The nested CV design ensures threshold selection never touches test data.

**Args**:
 pred_df: data.frame with columns 'pred_prob' and 'true_class'
 method: optimization criterion ("youden", "f1", or "balanced_accuracy")

**Returns**:
 list with optimal threshold, sensitivity, specificity, and criterion value


## FUNCTION 4: Run ANCOM-BC2 on Fold Training Data


**Purpose:** Perform differential abundance analysis on outer training subjects
only, ensuring no data leakage into test set.

**Args:**
 otu_data: OTU table with taxa as columns (absolute abundances)
 meta_data: Metadata with subject IDs and clinical variables
 outer_train_subjects: Vector of subject IDs for training

**Returns:**
 Character vector of selected significant taxa names

### FUNCTION 5: Train and Evaluate Model with Nested Cross-Validation

**Purpose**: Main function implementing the complete nested CV workflow

**Nested CV structure**:
- OUTER LOOP (5 folds): Provides unbiased performance estimates
- INNER LOOP (70/30 split): Optimizes classification threshold

**Workflow for each outer fold**:
1. Split data into outer_train (34 subjects) and outer_test (9 subjects)
2. Further split outer_train into inner_train (23) and inner_val (11)
3. Optimize threshold on inner_val using Youden's Index
4. Apply optimized threshold to outer_test (never seen during optimization)
5. Calculate performance metrics on outer_test
6. Aggregate results across 5 folds

**Critical design elements preventing data leakage**:
- CLR transformation fitted only on inner_train, applied to val/test
- Threshold optimization uses inner_val (independent from outer_test)
- Recipe preprocessing steps fitted only on inner_train
- Random seeds fixed at each step for reproducibility

Args:
 model_name: string identifier for model type
 model_spec: parsnip model specification
 clinical_data_all: full clinical dataset (all subjects)
 microbiome_data_all: full microbiome dataset (all subjects)
 approach_name: feature selection strategy identifier
 microbiome_option: "ANCOM_Taxa" or "Full_Microbiome"
 cv_folds: rsample vfold_cv object with pre-defined splits
 use_pca: whether to apply PCA to microbiome features (default FALSE)
Returns:
 list containing:
   - model_name, approach, microbiome: identifiers
   - fold_results: tibble with metrics for each fold
   - summary: tibble with mean ± SD across folds
   
   

### RECIPE CREATION: Define preprocessing pipeline
======================================================================
The recipe specifies all preprocessing steps that will be applied
during model training. Key steps:
- Zero-variance filter: remove constant features
- Correlation filter: remove highly correlated features (r > 0.95)
- Imputation: median for numeric, mode for categorical
- Normalization: standardize numeric features to mean=0, sd=1
- Dummy encoding: convert categorical to binary indicators
- Near-zero-variance filter: remove features with minimal variation

```{r nested_cv_helper_functions}
# ============================================================================
# HELPER FUNCTIONS FOR NESTED CV
# ============================================================================

# FUNCTION 1: Detect Probability Column Name
detect_prob_col <- function(df, pos_class = "1") {
  if(".pred_1" %in% names(df)) return(".pred_1")
  pcol <- paste0(".pred_", pos_class)
  if(pcol %in% names(df)) return(pcol)
  pcols <- grep("^\\.pred_", names(df), value = TRUE)
  pcols <- setdiff(pcols, ".pred_class")
  if (length(pcols) > 0) {
    return(pcols[1])
  } else {
    stop("No probability column found in predictions.")
  }
}

# FUNCTION 2: Apply Centered Log-Ratio (CLR) Transformation
apply_clr_transform <- function(data, taxa_cols) {
  # Add pseudocount to handle zeros
  pseudocount <- 0.65
  
  data_clr <- data %>%
    mutate(across(all_of(taxa_cols), 
                  ~{
                    x_pseudo <- . + pseudocount
                    geometric_mean <- exp(mean(log(x_pseudo)))
                    log(x_pseudo / geometric_mean)
                  }))
  
  return(data_clr)
}

# FUNCTION 3: Optimize Classification Threshold
optimize_threshold_cv <- function(val_predictions, method = "youden") {
  # Optimizes classification threshold on validation set
  # 
  # Args:
  #   val_predictions: tibble with columns 'true_class' and 'pred_prob'
  #   method: optimization criterion ('youden' or 'f1')
  #   
  # Returns:
  #   list with optimal threshold and associated metrics
  
  # ROC curve on validation set
  roc_obj <- roc(val_predictions$true_class, 
                 val_predictions$pred_prob,
                 levels = c("0", "1"), 
                 direction = "auto",
                 quiet = TRUE)
  
  # Get all possible thresholds with their metrics
  coords_all <- coords(roc_obj, "all", 
                       ret = c("threshold", "sensitivity", "specificity"))
  
  if(method == "youden") {
    # Maximize Youden Index (J = Sensitivity + Specificity - 1)
    coords_all <- coords_all %>%
      mutate(criterion = sensitivity + specificity - 1)
  } else if(method == "f1") {
    # Alternative: Maximize F1 score
    coords_all <- coords_all %>%
      mutate(
        precision = sensitivity / (sensitivity + (1 - specificity) + 1e-10),
        criterion = 2 * (precision * sensitivity) / (precision + sensitivity + 1e-10)
      )
  }
  
  # Select optimal threshold
  optimal <- coords_all %>%
    filter(!is.na(criterion), !is.infinite(threshold)) %>%
    filter(criterion == max(criterion, na.rm = TRUE)) %>%
    slice(1)
  
  return(list(
    threshold = optimal$threshold,
    sensitivity = optimal$sensitivity,
    specificity = optimal$specificity,
    criterion_value = optimal$criterion
  ))
}

# FUNCTION 4: Run ANCOM-BC2 on Fold Training Data
run_ancombc_on_fold <- function(otu_data, meta_data, outer_train_subjects) {
  
  # Suppress ANCOM-BC2 verbose output
  suppressMessages({
    suppressWarnings({
      
      # Filter to training subjects only
      training_indices <- which(meta_data$id %in% outer_train_subjects)
      otu_table_train <- otu_data[training_indices, ]
      meta_data_train <- meta_data[training_indices, ]
      
      # Format metadata
      meta_data_train$preterm <- as.factor(meta_data_train$preterm)
      meta_data_train$id <- as.factor(meta_data_train$id)
      meta_data_train$edad_cronologicamujer <- as.numeric(meta_data_train$edad_cronologicamujer)
      meta_data_train$imc_pregestacional <- as.numeric(meta_data_train$imc_pregestacional)
      
      # Impute missing values (median for numeric)
      if(any(is.na(meta_data_train$edad_cronologicamujer))) {
        median_age <- median(meta_data_train$edad_cronologicamujer, na.rm = TRUE)
        meta_data_train$edad_cronologicamujer[is.na(meta_data_train$edad_cronologicamujer)] <- median_age
      }
      
      if(any(is.na(meta_data_train$imc_pregestacional))) {
        median_bmi <- median(meta_data_train$imc_pregestacional, na.rm = TRUE)
        meta_data_train$imc_pregestacional[is.na(meta_data_train$imc_pregestacional)] <- median_bmi
      }
      
      # Create phyloseq object
      meta_data_ps <- sample_data(meta_data_train)
      otu_table_t <- t(otu_table_train)
      otu_table_ps <- otu_table(otu_table_t, taxa_are_rows = TRUE)
      
      pseq_train <- phyloseq(otu_table_ps, meta_data_ps)
      
      # Aggregate by subject (account for repeated measures)
      pseq_agregado <- merge_samples(pseq_train, group = "id")
      
      # Reconstruct metadata (one row per subject)
      meta_sujetos <- as(sample_data(pseq_train), "data.frame") %>%
        distinct(id, .keep_all = TRUE)
      rownames(meta_sujetos) <- meta_sujetos$id
      sample_data(pseq_agregado) <- meta_sujetos
      
      # Run ANCOM-BC2
      formula_ancombc <- "preterm + edad_cronologicamujer + imc_pregestacional"
      
      ancombc_output <- ancombc2(
        data = pseq_agregado,
        fix_formula = formula_ancombc,
        p_adj_method = "BH",
        lib_cut = 1000,
        pseudo = 0,
        pseudo_sens = TRUE,
        prv_cut = 0.05,
        group = "preterm",
        struc_zero = FALSE,
        neg_lb = FALSE,
        alpha = 0.10,
        verbose = FALSE
      )
      
      # Extract results
      ancombc_res <- ancombc_output$res
      
      # Select significant taxa (p < 0.10)
      # EXCLUDE o__Chloroplast (likely artifact)
      significant_taxa <- ancombc_res %>%
        filter(
          p_preterm1 < 0.10,
          taxon != "o__Chloroplast"
        ) %>%
        arrange(p_preterm1) %>%
        pull(taxon)
      
      return(as.character(significant_taxa))
      
    })
  })
}

# FUNCTION 5: Function to train and evaluate model with nested CV
train_with_nested_cv <- function(model_name, model_spec,
                                  clinical_data_all, microbiome_data_all,
                                  approach_name, microbiome_option,
                                  cv_folds, use_pca = FALSE) {
  
  cat(sprintf("\n========================================\n"))
  cat(sprintf("MODEL: %s | %s | %s\n", 
              model_name, approach_name, microbiome_option))
  cat(sprintf("========================================\n"))
  
  # Store results from each fold
  fold_results <- list()
  
  # For Approach 3, we'll also store which variables were selected in each fold
  if(approach_name == "Approach3_DataDriven") {
    selected_variables_by_fold <- list()
  }
  
  # Iterate through outer CV folds
  for(fold_idx in 1:nrow(cv_folds)) {
    
    set.seed(123 + fold_idx)
    
    cat(sprintf("\n--- Outer Fold %d/%d ---\n", fold_idx, nrow(cv_folds)))
    
    # Get outer fold split (subject IDs)
    outer_train_subjects <- analysis(cv_folds$splits[[fold_idx]])$id
    outer_test_subjects <- assessment(cv_folds$splits[[fold_idx]])$id
    
    # ========================================================================
    # IMPROVED: UNIVARIATE SCREENING FOR APPROACH 3
    # ========================================================================
    if(approach_name == "Approach3_DataDriven") {
      
      cat("  Performing improved univariate screening...\n")
      
      # Filter to OUTER TRAINING subjects only
      clinical_for_screening <- clinical_data_all %>%
        filter(id %in% outer_train_subjects)
      
      # Get candidate variables
      candidate_vars_all <- clinical_for_screening %>%
        select(-index, -id, -preterm) %>%
        names()
      
      # STEP 1: Filter by data completeness
      completeness_stats <- calculate_completeness(
        clinical_for_screening, 
        candidate_vars_all
      )
      
      candidate_vars_complete <- filter_by_completeness(
        candidate_vars_all, 
        completeness_stats,
        subject_threshold = 80,  # ≥80% subjects
        sample_threshold = 70    # ≥70% samples
      )
      
      cat(sprintf("  After completeness filter: %d variables\n", 
                  length(candidate_vars_complete)))
      
      # STEP 2: Calculate univariate associations
      univariate_results_fold <- tibble(variable = candidate_vars_complete) %>%
        mutate(
          association = map(
            variable, 
            ~calculate_univariate_association(.x, clinical_for_screening)
          )
        ) %>%
        unnest_wider(association) %>%
        filter(!is.na(p_value)) %>%
        arrange(p_value)
      
      # STEP 3: Prioritize continuous variables
      # Order by: continuous → numeric → categorical, then by p-value
      vars_by_type <- prioritize_continuous(
        univariate_results_fold$variable,
        clinical_for_screening
      )
      
      univariate_results_fold <- univariate_results_fold %>%
        mutate(priority_rank = match(variable, vars_by_type)) %>%
        arrange(priority_rank, p_value)
      
      # STEP 4: Select top variables (liberal p-value threshold)
      # Take top 15 candidates to allow for collinearity removal
      p_threshold <- 0.30  # Liberal threshold
      max_candidates <- 15
      
      top_candidates <- univariate_results_fold %>%
        filter(p_value < p_threshold) %>%
        head(max_candidates) %>%
        pull(variable)
      
      cat(sprintf("  Top candidates (p<%.2f): %d variables\n", 
                  p_threshold, length(top_candidates)))
      
      # STEP 5: Detect and resolve collinearity
      collinear_pairs <- detect_collinear_vars(
        clinical_for_screening,
        top_candidates,
        threshold = 0.95
      )
      
      if(nrow(collinear_pairs) > 0) {
        cat(sprintf("  Found %d collinear pairs (|r|>0.85)\n", 
                    nrow(collinear_pairs)))
        
        # Resolve by priority (continuous > others, lower p-value)
        candidate_vars_ordered <- univariate_results_fold %>%
          filter(variable %in% top_candidates) %>%
          arrange(priority_rank, p_value) %>%
          pull(variable)
        
        top_candidates <- resolve_collinearity(
          candidate_vars_ordered,
          collinear_pairs
        )
      }
      
      # STEP 6: Select final 10 variables
      max_features <- 10
      approach3_vars_fold <- head(top_candidates, max_features)
      
      cat(sprintf("  Final selection: %d variables\n", length(approach3_vars_fold)))
      
      # Show selected variables with details
      selected_info <- univariate_results_fold %>%
        filter(variable %in% approach3_vars_fold) %>%
        arrange(match(variable, approach3_vars_fold))
      
      cat("\n  Selected variables:\n")
      for(i in 1:nrow(selected_info)) {
        var_type <- if(is.numeric(clinical_for_screening[[selected_info$variable[i]]])) {
          n_unique <- length(unique(na.omit(clinical_for_screening[[selected_info$variable[i]]])))
          if(n_unique > 10) "continuous" else "numeric"
        } else "categorical"
        
        cat(sprintf("    %2d. %-30s p=%.4f  (%s)\n",
                    i,
                    selected_info$variable[i],
                    selected_info$p_value[i],
                    var_type))
      }
      cat("\n")
      
      # Store selected variables for this fold
      selected_variables_by_fold[[fold_idx]] <- selected_info %>%
       select(variable, p_value, coefficient, n_complete) %>%
       mutate(fold = fold_idx)
      
      # Create clinical dataset with ONLY selected variables
      clinical_data_fold <- clinical_data_all %>%
        select(index, id, all_of(approach3_vars_fold), preterm)
      
    } else {
      # For Approaches 1 and 2, use pre-defined clinical data
      clinical_data_fold <- clinical_data_all
    }
    
    # ========================================================================
    # ANCOM-BC2 ANALYSIS FOR MICROBIOME TAXA SELECTION
    # ========================================================================
    if(microbiome_option == "ANCOM_Taxa") {
      
      cat("  Performing ANCOM-BC2 on outer training subjects...\n")
      
      # Load absolute abundance data
      abs_file <- "C:/Users/marti/Documents/Datos_mexicanos/genus_rel_filtered_2025-05-25_abs.csv"
      
      if(!file.exists(abs_file)) {
        cat("  ERROR: Absolute abundance file not found\n")
        cat("  Path:", abs_file, "\n")
        fold_results[[fold_idx]] <- NULL
        next
      }
      
      full_data_abs <- read.csv(abs_file, row.names = 98)
      otu_table_raw <- full_data_abs[, 1:97]
      meta_data_raw <- full_data_abs[, 98:166]
      
      # Run ANCOM-BC2 on this fold's training subjects
      selected_taxa_fold <- tryCatch({
        run_ancombc_on_fold(otu_table_raw, meta_data_raw, outer_train_subjects)
      }, error = function(e) {
        cat("  ERROR in ANCOM-BC2:", e$message, "\n")
        return(character(0))
      })
      
      # Handle case where no taxa are significant
      if(length(selected_taxa_fold) == 0) {
        cat("  WARNING: No significant taxa found in this fold\n")
        cat("  Using top 5 taxa by mean abundance as fallback\n")
        
        # Fallback: use top abundant taxa in training set
        train_micro_data <- microbiome_data_all %>%
          filter(id %in% outer_train_subjects)
        
        taxa_cols_all <- setdiff(names(microbiome_data_all), 
                                  c("index", "id", "shannon_diversity"))
        
        taxa_abundance <- train_micro_data %>%
          select(all_of(taxa_cols_all)) %>%
          summarize(across(everything(), ~mean(., na.rm = TRUE))) %>%
          pivot_longer(everything(), names_to = "taxon", values_to = "mean_abundance") %>%
          filter(taxon != "o__Chloroplast") %>%  # Exclude chloroplast
          arrange(desc(mean_abundance))
        
        selected_taxa_fold <- head(taxa_abundance$taxon, 5)
      }
      
      cat(sprintf("  Selected %d ANCOM taxa for this fold\n", length(selected_taxa_fold)))
      cat("  Taxa:", paste(selected_taxa_fold, collapse = ", "), "\n")
      
      # CRITICAL: Create microbiome_data_fold with ONLY selected taxa
      microbiome_data_fold <- microbiome_data_all %>%
        select(index, id, shannon_diversity, all_of(selected_taxa_fold))
      
      cat(sprintf("  Created microbiome_data_fold: %d samples × %d taxa\n",
                  nrow(microbiome_data_fold),
                  length(selected_taxa_fold)))
      
      # Store selected taxa for this fold (for later analysis)
      if(!exists("selected_taxa_by_fold")) {
        selected_taxa_by_fold <- list()
      }
      selected_taxa_by_fold[[fold_idx]] <- tibble(
        fold = fold_idx,
        taxon = selected_taxa_fold
      )
      
    } else {
      # Full Microbiome - clean up microbiome_data_fold if it exists
      if(exists("microbiome_data_fold")) {
        rm(microbiome_data_fold)
      }
    }
    # ========================================================================
    
    # Further split outer training into inner train/validation (70/30)
    outer_train_labels <- subject_labels %>% 
      filter(id %in% outer_train_subjects)
    
    set.seed(123 + fold_idx + 1000)
    inner_split <- initial_split(outer_train_labels, prop = 0.70, strata = preterm)
    inner_train_subjects <- training(inner_split)$id
    inner_val_subjects <- testing(inner_split)$id
    
    cat(sprintf("  Inner train: %d subjects | Inner val: %d subjects | Outer test: %d subjects\n",
                length(inner_train_subjects), 
                length(inner_val_subjects),
                length(outer_test_subjects)))
    
    # ========================================================================
    # PREPARE DATA (COMBINE CLINICAL + MICROBIOME)
    # ========================================================================
    # Use microbiome_data_fold if it exists (ANCOM path), otherwise microbiome_data_all
    if(exists("microbiome_data_fold")) {
      microbiome_to_use <- microbiome_data_fold
      cat("  Using ANCOM-selected taxa:", ncol(microbiome_data_fold) - 2, "genera\n")
    } else {
      microbiome_to_use <- microbiome_data_all
      cat("  Using full microbiome:", ncol(microbiome_data_all) - 2, "genera\n")
    }
    
    full_data <- clinical_data_fold %>%
      left_join(microbiome_to_use, by = c("index", "id")) %>%
      mutate(preterm = factor(preterm, levels = c("0", "1")))
    
    inner_train_data <- full_data %>% filter(id %in% inner_train_subjects)
    inner_val_data <- full_data %>% filter(id %in% inner_val_subjects)
    outer_test_data <- full_data %>% filter(id %in% outer_test_subjects)
    
    # ========================================================================
    # IDENTIFY TAXA COLUMNS (CRITICAL: BASED ON ACTUAL DATA)
    # ========================================================================
    # Get clinical variable names from clinical_data_fold
    clinical_var_names <- setdiff(names(clinical_data_fold), 
                                   c("index", "id", "preterm"))
    
    # Taxa cols = all columns EXCEPT: index, id, shannon_diversity, preterm, clinical vars
    taxa_cols <- setdiff(
      names(full_data),
      c("index", "id", "shannon_diversity", "preterm", clinical_var_names)
    )
    
    cat(sprintf("  Taxa columns identified for CLR: %d\n", length(taxa_cols)))
    
    # ========================================================================
    # APPLY CLR TRANSFORMATION
    # ========================================================================
    
    if(length(taxa_cols) > 0) {
      inner_train_data <- apply_clr_transform(inner_train_data, taxa_cols)
      inner_val_data <- apply_clr_transform(inner_val_data, taxa_cols)
      outer_test_data <- apply_clr_transform(outer_test_data, taxa_cols)
    }
    
    # Create recipe
    if(use_pca && length(taxa_cols) > 10) {
      rec <- recipe(preterm ~ ., data = inner_train_data) %>%
        update_role(index, id, new_role = "ID") %>%
        step_zv(all_predictors()) %>%
        step_corr(all_numeric_predictors(), threshold = 0.95) %>%
        step_novel(all_nominal_predictors(), new_level = "(new)") %>%
        step_other(all_nominal_predictors(), threshold = 0.01) %>%
        step_impute_median(all_numeric_predictors()) %>%
        step_impute_mode(all_nominal_predictors()) %>%
        step_normalize(all_numeric_predictors()) %>%
        step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
        step_pca(starts_with("CLR_") | starts_with("Genus_"), 
                 num_comp = min(20, length(taxa_cols)), prefix = "PC_genus_") %>%
        step_nzv(all_predictors())
    } else {
      rec <- recipe(preterm ~ ., data = inner_train_data) %>%
        update_role(index, id, new_role = "ID") %>%
        step_zv(all_predictors()) %>%
        step_corr(all_numeric_predictors(), threshold = 0.95) %>%
        step_novel(all_nominal_predictors(), new_level = "(new)") %>%
        step_other(all_nominal_predictors(), threshold = 0.01) %>%
        step_impute_median(all_numeric_predictors()) %>%
        step_impute_mode(all_nominal_predictors()) %>%
        step_normalize(all_numeric_predictors()) %>%
        step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
        step_nzv(all_predictors())
    }
    
    # Create workflow and fit on inner training
    wf <- workflow() %>%
      add_recipe(rec) %>%
      add_model(model_spec)
    
    set.seed(123 + fold_idx + 2000)
    
    final_fit <- tryCatch({
      fit(wf, data = inner_train_data)
    }, error = function(e) {
      cat("  ERROR in fitting:", e$message, "\n")
      return(NULL)
    })
    
    if(is.null(final_fit)) {
      fold_results[[fold_idx]] <- NULL
      next
    }
    
    # ========================================================================
    # STEP 1: Optimize threshold on INNER VALIDATION set
    # ========================================================================
    
    val_preds_prob <- predict(final_fit, new_data = inner_val_data, type = "prob")
    prob_col <- detect_prob_col(val_preds_prob)
    
    val_preds_subject <- inner_val_data %>%
      select(id, preterm) %>%
      bind_cols(val_preds_prob) %>%
      group_by(id) %>%
      summarize(
        pred_prob = mean(!!sym(prob_col), na.rm = TRUE),
        true_class = first(preterm),
        .groups = "drop"
      ) %>%
      mutate(true_class = factor(true_class, levels = c("0", "1")))
    
    threshold_opt <- optimize_threshold_cv(val_preds_subject, method = "youden")
    
    cat(sprintf("  Optimal threshold (inner validation): %.4f (Youden=%.3f)\n", 
                threshold_opt$threshold, threshold_opt$criterion_value))
    
    # ========================================================================
    # STEP 2: Apply optimized threshold to OUTER TEST fold
    # ========================================================================
    
    test_preds_prob <- predict(final_fit, new_data = outer_test_data, type = "prob")
    test_prob_col <- detect_prob_col(test_preds_prob)
    
    if(nrow(test_preds_prob) == 0) {
      cat("  ERROR: No predictions generated for test set\n")
      fold_results[[fold_idx]] <- NULL
      next
    }
    
    test_combined <- outer_test_data %>%
      select(id, preterm) %>%
      bind_cols(test_preds_prob)
    
    if(nrow(test_combined) == 0) {
      cat("  ERROR: bind_cols produced 0 rows\n")
      fold_results[[fold_idx]] <- NULL
      next
    }
    
    test_preds_subject <- test_combined %>%
      group_by(id) %>%
      summarize(
        pred_prob = mean(!!sym(test_prob_col), na.rm = TRUE),
        true_class = first(preterm),
        .groups = "drop"
      )
    
    if(nrow(test_preds_subject) == 0) {
      cat("  ERROR: Subject aggregation produced 0 subjects\n")
      fold_results[[fold_idx]] <- NULL
      next
    }
    
    test_preds_subject <- test_preds_subject %>%
      mutate(
        true_class = factor(true_class, levels = c("0", "1")),
        pred_class = factor(
          ifelse(pred_prob >= threshold_opt$threshold, "1", "0"),
          levels = c("0", "1")
        )
      )
    
    # Calculate metrics
    acc <- accuracy(test_preds_subject, truth = true_class, estimate = pred_class)$.estimate
    sens <- sens(test_preds_subject, truth = true_class, estimate = pred_class,
                 event_level = "second")$.estimate
    spec <- spec(test_preds_subject, truth = true_class, estimate = pred_class,
                 event_level = "second")$.estimate
    
    bal_acc <- (sens + spec) / 2
    
    roc_test <- roc(test_preds_subject$true_class, 
                    test_preds_subject$pred_prob,
                    levels = c("0", "1"), 
                    direction = "auto",
                    quiet = TRUE)
    auroc <- auc(roc_test)
    
    pr_data_input <- test_preds_subject %>%
      rename(.pred_1 = pred_prob)
    prauc <- pr_auc(pr_data_input, truth = true_class, .pred_1)$.estimate
    
    cat(sprintf("  Outer test metrics: AUROC=%.3f, Sens=%.3f, Spec=%.3f, Acc=%.3f\n", 
                auroc, sens, spec, acc))
    
    # Store fold results
    fold_results[[fold_idx]] <- tibble(
      fold = fold_idx,
      threshold = threshold_opt$threshold,
      n_test = nrow(test_preds_subject),
      n_ptb = sum(test_preds_subject$true_class == "1"),
      AUROC = as.numeric(auroc),
      PRAUC = prauc,
      Accuracy = acc,
      Balanced_Accuracy = bal_acc,
      Sensitivity = sens,
      Specificity = spec,
      Youden = sens + spec - 1
    )
    
        # ========================================================================
    # STORE ROC AND PR CURVES
    # ========================================================================
    
    # ROC curve
    roc_obj <- roc(test_preds_subject$true_class, 
                   test_preds_subject$pred_prob,
                   levels = c("0", "1"), 
                   direction = "auto")
    
    if(!exists("roc_curves_storage")) {
      roc_curves_storage <- list()
    }
    
    roc_curves_storage[[fold_idx]] <- tibble(
      fold = fold_idx,
      sensitivity = roc_obj$sensitivities,
      specificity = roc_obj$specificities,
      threshold = roc_obj$thresholds
    )
    
    # PR curve
    pr_obj <- pr.curve(
      scores.class0 = test_preds_subject$pred_prob[test_preds_subject$true_class == "1"],
      scores.class1 = test_preds_subject$pred_prob[test_preds_subject$true_class == "0"],
      curve = TRUE
    )
    
    if(!exists("pr_curves_storage")) {
      pr_curves_storage <- list()
    }
    
    pr_curves_storage[[fold_idx]] <- tibble(
      fold = fold_idx,
      recall = pr_obj$curve[, 1],
      precision = pr_obj$curve[, 2],
      threshold = pr_obj$curve[, 3]
    )
    
    cat(sprintf("  ✓ ROC curve: %d points | PR curve: %d points\n", 
                length(roc_obj$sensitivities),
                nrow(pr_obj$curve)))
    
    # ========================================================================
    # CLEANUP: Remove fold-specific variables
    # ========================================================================
    if(exists("microbiome_data_fold")) {
      rm(microbiome_data_fold)
    }
    if(exists("microbiome_to_use")) {
      rm(microbiome_to_use)
    }
    # ========================================================================
    
  }  # End of fold loop
  
  # Aggregate results across folds
  fold_results_df <- bind_rows(fold_results)
  
  if(nrow(fold_results_df) == 0) {
    return(NULL)
  }
  
  # Calculate mean and SD across folds
  summary_results <- fold_results_df %>%
    summarize(
      n_folds = n(),
      across(c(threshold, AUROC, PRAUC, Accuracy, Balanced_Accuracy, 
               Sensitivity, Specificity, Youden),
             list(mean = ~mean(., na.rm = TRUE),
                  sd = ~sd(., na.rm = TRUE)),
             .names = "{.col}_{.fn}")
    )
  
  cat("\n=== CROSS-VALIDATION SUMMARY ===\n")
  cat(sprintf("  AUROC: %.3f ± %.3f\n", 
              summary_results$AUROC_mean, summary_results$AUROC_sd))
  cat(sprintf("  Sensitivity: %.3f ± %.3f\n", 
              summary_results$Sensitivity_mean, summary_results$Sensitivity_sd))
  cat(sprintf("  Specificity: %.3f ± %.3f\n", 
              summary_results$Specificity_mean, summary_results$Specificity_sd))
  cat(sprintf("  Accuracy: %.3f ± %.3f\n", 
              summary_results$Accuracy_mean, summary_results$Accuracy_sd))
  cat(sprintf("  Threshold: %.3f ± %.3f\n", 
              summary_results$threshold_mean, summary_results$threshold_sd))
  
    # ========================================================================
  # TRAIN FINAL MODEL ON FULL DATA
  # ========================================================================
  
  cat("\n╔════════════════════════════════════════╗\n")
  cat("║  TRAINING FINAL MODEL (Full Dataset)  ║\n")
  cat("╚════════════════════════════════════════╝\n\n")
  
  # Use appropriate clinical data
  if(approach_name == "Approach3_DataDriven") {
    clinical_data_full <- clinical_data_all  # All variables for Approach 3
  } else {
    clinical_data_full <- clinical_data_all
  }
  
  # ========================================================================
  # CRITICAL FIX: Use correct microbiome based on option
  # ========================================================================
  
  if(microbiome_option == "ANCOM_Taxa") {
    cat("  Microbiome: ANCOM Taxa (using fold-selected taxa)\n")
    
    # Get taxa selected across folds
    if(exists("selected_taxa_by_fold") && length(selected_taxa_by_fold) > 0) {
      
      # =================================================================
      # CRITICAL FIX: Convert list to tibble first
      # =================================================================
      selected_taxa_df <- bind_rows(selected_taxa_by_fold)
      # =================================================================
      
      # Count frequency across folds
      taxa_freq <- selected_taxa_df %>%
        group_by(taxon) %>%
        summarize(n_folds = n(), .groups = "drop") %>%
        arrange(desc(n_folds))
      
      cat(sprintf("  Taxa selected across %d folds:\n", 
                  length(unique(selected_taxa_df$fold))))
      print(taxa_freq)
      
      # Use taxa in ≥3 folds (majority)
      n_total_folds <- length(unique(selected_taxa_df$fold))
      min_folds <- max(3, ceiling(n_total_folds / 2))
      
      selected_taxa_final <- taxa_freq %>%
        filter(n_folds >= min_folds) %>%
        pull(taxon)
      
      if(length(selected_taxa_final) == 0) {
        cat("  WARNING: No taxa in majority of folds, using all\n")
        selected_taxa_final <- unique(selected_taxa_df$taxon)
      }
      
      cat(sprintf("  Using %d ANCOM taxa (≥%d folds):\n", 
                  length(selected_taxa_final), min_folds))
      cat(paste("   ", selected_taxa_final, collapse = "\n"), "\n")
      
      # Filter microbiome to only these taxa
      available_taxa <- intersect(selected_taxa_final, names(microbiome_data_all))
      
      if(length(available_taxa) == 0) {
        stop("ERROR: No ANCOM taxa found in microbiome_data_all")
      }
      
      microbiome_data_full <- microbiome_data_all %>%
        select(index, id, shannon_diversity, all_of(available_taxa))
      
      cat(sprintf("  ✓ Final model: %d ANCOM taxa\n\n", length(available_taxa)))
      
    } else {
      cat("  WARNING: No selected_taxa_by_fold found\n")
      cat("  Using full microbiome as fallback\n")
      microbiome_data_full <- microbiome_data_all
    }
    
  } else {
    # Full microbiome
    cat("  Microbiome: Full Microbiome (all taxa)\n")
    microbiome_data_full <- microbiome_data_all
  }
  # ========================================================================
  
  # Combine
  full_data_final <- clinical_data_full %>%
    left_join(microbiome_data_full, by = c("index", "id")) %>%
    mutate(preterm = factor(preterm, levels = c("0", "1")))
  
  # Identify taxa columns
  clinical_var_names_full <- setdiff(names(clinical_data_full), 
                                      c("index", "id", "preterm"))
  
  taxa_cols_full <- setdiff(
    names(full_data_final),
    c("index", "id", "shannon_diversity", "preterm", clinical_var_names_full)
  )
  
  cat(sprintf("  Clinical variables: %d\n", length(clinical_var_names_full)))
  cat(sprintf("  Microbiome taxa: %d\n", length(taxa_cols_full)))
  
  # Apply CLR
  if(length(taxa_cols_full) > 0) {
    full_data_final <- apply_clr_transform(full_data_final, taxa_cols_full)
  }
  
  # Create recipe (same structure as in folds)
  if(use_pca && length(taxa_cols_full) > 10) {
    rec_final <- recipe(preterm ~ ., data = full_data_final) %>%
      update_role(index, id, new_role = "ID") %>%
      step_zv(all_predictors()) %>%
      step_corr(all_numeric_predictors(), threshold = 0.95) %>%
      step_novel(all_nominal_predictors(), new_level = "(new)") %>%
      step_other(all_nominal_predictors(), threshold = 0.01) %>%
      step_impute_median(all_numeric_predictors()) %>%
      step_impute_mode(all_nominal_predictors()) %>%
      step_normalize(all_numeric_predictors()) %>%
      step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
      step_pca(starts_with("CLR_") | starts_with("Genus_"), 
               num_comp = min(20, length(taxa_cols_full)), 
               prefix = "PC_genus_") %>%
      step_nzv(all_predictors())
  } else {
    rec_final <- recipe(preterm ~ ., data = full_data_final) %>%
      update_role(index, id, new_role = "ID") %>%
      step_zv(all_predictors()) %>%
      step_corr(all_numeric_predictors(), threshold = 0.95) %>%
      step_novel(all_nominal_predictors(), new_level = "(new)") %>%
      step_other(all_nominal_predictors(), threshold = 0.01) %>%
      step_impute_median(all_numeric_predictors()) %>%
      step_impute_mode(all_nominal_predictors()) %>%
      step_normalize(all_numeric_predictors()) %>%
      step_dummy(all_nominal_predictors(), one_hot = FALSE) %>%
      step_nzv(all_predictors())
  }
  
  # Fit final model
  wf_final <- workflow() %>%
    add_recipe(rec_final) %>%
    add_model(model_spec)
  
  set.seed(42)  # Reproducibility
  
  final_model_fit <- tryCatch({
    fit(wf_final, data = full_data_final)
  }, error = function(e) {
    cat("  ✗ ERROR:", e$message, "\n")
    return(NULL)
  })
  
  if(!is.null(final_model_fit)) {
    cat("  ✓ Final model trained successfully\n")
  } else {
    cat("  ✗ Final model training failed\n")
  }
  
  # ========================================================================
  # RETURN COMPREHENSIVE RESULTS
  # ========================================================================
  
  cat("\n╔════════════════════════════════════════╗\n")
  cat("║      PACKAGING RESULTS                 ║\n")
  cat("╚════════════════════════════════════════╝\n\n")
  
  result_list <- list(
    model_name = model_name,
    approach = approach_name,
    microbiome = microbiome_option,
    fold_results = fold_results_df,
    summary = summary_results
  )
  
  # Add ROC curves
  if(exists("roc_curves_storage") && length(roc_curves_storage) > 0) {
    result_list$roc_curves <- bind_rows(roc_curves_storage)
    cat(sprintf("  ✓ ROC curves: %d folds, %d total points\n",
                length(roc_curves_storage),
                nrow(result_list$roc_curves)))
  } else {
    cat("  ✗ WARNING: No ROC curves stored\n")
  }
  
  # Add PR curves
  if(exists("pr_curves_storage") && length(pr_curves_storage) > 0) {
    result_list$pr_curves <- bind_rows(pr_curves_storage)
    cat(sprintf("  ✓ PR curves: %d folds, %d total points\n",
                length(pr_curves_storage),
                nrow(result_list$pr_curves)))
  } else {
    cat("  ✗ WARNING: No PR curves stored\n")
  }
  
  # Add selected variables (Approach 3)
  if(approach_name == "Approach3_DataDriven" && exists("selected_variables_by_fold")) {
    result_list$selected_variables <- bind_rows(selected_variables_by_fold)
    cat(sprintf("  ✓ Selected variables: %d folds\n",
                length(selected_variables_by_fold)))
  }
  
  # Add selected taxa (ANCOM)
  if(microbiome_option == "ANCOM_Taxa" && exists("selected_taxa_by_fold")) {
    result_list$selected_taxa <- bind_rows(selected_taxa_by_fold)
    cat(sprintf("  ✓ Selected taxa: %d folds\n",
                length(selected_taxa_by_fold)))
  }
  
  # Add final model
  if(exists("final_model_fit") && !is.null(final_model_fit)) {
    result_list$final_model <- final_model_fit
    cat("  ✓ Final model object stored\n")
  } else {
    cat("  ✗ WARNING: No final model stored\n")
  }
  
  cat("\n")
  return(result_list)
  
}  # End of train_with_nested_cv function
```



## COMPREHENSIVE NESTED CROSS-VALIDATION ANALYSIS

**Purpose**: Execute nested CV for all model combinations and generate unbiased
        performance estimates

This chunk represents the main computational analysis, evaluating:
- 2 algorithms: Random Forest, Elastic Net (glmnet)
- 3 feature selection approaches: DREAM, Literature-based, Data-driven
- 2 microbiome representations: ANCOM-selected taxa, Full profile
- Total: 12 model combinations

For each combination:
- 5-fold cross-validation at subject level
- Threshold optimization on inner validation sets
- Performance evaluation on independent test folds
- Total model training iterations: 60 (12 models × 5 folds)

Computational requirements:
- Estimated runtime: 2-3 hours on standard hardware
- Memory: ~4GB RAM
- Processor: Multi-core recommended (parallel processing not implemented)

Output:
- cv_results_all: list containing results for all 12 models
- Each model includes fold-level results and summary statistics

```{r run_nested_cv, cache=FALSE, message=FALSE, warning=FALSE}
# ============================================================================
# COMPREHENSIVE NESTED CV ANALYSIS - FINAL VERSION
# ============================================================================
# Configuration: 3 models × 3 approaches × 2 microbiomes = 18 combinations
# Estimated completion time: 2.5-3 hours
# ============================================================================

cat("╔════════════════════════════════════════════════════════════════════╗\n",
    "║     COMPREHENSIVE NESTED CROSS-VALIDATION ANALYSIS                ║\n",
    "╚════════════════════════════════════════════════════════════════════╝\n\n")

# ============================================================================
# DEFINE MODEL CONFIGURATIONS
# ============================================================================

models_to_run <- list(
  list(name = "rf_base", spec = rf_spec, pca = FALSE),
  list(name = "glmnet_base", spec = glmnet_spec, pca = FALSE)
)

# ============================================================================
# DEFINE APPROACH CONFIGURATIONS
# ============================================================================

approaches_config <- list(
  list(
    name = "Approach1_DREAM",
    clinical = approach1_clinical_all
  ),
  list(
    name = "Approach2_Literature",
    clinical = approach2_clinical_all
  ),
  list(
    name = "Approach3_DataDriven",
    clinical = approach3_clinical_all
  )
)

# ============================================================================
# DEFINE MICROBIOME CONFIGURATIONS
# ============================================================================

# NOTE: For ANCOM_Taxa, taxa are selected WITHIN each fold
#       We pass full data here, but it will be filtered per fold

microbiome_config <- list(
  list(
    name = "ANCOM_Taxa",
    data = micro_genus_full,  # Base data (taxa selected per fold)
    use_ancom = TRUE          # Flag to trigger ANCOM analysis
  ),
  list(
    name = "Full_Microbiome",
    data = micro_genus_full,
    use_ancom = FALSE         # Use all taxa as-is
  )
)

cat(
  "✓ Models configured: ", paste(sapply(models_to_run, `[[`, "name"), collapse = ", "), "\n\n",
  "✓ Approaches configured: ", paste(sapply(approaches_config, `[[`, "name"), collapse = ", "), "\n\n",
  "✓ Microbiome options: ", "2", "\n\n",
  sep = ""
)

# ============================================================================
# RUN NESTED CV FOR ALL COMBINATIONS
# ============================================================================

cv_results_all <- list()
result_idx <- 1
successful_models <- 0
failed_models <- 0
total_combinations <- length(models_to_run) * length(approaches_config) * 2

start_time <- Sys.time()

for(model in models_to_run) {
  for(approach in approaches_config) {
    for(microbiome in microbiome_config) {
      
cat(
  sprintf("\n┌─────────────────────────────────────────────────────────────┐\n"),
  sprintf("│ [%2d/%2d] %s | %s | %s\n", 
          result_idx, total_combinations,
          model$name, approach$name, microbiome$name),
  sprintf("└─────────────────────────────────────────────────────────────┘\n"),
  sep = ""
)
      
      result <- train_with_nested_cv(
        model_name = model$name,
        model_spec = model$spec,
        clinical_data_all = approach$clinical,
        microbiome_data_all = microbiome$data,
        approach_name = approach$name,
        microbiome_option = microbiome$name,
        cv_folds = cv_folds,
        use_pca = model$pca
      )
      
      if(!is.null(result)) {
        cv_results_all[[result_idx]] <- result
        successful_models <- successful_models + 1
        cat(sprintf("\n✓ Model completed successfully\n"))
      } else {
        failed_models <- failed_models + 1
        cat(sprintf("\n✗ Model failed - check error messages above\n"))
      }
      
      result_idx <- result_idx + 1
      
      # Progress update with time estimates
      elapsed <- as.numeric(difftime(Sys.time(), start_time, units = "mins"))
      avg_time_per_model <- elapsed / (successful_models + failed_models)
      remaining_models <- total_combinations - result_idx + 1
      estimated_remaining <- avg_time_per_model * remaining_models
      
      cat(
  sprintf("\n┌─────────────────────────────────────────────────────────────┐\n"),
  sprintf("│ PROGRESS SUMMARY                                            │\n"),
  sprintf("│ Completed: %2d/%2d (%.1f%%)                                 │\n", 
          result_idx - 1, total_combinations, 
          100 * (result_idx - 1) / total_combinations),
  sprintf("│ Successful: %2d | Failed: %2d                               │\n", 
          successful_models, failed_models),
  sprintf("│ Elapsed time: %.1f min                                      │\n", elapsed),
  sprintf("│ Est. remaining: %.1f min                                    │\n", estimated_remaining),
  sprintf("│ Est. total time: %.1f min                                   │\n", elapsed + estimated_remaining),
  sprintf("└─────────────────────────────────────────────────────────────┘\n"),
  sep = ""
)
    }
  }
}

end_time <- Sys.time()
total_time <- as.numeric(difftime(end_time, start_time, units = "mins"))

# ============================================================================
# FINAL SUMMARY
# ============================================================================

cat(
  "\n\n",
  "╔════════════════════════════════════════════════════════════════════╗\n",
  "║                    ANALYSIS COMPLETED                              ║\n",
  "╚════════════════════════════════════════════════════════════════════╝\n\n",
  sprintf("Total combinations attempted: %d\n", total_combinations),
  sprintf("Successful models: %d\n", successful_models),
  sprintf("Failed models: %d\n", failed_models),
  sprintf("Total execution time: %.1f minutes (%.1f hours)\n", total_time, total_time/60),
  sprintf("\nStart time: %s\n", format(start_time, "%Y-%m-%d %H:%M:%S")),
  sprintf("End time: %s\n", format(end_time, "%Y-%m-%d %H:%M:%S")),
  sep = ""
)
```

## Variable Selection Visualization (Approach 3)

**Purpose:** Visualize the frequency and stability of variable selection across
the 5 cross-validation folds for the data-driven approach.

```{r approach3_variable_visualization, fig.width=12, fig.height=8}
# Extract variable selection data for Approach 3
approach3_selection_data <- list()

for(result in cv_results_all) {
  if(!is.null(result) && 
     result$approach == "Approach3_DataDriven" &&
     !is.null(result$selected_variables)) {
    
    model_id <- paste(result$model_name, result$microbiome, sep = "_")
    approach3_selection_data[[model_id]] <- result$selected_variables
  }
}

if(length(approach3_selection_data) > 0) {
  
  # Combine all selections across models
  all_selections_approach3 <- bind_rows(
    approach3_selection_data, 
    .id = "model_config"
  )
  
  # Calculate selection frequency
  var_frequency_approach3 <- all_selections_approach3 %>%
    group_by(variable) %>%
    summarize(
      n_folds_selected = n_distinct(fold),
      total_folds = 5,
      selection_rate = n_folds_selected / 5 * 100,
      mean_p_value = mean(p_value, na.rm = TRUE),
      min_p_value = min(p_value, na.rm = TRUE),
      max_p_value = max(p_value, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    arrange(desc(n_folds_selected), mean_p_value)
  
  # ========================================================================
  # VISUALIZATION 1: Selection Frequency Bar Plot
  # ========================================================================
  
  p1 <- var_frequency_approach3 %>%
    mutate(
      selection_category = case_when(
        n_folds_selected == 5 ~ "All 5 folds (100%)",
        n_folds_selected >= 4 ~ "4-5 folds (80-100%)",
        n_folds_selected >= 3 ~ "3 folds (60%)",
        n_folds_selected >= 2 ~ "2 folds (40%)",
        TRUE ~ "1 fold (20%)"
      ),
      selection_category = factor(
        selection_category,
        levels = c("All 5 folds (100%)", "4-5 folds (80-100%)", 
                   "3 folds (60%)", "2 folds (40%)", "1 fold (20%)")
      )
    ) %>%
    ggplot(aes(x = reorder(variable, n_folds_selected), 
               y = n_folds_selected)) +
    geom_col(aes(fill = selection_category), alpha = 0.8) +
    geom_hline(yintercept = 5, linetype = "dashed", 
               color = "red", size = 1) +
    geom_hline(yintercept = 4, linetype = "dashed", 
               color = "orange", size = 0.5, alpha = 0.5) +
    scale_fill_manual(
      name = "Selection Stability",
      values = c(
        "All 5 folds (100%)" = "#1B5E20",
        "4-5 folds (80-100%)" = "#388E3C",
        "3 folds (60%)" = "#FBC02D",
        "2 folds (40%)" = "#F57C00",
        "1 fold (20%)" = "#D32F2F"
      )
    ) +
    coord_flip() +
    labs(
      title = "Variable Selection Frequency Across CV Folds (Approach 3)",
      subtitle = "Data-driven univariate screening performed independently in each fold",
      x = "Clinical Variable",
      y = "Number of Folds Selected (out of 5)",
      caption = "Red dashed line: selected in all folds | Orange dashed line: selected in 4+ folds"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      legend.position = "bottom",
      plot.title = element_text(face = "bold", size = 14),
      axis.text.y = element_text(size = 10)
    )
  
  print(p1)
  
  # ========================================================================
  # VISUALIZATION 2: P-value Ranges with Selection Frequency
  # ========================================================================
  
  p2 <- var_frequency_approach3 %>%
    filter(n_folds_selected >= 2) %>%  # Show variables selected in 2+ folds
    ggplot(aes(x = reorder(variable, mean_p_value), 
               y = -log10(mean_p_value))) +
    geom_col(aes(fill = n_folds_selected), alpha = 0.7) +
    geom_errorbar(
      aes(ymin = -log10(max_p_value), 
          ymax = -log10(min_p_value)),
      width = 0.3, alpha = 0.5
    ) +
    geom_point(aes(size = selection_rate), alpha = 0.8) +
    geom_hline(yintercept = -log10(0.05), linetype = "dashed",
               color = "red", size = 1) +
    geom_hline(yintercept = -log10(0.20), linetype = "dashed",
               color = "orange", size = 1) +
    scale_fill_gradient(
      low = "#FFF9C4", 
      high = "#1976D2",
      name = "Folds\nSelected",
      breaks = c(2, 3, 4, 5)
    ) +
    scale_size_continuous(
      name = "Selection\nRate (%)",
      range = c(2, 6)
    ) +
    coord_flip() +
    labs(
      title = "Variable Significance and Selection Stability (Approach 3)",
      subtitle = "Variables selected in 2+ folds | Error bars show range of p-values across folds",
      x = "Clinical Variable",
      y = expression(-log[10](mean~p-value)),
      caption = "Red line: p=0.05 | Orange line: p=0.20 (screening threshold)"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      legend.position = "right",
      plot.title = element_text(face = "bold", size = 14),
      axis.text.y = element_text(size = 10)
    )
  
  print(p2)
  
  # ========================================================================
  # VISUALIZATION 3: Heatmap of Selection Across Folds
  # ========================================================================
  
  # Prepare data for heatmap
  selection_matrix <- all_selections_approach3 %>%
    select(fold, variable) %>%
    distinct() %>%
    mutate(selected = 1) %>%
    complete(fold = 1:5, 
             variable = unique(all_selections_approach3$variable),
             fill = list(selected = 0))
  
  # Add p-values
  selection_matrix <- selection_matrix %>%
    left_join(
      all_selections_approach3 %>% 
        select(fold, variable, p_value),
      by = c("fold", "variable")
    )
  
  # Calculate selection frequency for ordering
  var_order <- var_frequency_approach3 %>%
    arrange(desc(n_folds_selected), mean_p_value) %>%
    pull(variable)
  
  p3 <- selection_matrix %>%
    mutate(
      variable = factor(variable, levels = var_order),
      fold = factor(fold, levels = 1:5),
      p_value_label = ifelse(selected == 1, 
                              sprintf("%.3f", p_value), 
                              "")
    ) %>%
    ggplot(aes(x = fold, y = variable, fill = as.factor(selected))) +
    geom_tile(color = "white", size = 1) +
    geom_text(aes(label = p_value_label), size = 3, color = "black") +
    scale_fill_manual(
      values = c("0" = "gray90", "1" = "#1976D2"),
      labels = c("Not selected", "Selected"),
      name = ""
    ) +
    labs(
      title = "Variable Selection Pattern Across CV Folds (Approach 3)",
      subtitle = "P-values shown for selected variables",
      x = "Cross-Validation Fold",
      y = "Clinical Variable",
      caption = "Blue cells: variable selected | Numbers: p-value from univariate screening"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(face = "bold", size = 14),
      axis.text.x = element_text(size = 11, face = "bold"),
      axis.text.y = element_text(size = 10),
      legend.position = "bottom",
      panel.grid = element_blank()
    )
  
  print(p3)
  
  # ========================================================================
  # SUMMARY TABLE
  # ========================================================================
  
  cat("\n\n=== VARIABLE SELECTION SUMMARY (APPROACH 3) ===\n\n")
  
  stability_summary <- var_frequency_approach3 %>%
    summarize(
      `Very Stable (5 folds)` = sum(n_folds_selected == 5),
      `Stable (4 folds)` = sum(n_folds_selected == 4),
      `Moderate (3 folds)` = sum(n_folds_selected == 3),
      `Low (2 folds)` = sum(n_folds_selected == 2),
      `Unstable (1 fold)` = sum(n_folds_selected == 1)
    )
  
  print(stability_summary)
  
  cat("\n\nMost stable variables (selected in all 5 folds):\n")
  most_stable <- var_frequency_approach3 %>%
    filter(n_folds_selected == 5) %>%
    arrange(mean_p_value)
  
  if(nrow(most_stable) > 0) {
    for(i in 1:nrow(most_stable)) {
      cat(sprintf("  %d. %s (mean p=%.4f)\n",
                  i,
                  most_stable$variable[i],
                  most_stable$mean_p_value[i]))
    }
  } else {
    cat("  None (no variables selected in all 5 folds)\n")
  }
  
  # Detailed table
  cat("\n\nDetailed selection statistics:\n")
  print(var_frequency_approach3 %>%
          arrange(desc(n_folds_selected), mean_p_value) %>%
          select(Variable = variable,
                 `Folds Selected` = n_folds_selected,
                 `Selection Rate (%)` = selection_rate,
                 `Mean p-value` = mean_p_value,
                 `Min p-value` = min_p_value,
                 `Max p-value` = max_p_value))
  
} else {
  cat("No Approach 3 results with variable selection data found.\n")
}
```

**Interpretation of Selection Variability:**

The absence of variables selected in all 5 folds reflects the expected behavior of data-driven selection with small samples (n~34 per fold). Key observations:

1. **Moderately stable features** (selected in 3-4 folds) likely represent genuine PTB associations:
   - Extreme BMI and extreme age show consistent univariate associations
   - First trimester bleeding (comp1tribleed) emerges as population-specific risk factor
   - Work outside home may capture socioeconomic or stress-related pathways

2. **Fold-specific selection** (1-2 folds) expected due to:
   - Statistical power limitations (with p<0.20 threshold, genuine associations may miss significance in some folds)
   - Heterogeneity in PTB cases across folds (different folds may contain cases driven by different etiologies)
   - Correlation structure changes when subsets are selected

3. **Methodological validity:**
   This variability does NOT indicate methodological failure. Rather, it demonstrates appropriate behavior for small-sample empirical selection:
   - Variables with strong effects (e.g., extreme BMI) selected consistently
   - Variables with moderate effects selected intermittently based on fold composition
   - Weak associations correctly excluded across all folds

The presence of several moderately stable variables (60-80% selection rate) provides confidence that data-driven selection identifies genuine signals rather than pure noise, while the absence of universally selected variables reflects the statistical reality of small-sample inference.

## Taxa Selection Visualization (ANCOM-BC2)

**Purpose:** Visualize which microbiome taxa were selected across CV folds when
using ANCOM-BC2 differential abundance analysis.

```{r ancom_taxa_visualization, fig.width=12, fig.height=6}
# Extract taxa selection data
ancom_selection_data <- list()

for(result in cv_results_all) {
  if(!is.null(result) && 
     result$microbiome == "ANCOM_Taxa" &&
     !is.null(result$selected_taxa)) {
    
    model_id <- paste(result$model_name, result$approach, sep = "_")
    ancom_selection_data[[model_id]] <- result$selected_taxa
  }
}

if(length(ancom_selection_data) > 0) {
  
  all_taxa_selections <- bind_rows(ancom_selection_data, .id = "model_config")
  
  # Calculate taxa selection frequency
  taxa_frequency <- all_taxa_selections %>%
    group_by(taxon) %>%
    summarize(
      n_folds_selected = n_distinct(fold),
      selection_rate = n_folds_selected / 5 * 100,
      .groups = "drop"
    ) %>%
    arrange(desc(n_folds_selected))
  
  # Visualization
  p_taxa <- taxa_frequency %>%
    mutate(
      stability = case_when(
        n_folds_selected == 5 ~ "Very Stable (5/5)",
        n_folds_selected >= 4 ~ "Stable (4/5)",
        n_folds_selected >= 3 ~ "Moderate (3/5)",
        TRUE ~ "Low (1-2/5)"
      )
    ) %>%
    ggplot(aes(x = reorder(taxon, n_folds_selected), y = n_folds_selected)) +
    geom_col(aes(fill = stability), alpha = 0.8) +
    geom_hline(yintercept = 5, linetype = "dashed", color = "red", size = 1) +
    scale_fill_manual(
      values = c(
        "Very Stable (5/5)" = "#1B5E20",
        "Stable (4/5)" = "#388E3C",
        "Moderate (3/5)" = "#FBC02D",
        "Low (1-2/5)" = "#F57C00"
      ),
      name = "Selection Stability"
    ) +
    coord_flip() +
    labs(
      title = "ANCOM-BC2 Taxa Selection Frequency Across CV Folds",
      subtitle = "Differentially abundant taxa identified independently in each fold",
      x = "Bacterial Genus",
      y = "Number of Folds Selected (out of 5)",
      caption = "Red dashed line: selected in all folds"
    ) +
    theme_minimal(base_size = 12) +
    theme(
      legend.position = "bottom",
      plot.title = element_text(face = "bold", size = 14)
    )
  
  print(p_taxa)
  
  cat("\n=== TAXA SELECTION SUMMARY ===\n\n")
  print(taxa_frequency %>%
          arrange(desc(n_folds_selected)) %>%
          select(Taxon = taxon,
                 `Folds Selected` = n_folds_selected,
                 `Selection Rate (%)` = selection_rate))
  
} else {
  cat("No ANCOM taxa selection data found.\n")
}
```

**Interpretation of Taxa Selection Patterns:**

**Reproducibly identified taxa** (selected in ≥60% of folds):
- **Peptostreptococcus** (100%): Only taxon selected in all folds; strong candidate for PTB biomarker in Mexican women
- **Mycoplasma** (80%): Established reproductive pathogen; high stability supports causal role
- **Environmental taxa** (60%): Mesorhizobium, Methylorubrum, Microbacterium show surprising stability

**Biological vs. Technical Signals:**

The high stability of environmental taxa raises important questions:

*Technical explanation (contamination):*
- Low-biomass vaginal samples prone to reagent contamination
- Environmental bacteria common contaminants in extraction kits and PCR reagents
- Stability across folds suggests systematic rather than random contamination

*Biological explanation (true exposures):*
- Occupational exposures (agricultural work common in Puebla, Mexico)
- Dietary sources (fermented foods, unwashed produce)
- Hygiene practices varying between PTB and term pregnancies

**Critical need:** Future studies must implement rigorous negative controls (extraction blanks, PCR blanks) to distinguish between these possibilities. The stability of environmental taxa indicates they are genuine signals in this dataset—whether biological or technical—rather than random noise.

**Clinical relevance:**
Even if environmental taxa represent contamination, their consistent association with PTB outcomes suggests potential utility as "nuisance biomarkers." However, reliance on potentially artifactual signals would be scientifically unsatisfying and clinically risky without mechanistic understanding.

**Taxa variability** (selected in 20-40% of folds):
- Common reproductive tract organisms (Streptococcus, Bifidobacterium, Lactobacillus) show lower stability than expected
- May reflect genuine biological variability in which organisms drive PTB in specific cases
- Could indicate that these taxa contribute to PTB risk conditionally (e.g., in presence of other risk factors) rather than universally

**Final model consensus approach:**
For the final model trained on complete data, we selected taxa present in ≥3 folds (60% threshold), yielding 7 consensus taxa. This conservative approach balances two objectives:
1. Including only the most reproducible signals
2. Retaining sufficient features for meaningful prediction

This consensus set includes Peptostreptococcus, Mycoplasma, and 5 additional taxa, providing a focused feature space for interpretation and potential biomarker validation.

## PERFORMANCE TABLE: NESTED CV RESULTS

Purpose: Generate publication-ready table of model performance metrics

This chunk transforms the cv_results_all list into a formatted table
suitable for inclusion in manuscripts. The table includes:
- Model identification (algorithm, approach, microbiome)
- Threshold-independent metrics (AUROC, PRAUC)
- Threshold-dependent metrics (Sensitivity, Specificity, Accuracy)
- Performance reported as mean ± SD across 5 folds

Table features:
- Sorted by AUROC (descending) to highlight best-performing models
- Color-coded top 3 models for visual emphasis
- Professional formatting using kableExtra
- Footnotes explaining methodology

Output: HTML table with bootstrap styling for presentation

```{r cv_results_table}
# ============================================================================
# PERFORMANCE TABLE: NESTED CV RESULTS
# ============================================================================

cat("\n=== CREATING PERFORMANCE TABLE ===\n\n")

cv_performance <- map_dfr(cv_results_all, function(res) {
  tibble(
    Model = res$model_name,
    Approach = res$approach,
    Microbiome = res$microbiome,
    AUROC = sprintf("%.3f ± %.3f", 
                    res$summary$AUROC_mean, res$summary$AUROC_sd),
    PRAUC = sprintf("%.3f ± %.3f", 
                    res$summary$PRAUC_mean, res$summary$PRAUC_sd),
    Sensitivity = sprintf("%.3f ± %.3f", 
                          res$summary$Sensitivity_mean, res$summary$Sensitivity_sd),
    Specificity = sprintf("%.3f ± %.3f", 
                          res$summary$Specificity_mean, res$summary$Specificity_sd),
    Accuracy = sprintf("%.3f ± %.3f", 
                       res$summary$Accuracy_mean, res$summary$Accuracy_sd),
    Bal_Accuracy = sprintf("%.3f ± %.3f", 
                           res$summary$Balanced_Accuracy_mean, 
                           res$summary$Balanced_Accuracy_sd),
    Threshold = sprintf("%.3f ± %.3f", 
                        res$summary$threshold_mean, res$summary$threshold_sd),
    Youden = sprintf("%.3f ± %.3f", 
                     res$summary$Youden_mean, res$summary$Youden_sd),
    AUROC_mean = res$summary$AUROC_mean,  # For sorting
    PRAUC_mean = res$summary$PRAUC_mean   # For secondary sorting
  )
}) %>%
  arrange(desc(AUROC_mean), desc(PRAUC_mean)) %>%
  select(-AUROC_mean, -PRAUC_mean)

# Display table
kable(cv_performance,
      caption = "Model Performance with Nested Cross-Validation (Mean ± SD across 5 folds)",
      format = "html",
      align = c("l", "l", "l", rep("r", 8))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE,
                font_size = 10) %>%
  row_spec(0, bold = TRUE, background = "#E3F2FD") %>%
  column_spec(1, bold = TRUE, width = "7em") %>%
  column_spec(2, width = "10em") %>%
  row_spec(1:3, background = "#C8E6C9") %>%
  footnote(
    general = c(
      "Thresholds optimized on inner validation sets using Youden Index.",
      "Performance metrics calculated on independent outer test folds.",
      "This approach avoids data leakage and provides unbiased estimates."
    ),
    general_title = "Notes:",
    footnote_as_chunk = TRUE
  )

cat("\n✓ Performance table created successfully\n")
cat(sprintf("Total models: %d\n", nrow(cv_performance)))
```

```{r plot_roc_pr_no_retrain, fig.width=10, fig.height=10}
# ============================================================================
# ROC/PR CURVES FROM STORED DATA (NO RETRAINING)
# ============================================================================

cat("\n╔════════════════════════════════════════╗\n")
cat("║  PLOTTING ROC/PR FROM STORED DATA     ║\n")
cat("╚════════════════════════════════════════╝\n\n")

# Helper function: interpolate to common grid
interpolate_curve <- function(x, y, x_grid) {
  df <- tibble(x = x, y = y) %>% distinct() %>% arrange(x)
  if(nrow(df) < 2) return(rep(mean(y, na.rm = TRUE), length(x_grid)))
  approx(x = df$x, y = df$y, xout = x_grid, method = "linear", rule = 2)$y
}

# ============================================================================
# GET TOP 3 MODELS WITH FULL METRICS
# ============================================================================

top_3_idx <- map_dfr(1:length(cv_results_all), function(i) {
  res <- cv_results_all[[i]]
  tibble(
    idx = i,
    model_name = res$model_name,
    approach = res$approach,
    microbiome = res$microbiome,
    auroc_mean = res$summary$AUROC_mean,
    auroc_sd = res$summary$AUROC_sd,
    prauc_mean = res$summary$PRAUC_mean,
    prauc_sd = res$summary$PRAUC_sd
  )
}) %>%
  arrange(desc(auroc_mean), desc(prauc_mean)) %>%
  head(3) %>%
  mutate(rank = row_number())  # Explicit rank 1, 2, 3

cat("Top 3 models (in order):\n")
print(top_3_idx %>% select(rank, model_name, approach, microbiome, auroc_mean))
cat("\n")

# ============================================================================
# EXTRACT ROC DATA WITH PROPER RANKING
# ============================================================================

roc_data <- map_dfr(1:nrow(top_3_idx), function(i) {
  idx <- top_3_idx$idx[i]
  res <- cv_results_all[[idx]]
  
  if(is.null(res$roc_curves)) {
    cat(sprintf("WARNING: Model rank %d missing ROC curves\n", i))
    return(NULL)
  }
  
  # Create label WITH metrics
  model_label_with_metrics <- sprintf(
    "%s | %s | %s\n(AUROC: %.3f ± %.3f)",
    res$model_name,
    gsub("Approach[0-9]_", "", res$approach),
    gsub("_", " ", res$microbiome),
    top_3_idx$auroc_mean[i],
    top_3_idx$auroc_sd[i]
  )
  
  res$roc_curves %>%
    mutate(
      model_rank = i,  # Explicit rank from top_3_idx order
      model_label = model_label_with_metrics,
      fpr = 1 - specificity
    )
})

if(nrow(roc_data) == 0) {
  stop("No ROC data available. Re-run nested CV with curve storage.")
}

# ============================================================================
# INTERPOLATE ROC CURVES
# ============================================================================

fpr_grid <- seq(0, 1, 0.01)

roc_interp <- roc_data %>%
  group_by(model_label, model_rank, fold) %>%
  arrange(fpr) %>%
  summarize(
    fpr_interp = list(fpr_grid),
    sens_interp = list(interpolate_curve(fpr, sensitivity, fpr_grid)),
    .groups = "drop"
  ) %>%
  unnest(c(fpr_interp, sens_interp))

roc_summary <- roc_interp %>%
  group_by(model_label, model_rank, fpr_interp) %>%
  summarize(
    sens_mean = mean(sens_interp, na.rm = TRUE),
    sens_sd = sd(sens_interp, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  mutate(
    sens_se = sens_sd / sqrt(n),
    sens_lower = pmax(0, sens_mean - 1.96 * sens_se),
    sens_upper = pmin(1, sens_mean + 1.96 * sens_se)
  )

# ============================================================================
# EXTRACT PR DATA WITH PROPER RANKING
# ============================================================================

pr_data <- map_dfr(1:nrow(top_3_idx), function(i) {
  idx <- top_3_idx$idx[i]
  res <- cv_results_all[[idx]]
  
  if(is.null(res$pr_curves)) {
    cat(sprintf("WARNING: Model rank %d missing PR curves\n", i))
    return(NULL)
  }
  
  # Create label WITH metrics
  model_label_with_metrics <- sprintf(
    "%s | %s | %s\n(AUROC: %.3f ± %.3f)",
    res$model_name,
    gsub("Approach[0-9]_", "", res$approach),
    gsub("_", " ", res$microbiome),
    top_3_idx$auroc_mean[i],
    top_3_idx$auroc_sd[i]
  )
  
  res$pr_curves %>%
    mutate(
      model_rank = i,
      model_label = model_label_with_metrics
    )
})

# ============================================================================
# INTERPOLATE PR CURVES
# ============================================================================

recall_grid <- seq(0, 1, 0.01)

pr_interp <- pr_data %>%
  group_by(model_label, model_rank, fold) %>%
  arrange(recall) %>%
  summarize(
    recall_interp = list(recall_grid),
    prec_interp = list(interpolate_curve(recall, precision, recall_grid)),
    .groups = "drop"
  ) %>%
  unnest(c(recall_interp, prec_interp))

pr_summary <- pr_interp %>%
  group_by(model_label, model_rank, recall_interp) %>%
  summarize(
    prec_mean = mean(prec_interp, na.rm = TRUE),
    prec_sd = sd(prec_interp, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) %>%
  mutate(
    prec_se = prec_sd / sqrt(n),
    prec_lower = pmax(0, prec_mean - 1.96 * prec_se),
    prec_upper = pmin(1, prec_mean + 1.96 * prec_se)
  )

# ============================================================================
# COLORS AND LABELS (CRITICAL FIX)
# ============================================================================

# Define colors for ranks
colors <- c("#E74C3C", "#3498DB", "#2ECC71")  # Red, Blue, Green for ranks 1, 2, 3

# Get labels in rank order (1, 2, 3)
labels_ordered <- roc_summary %>%
  distinct(model_rank, model_label) %>%
  arrange(model_rank) %>%
  pull(model_label)

cat("\nColor assignment:\n")
cat(sprintf("  Rank 1 (Red):   %s\n", gsub("\n.*", "", labels_ordered[1])))
cat(sprintf("  Rank 2 (Blue):  %s\n", gsub("\n.*", "", labels_ordered[2])))
cat(sprintf("  Rank 3 (Green): %s\n\n", gsub("\n.*", "", labels_ordered[3])))

# ============================================================================
# PLOT ROC CURVES
# ============================================================================

p_roc <- ggplot() +
  # Confidence bands
  geom_ribbon(
    data = roc_summary,
    aes(x = fpr_interp, 
        ymin = sens_lower,
        ymax = sens_upper,
        fill = factor(model_rank, levels = 1:3)),
    alpha = 0.2
  ) +
  # Lines for ranks 2 and 3 (thinner)
  geom_line(
    data = roc_summary %>% filter(model_rank %in% c(2, 3)),
    aes(x = fpr_interp, 
        y = sens_mean,
        color = factor(model_rank, levels = 1:3)),
    linewidth = 1.0,
    alpha = 0.8
  ) +
  # Line for rank 1 (thicker, winner)
  geom_line(
    data = roc_summary %>% filter(model_rank == 1),
    aes(x = fpr_interp, 
        y = sens_mean,
        color = factor(model_rank, levels = 1:3)),
    linewidth = 1.5,
    alpha = 1.0
  ) +
  # Diagonal reference
  geom_abline(
    intercept = 0, slope = 1, 
    linetype = "dashed", 
    color = "gray40",
    linewidth = 0.8
  ) +
  # Color scales with explicit factor levels
  scale_color_manual(
    values = colors,
    labels = labels_ordered,
    name = "",
    breaks = factor(1:3, levels = 1:3)
  ) +
  scale_fill_manual(
    values = colors,
    labels = labels_ordered,
    name = "",
    breaks = factor(1:3, levels = 1:3)
  ) +
  coord_equal(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(
    title = "ROC Curves: Top 3 Models",
    subtitle = "5-fold CV with 95% CI (from stored data, no retraining)",
    x = "False Positive Rate (1 - Specificity)",
    y = "True Positive Rate (Sensitivity)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 9),
    plot.title = element_text(face = "bold", size = 15),
    panel.grid.minor = element_blank()
  ) +
  guides(
    color = guide_legend(ncol = 1, override.aes = list(linewidth = 2)),
    fill = guide_legend(ncol = 1)
  )

print(p_roc)

# ============================================================================
# PLOT PR CURVES
# ============================================================================

prevalence <- 0.326

p_pr <- ggplot() +
  # Confidence bands
  geom_ribbon(
    data = pr_summary,
    aes(x = recall_interp, 
        ymin = prec_lower,
        ymax = prec_upper,
        fill = factor(model_rank, levels = 1:3)),
    alpha = 0.2
  ) +
  # Lines for ranks 2 and 3
  geom_line(
    data = pr_summary %>% filter(model_rank %in% c(2, 3)),
    aes(x = recall_interp, 
        y = prec_mean,
        color = factor(model_rank, levels = 1:3)),
    linewidth = 1.0,
    alpha = 0.8
  ) +
  # Line for rank 1 (winner)
  geom_line(
    data = pr_summary %>% filter(model_rank == 1),
    aes(x = recall_interp, 
        y = prec_mean,
        color = factor(model_rank, levels = 1:3)),
    linewidth = 1.5,
    alpha = 1.0
  ) +
  # Baseline
  geom_hline(
    yintercept = prevalence,
    linetype = "dashed",
    color = "gray40",
    linewidth = 0.8
  ) +
  annotate(
    "text", 
    x = 0.85, y = prevalence + 0.03, 
    label = sprintf("Baseline (prevalence = %.3f)", prevalence),
    size = 3.5, color = "gray40"
  ) +
  # Color scales
  scale_color_manual(
    values = colors,
    labels = labels_ordered,
    name = "",
    breaks = factor(1:3, levels = 1:3)
  ) +
  scale_fill_manual(
    values = colors,
    labels = labels_ordered,
    name = "",
    breaks = factor(1:3, levels = 1:3)
  ) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  labs(
    title = "Precision-Recall Curves: Top 3 Models",
    subtitle = "5-fold CV with 95% CI (from stored data, no retraining)",
    x = "Recall (Sensitivity)",
    y = "Precision (PPV)"
  ) +
  theme_minimal(base_size = 13) +
  theme(
    legend.position = "bottom",
    legend.text = element_text(size = 9),
    plot.title = element_text(face = "bold", size = 15),
    panel.grid.minor = element_blank()
  ) +
  guides(
    color = guide_legend(ncol = 1, override.aes = list(linewidth = 2)),
    fill = guide_legend(ncol = 1)
  )

print(p_pr)

cat("\n✓ Curves plotted with correct color assignment\n")
cat("✓ Metrics included in legends\n")
cat("✓ NO RETRAINING - using original nested CV results\n")
```

```{r feature_importance_no_retrain, fig.width=10, fig.height=9}
# ============================================================================
# FEATURE IMPORTANCE FROM BEST-PERFORMING RF MODEL
# ============================================================================

cat("\n╔════════════════════════════════════════╗\n")
cat("║  FEATURE IMPORTANCE (BEST RF MODEL)   ║\n")
cat("╚════════════════════════════════════════╝\n\n")

# ============================================================================
# FIND BEST RF MODEL (by AUROC, then PRAUC)
# ============================================================================

rf_models <- map_dfr(1:length(cv_results_all), function(i) {
  res <- cv_results_all[[i]]
  if(res$model_name == "rf_base") {
    tibble(
      idx = i,
      model_name = res$model_name,
      approach = res$approach,
      microbiome = res$microbiome,
      auroc_mean = res$summary$AUROC_mean,
      auroc_sd = res$summary$AUROC_sd,
      prauc_mean = res$summary$PRAUC_mean,
      prauc_sd = res$summary$PRAUC_sd
    )
  } else {
    NULL
  }
}) %>%
  arrange(desc(auroc_mean), desc(prauc_mean))

if(nrow(rf_models) == 0) {
  cat("No RF model found\n")
} else {
  
  # Select best RF (first row after sorting)
  best_rf_info <- rf_models[1, ]
  best_rf <- cv_results_all[[best_rf_info$idx]]
  
  cat(sprintf("Best RF Model:\n"))
  cat(sprintf("  Model: %s\n", best_rf_info$model_name))
  cat(sprintf("  Approach: %s\n", gsub("Approach[0-9]_", "", best_rf_info$approach)))
  cat(sprintf("  Microbiome: %s\n", gsub("_", " ", best_rf_info$microbiome)))
  cat(sprintf("  AUROC: %.3f ± %.3f\n", best_rf_info$auroc_mean, best_rf_info$auroc_sd))
  cat(sprintf("  PRAUC: %.3f ± %.3f\n\n", best_rf_info$prauc_mean, best_rf_info$prauc_sd))
  
  # ========================================================================
  # CHECK FINAL MODEL EXISTS
  # ========================================================================
  
  if(is.null(best_rf$final_model)) {
    cat("ERROR: No final model stored.\n")
    cat("Re-run nested CV with final model storage enabled.\n")
  } else {
    
    cat("Using stored final model\n")
    
    # ======================================================================
    # EXTRACT FEATURE IMPORTANCE
    # ======================================================================
    
    rf_fit <- extract_fit_parsnip(best_rf$final_model)
    
    if(is.null(rf_fit$fit$variable.importance)) {
      cat("ERROR: Model does not have variable importance\n")
    } else {
      
      imp_df <- tibble(
        feature = names(rf_fit$fit$variable.importance),
        importance = rf_fit$fit$variable.importance
      ) %>% 
        arrange(desc(importance))
      
      # ==================================================================
      # CLASSIFY FEATURES (COMPREHENSIVE CLINICAL LIST)
      # ==================================================================
      
      clinical_vars_comprehensive <- c(
        # Basic demographics
        "sdg_visita", "edad_cronologicamujer",
        
        # Anthropometry
        "imc_pregestacional", "imc_visita", "peso_kg", 
        "peso_pregestacional_kg", "talla_mujer_cm",
        
        # BMI categories
        "extreme_bmi", "obese", "underweight",
        "imc_pregest_categ", "imc_categ",
        
        # Age categories
        "extreme_age", "age_risk_category_optimal", 
        "age_risk_category_very_young", "age_risk_category_advanced_maternal_age",
        
        # Supplements
        "vitaminsup", "dietsuppl3mon", "supintakefreq", "folic_ac_correg",
        
        # Complications
        "complication_count", "any_complication", "multiple_complications",
        "comp1tribleed", "compvaginf", "compsexualinf", "comppreeclam",
        
        # Pregnancy conditions
        "rpm_preterm", "rpm", "diabetes_gest", "rciu",
        "oligohidramnios", "anemia_visita",
        
        # Hematology
        "hemoglobin_alti_adj", "hemoglobin_g_dl",
        
        # Infections
        "infection_burden", "any_infection",
        
        # Socioeconomic
        "low_education", "unmarried", "ses_risk_score",
        "nivel_academico", "maritalstat", "workouthome",
        
        # Baby
        "sex_baby",
        
        # Diversity (special category)
        "shannon_diversity"
      )
      
      imp_plot <- imp_df %>%
        head(25) %>%
        mutate(
          # =================================================================
          # CLASSIFY - VECTORIZED
          # =================================================================
          
          feature_type = case_when(
            # 1. Shannon
            grepl("shannon", feature, ignore.case = TRUE) ~ "Diversity",
            
            # 2. Exact match
            feature %in% clinical_vars_comprehensive ~ "Clinical",
            
            # 3. Base name (remove suffix)
            gsub("_[^_]+$", "", feature) %in% clinical_vars_comprehensive ~ "Clinical",
            
            # 4. Starts with clinical (VECTORIZED)
            sapply(feature, function(f) {
              any(sapply(clinical_vars_comprehensive, function(cv) {
                startsWith(f, cv)
              }))
            }) ~ "Clinical",
            
            # 5. CLR patterns
            grepl("^CLR_", feature) ~ "Microbiome",
            grepl("^Genus_", feature) ~ "Microbiome",
            
            # 6. Known genera (VECTORIZED)
            sapply(feature, function(f) {
              genera <- c(
                "Lactobacillus", "Gardnerella", "Prevotella", "Sneathia",
                "Atopobium", "Megasphaera", "Streptococcus", "Dialister",
                "Mobiluncus", "Anaerococcus", "Peptoniphilus", "Finegoldia",
                "Porphyromonas", "Bacteroides", "Ureaplasma", "Mycoplasma",
                "Bifidobacterium", "Corynebacterium", "Staphylococcus",
                "Ralstonia", "Fannyhessea", "Hydrotalea", "HT002",
                "Lawsonella", "Mesorhizobium", "Methylobacterium",
                "Gordonia"
              )
              any(sapply(genera, function(g) {
                grepl(g, f, ignore.case = TRUE)
              }))
            }) ~ "Microbiome",
            
            # 7. Default
            TRUE ~ "Microbiome"
          ),
          
          # =================================================================
          # CLEAN LABELS
          # =================================================================
          
          feature_clean = case_when(
            feature == "edad_cronologicamujer" ~ "Maternal Age",
            feature == "imc_pregestacional" ~ "Pre-pregnancy BMI",
            feature == "imc_visita" ~ "BMI at Visit",
            feature == "sdg_visita" ~ "Gestational Age",
            feature == "shannon_diversity" ~ "Shannon Diversity",
            feature == "nivel_academico" ~ "Academic level",
            feature == "extreme_bmi" ~ "Extreme BMI",
            feature == "underweight" ~ "Underweight (Low BMI)",
            feature == "peso_kg" ~ "Weight (kg)",
            feature == "peso_pregestacional_kg" ~ "Pre-pregnancy Weight",
            feature == "rpm_preterm" ~ "PROM (preterm)",
            feature == "hemoglobin_alti_adj" ~ "Hemoglobin (altitude-adj)",
            feature == "folic_ac_correg" ~ "Folic acid intake",
            feature == "vitaminsup" ~ "Vitamin Supplementation",
            feature == "talla_mujer_cm" ~ "Height (cm)",
            feature == "obese" ~ "Obese",
            grepl("^CLR_", feature) ~ gsub("CLR_|Genus_", "", feature),
            grepl("^Genus_", feature) ~ gsub("Genus_", "", feature),
            grepl("imc_pregest_categ", feature) ~ gsub("imc_pregest_categ_", "BMI: ", feature),
            grepl("age_risk_category", feature) ~ gsub("age_risk_category_", "Age: ", feature),
            TRUE ~ feature
          ),
          
          feature_clean = str_trunc(feature_clean, width = 35)
        ) %>%
        arrange(desc(importance))
      
      # ==================================================================
      # VERIFY CLASSIFICATION
      # ==================================================================
      
      cat("\nFeature type distribution (top 25):\n")
      type_table <- table(imp_plot$feature_type)
      print(type_table)
      
      if(sum(type_table["Clinical"]) < 3) {
        cat("\n⚠️  WARNING: Very few clinical features detected.\n")
        cat("   This may indicate classification issues.\n\n")
      }
      
      # ==================================================================
      # CREATE DESCRIPTIVE TITLE
      # ==================================================================
      
      # Format approach name
      approach_clean <- case_when(
        grepl("DREAM", best_rf_info$approach) ~ "minimal clinical features",
        grepl("Literature", best_rf_info$approach) ~ "literature-based features",
        grepl("DataDriven", best_rf_info$approach) ~ "data-driven features",
        TRUE ~ gsub("Approach[0-9]_", "", best_rf_info$approach)
      )
      
      # Format microbiome name
      microbiome_clean <- case_when(
        best_rf_info$microbiome == "Full_Microbiome" ~ "full microbiome",
        best_rf_info$microbiome == "ANCOM_Taxa" ~ "ANCOM-selected taxa",
        TRUE ~ tolower(gsub("_", " ", best_rf_info$microbiome))
      )
      
      # Create title and subtitle
      plot_title <- "Feature Importance: Best-Performing Model"
      
      plot_subtitle <- sprintf(
        "Random Forest with %s and %s (AUROC = %.3f, PRAUC = %.3f)",
        approach_clean,
        microbiome_clean,
        best_rf_info$auroc_mean,
        best_rf_info$prauc_mean
      )
      
      # ==================================================================
      # PLOT
      # ==================================================================
      
      p <- imp_plot %>%
        ggplot(aes(x = reorder(feature_clean, importance), 
                   y = importance, 
                   fill = feature_type)) +
        geom_col(alpha = 0.85, width = 0.8) +
        coord_flip() +
        scale_fill_manual(
          values = c(
            "Clinical" = "#E74C3C",      # Red
            "Microbiome" = "#3498DB",    # Blue
            "Diversity" = "#2ECC71"      # Green
          ),
          name = "Feature Type"
        ) +
        labs(
          title = plot_title,
          subtitle = plot_subtitle,
          x = "",
          y = "Variable Importance (Mean Decrease in Gini Impurity)",
          caption = "Importance measured from Random Forest trained on full dataset"
        ) +
        theme_minimal(base_size = 12) +
        theme(
          legend.position = "bottom",
          legend.title = element_text(face = "bold", size = 11),
          legend.text = element_text(size = 10),
          plot.title = element_text(face = "bold", size = 14, hjust = 0),
          plot.subtitle = element_text(size = 11, color = "gray30", hjust = 0),
          plot.caption = element_text(size = 9, color = "gray50", hjust = 0),
          axis.text.y = element_text(size = 10),
          axis.text.x = element_text(size = 10),
          axis.title.x = element_text(size = 11, face = "bold"),
          panel.grid.major.y = element_blank(),
          panel.grid.minor = element_blank()
        )
      
      print(p)
      
      # ==================================================================
      # PRINT TOP 10
      # ==================================================================
      
      cat("\n=== TOP 10 MOST IMPORTANT FEATURES ===\n\n")
      
      top10 <- imp_plot %>%
        head(25) %>%
        select(Rank = everything(), Feature = feature_clean, 
               Type = feature_type, Importance = importance) %>%
        mutate(
          Rank = 1:n(),
          Importance = round(Importance, 3)
        )
      
      print(top10)
      
      cat("\n✓ Feature importance from best-performing RF model\n")
      cat("✓ NO RETRAINING required\n")
    }
  }
}
```


```{r performance_comparison_plot, fig.width=12, fig.height=8}
# ============================================================================
# COMPREHENSIVE PERFORMANCE COMPARISON
# ============================================================================

# Prepare data for plotting
plot_data_all <- map_dfr(cv_results_all, function(m) {
  m$fold_results %>%
    mutate(
      model = m$model_name,
      approach = m$approach,
      microbiome = m$microbiome,
      model_combo = paste(model, approach, microbiome, sep = "\n")
    )
})

# AUROC comparison
p_auroc <- ggplot(plot_data_all, 
                  aes(x = reorder(paste(model, approach), -AUROC), 
                      y = AUROC, fill = microbiome)) +
  geom_boxplot(alpha = 0.7) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "red") +
  scale_fill_manual(values = c("ANCOM_Taxa" = "#66C2A5", 
                                "Full_Microbiome" = "#FC8D62")) +
  labs(
    title = "Model Performance: AUROC Across All Combinations",
    subtitle = "12 model combinations with 5-fold nested cross-validation",
    x = "Model + Approach",
    y = "AUROC",
    fill = "Microbiome"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold", size = 14),
    legend.position = "top"
  )

print(p_auroc)

# Sensitivity vs Specificity scatter
summary_data <- map_dfr(cv_results_all, function(m) {
  tibble(
    model = m$model_name,
    approach = m$approach,
    microbiome = m$microbiome,
    AUROC = m$summary$AUROC_mean,
    Sensitivity = m$summary$Sensitivity_mean,
    Specificity = m$summary$Specificity_mean,
    Sens_sd = m$summary$Sensitivity_sd,
    Spec_sd = m$summary$Specificity_sd
  )
})

p_sens_spec <- ggplot(summary_data, 
                      aes(x = Specificity, y = Sensitivity, 
                          color = AUROC, shape = model)) +
  geom_point(size = 4, alpha = 0.8) +
  geom_errorbar(aes(ymin = Sensitivity - Sens_sd, 
                    ymax = Sensitivity + Sens_sd), alpha = 0.3) +
  geom_errorbarh(aes(xmin = Specificity - Spec_sd, 
                     xmax = Specificity + Spec_sd), alpha = 0.3) +
  scale_color_gradient2(low = "#D73027", mid = "#FEE090", high = "#1A9850",
                        midpoint = 0.7, limits = c(0.5, 1)) +
  scale_shape_manual(values = c(16, 17)) +
  labs(
    title = "Sensitivity vs Specificity Trade-off",
    subtitle = "Error bars show ±1 SD across folds",
    x = "Specificity",
    y = "Sensitivity",
    color = "AUROC",
    shape = "Model"
  ) +
  theme_minimal(base_size = 12) +
  theme(plot.title = element_text(face = "bold")) +
  coord_equal(xlim = c(0, 1), ylim = c(0, 1))

print(p_sens_spec)

cat("\n✓ Performance comparison plots generated\n")
```


# Key Findings Summary

## Model Performance in Context

Our analysis of 12 model combinations in a Mexican pregnancy cohort (n=43, 14 preterm births) yielded several important findings that must be interpreted within the constraints of the small sample size and exploratory study design.

### Primary Finding – Discrimination Performance

The best-performing model (**Random Forest + data-driven features + full microbiome**) achieved **AUROC 0.849 ± 0.130** (95% CI: 0.589-1.000), representing good to excellent discrimination despite the severely limited sample size.

**Contextual interpretation:**
This performance compares favorably to several international benchmarks, though direct comparison requires caution due to methodological differences:

| Study | Population | n | PTB Definition | AUROC | Notes |
|-------|-----------|---|----------------|-------|-------|
| **Current study** | Mexican | 43 | <37 weeks | **0.849** | Case-enriched (11.6% PTB) |
| Callahan et al. 2017 | US (mixed) | 135 | <37 weeks | 0.66 | Community-based cohort |
| DREAM Challenge 2024 | US (mixed) | 1,268 | <37 weeks | 0.69 (single), 0.74 (ensemble) | Large-scale benchmark |
| Park et al. 2022 | Korean | 150 | <34 weeks | 0.84 | Microbiome + ultrasound |
| Chakoory et al. 2024 | US (mixed) | 561 | <37 weeks | 0.88 | Deep learning, larger sample |
| DREAM Challenge 2024 | US (mixed) | 1,268 | <34 weeks | 0.87 (single), 0.91 (ensemble) | Early PTB definition |

Key observations:
- Our results (AUROC 0.849) exceed several larger studies despite n=43
- Performance comparable to specialized deep learning models (Chakoory: 0.88, n=561)
- Substantially exceeds DREAM Challenge late PTB benchmark (0.69-0.74, n=1268)
- Approaches Park et al. Korean cohort (0.84, n=150) using similar sample size
- Wide confidence intervals (0.589-1.000) reflect small sample uncertainty

**Critical caution**: The case-enriched design (11.6% PTB vs. ~10% population prevalence) and small sample size mean these results are hypothesis-generating, not actionable. External validation in larger, independent Mexican cohorts is essential before any clinical consideration.

### Consistency Across Approaches

Multiple model configurations achieved **AUROC ≥0.76**, indicating robust predictive signals rather than single-model artifacts:

- **RF + data-driven + full microbiome:** AUROC 0.849 ± 0.130 (rank 1)
- **RF + data-driven + ANCOM taxa:** AUROC 0.782 ± 0.142 (rank 2)
- **EN + literature-based + ANCOM:** AUROC 0.767 ± 0.149 (rank 3)
- **RF + DREAM-style + full microbiome:** AUROC 0.751 ± 0.087 (rank 4)

This consistency across diverse feature selection strategies suggests that observed performance reflects genuine biological signals rather than overfitting to specific modeling choices.

### Algorithm Performance

Random Forest consistently outperformed Elastic Net across all feature combinations:

- **Random Forest mean AUROC:** 0.735 (range: 0.656-0.849)
- **Elastic Net mean AUROC:** 0.685 (range: 0.616-0.767)

This 7% average performance advantage aligns with findings from the DREAM Challenge, where tree-based ensemble methods dominated top rankings. Random Forest's superior performance likely reflects:
1. Ability to capture non-linear microbiome-outcome relationships
2. Implicit feature interaction modeling through recursive partitioning
3. Robustness to outliers and scale differences via bootstrap aggregation

However, wide standard deviations (0.10–0.25) across both algorithms indicate substantial fold-to-fold instability due to having only ~1 PTB case per outer test fold.

---

## Feature Selection Strategy Impact

### Data-Driven Empirical Selection (Best Performing)

**Performance:**
- RF + Full Microbiome: AUROC 0.849 ± 0.130 (rank 1)
- RF + ANCOM Taxa: AUROC 0.782 ± 0.142 (rank 2)
- Average across algorithms: AUROC 0.732

**Selected features** (most stable across folds):
- Extreme BMI (80% of folds)
- First trimester bleeding (80%)
- Extreme maternal age (80%)
- Work outside home (80%)
- Obesity status (60%)
- Oligohydramnios (60%)

**Interpretation:**
Data-driven selection achieved highest discrimination by identifying population-specific risk patterns relevant to Mexican pregnancies. The prominence of anthropometric extremes (BMI, obesity, underweight) and workplace factors suggests socioeconomic and nutritional pathways may be particularly important in this population. This differs from literature-based emphasis on obstetric complications, highlighting potential population-specificity in PTB risk factors.

### Literature-Based Comprehensive Selection

**Performance:**
- EN + ANCOM Taxa: AUROC 0.767 ± 0.149 (rank 3)
- RF + Full Microbiome: AUROC 0.707 ± 0.107 (rank 6)
- Average across algorithms: AUROC 0.685

**Interpretation:**
Literature-based features showed intermediate performance with notable variability. Potential explanations:
1. **Population heterogeneity**: Risk factors established in US/European populations may show different effect sizes in Mexican women due to genetic, environmental, or healthcare differences
2. **Feature-sample size mismatch**: 9 clinical features may approach the limit of what n=43 can reliably model
3. **Multicollinearity**: Evidence-based features often correlate (e.g., preeclampsia and oligohydramnios), potentially destabilizing model estimates

Interestingly, literature-based features performed better with Elastic Net than Random Forest, suggesting that linear additive effects dominate for these evidence-based predictors.

### DREAM-Style Minimal Selection

**Performance:**
- RF + Full Microbiome: AUROC 0.751 ± 0.087 (rank 4)
- EN + ANCOM Taxa: AUROC 0.707 ± 0.173 (rank 5)
- Average across algorithms: AUROC 0.686

**Features:** Only 2 clinical variables (gestational age, maternal age)

**Interpretation:**
Using minimal clinical adjustment (DREAM Challenge approach) achieved **AUROC 0.686-0.751**, indicating that microbiome features provide substantial predictive information beyond basic demographics alone. The 8-16% performance improvement over chance (AUROC=0.5) suggests microbiome dysbiosis patterns carry meaningful PTB risk signals even after accounting for sample timing and maternal age.

However, adding data-driven clinical features increased AUROC by ~10 percentage points (0.751 → 0.849), demonstrating that clinical context substantially enhances microbiome-based prediction.

---

## Microbiome Feature Engineering

### Full Microbiome vs. ANCOM-Selected Taxa

**Full Microbiome (59 genera):**
- Higher mean AUROC: 0.734 vs. 0.704 (ANCOM)
- Improved specificity: Mean 58.5% vs. 54.7%
- Better balanced sensitivity-specificity trade-off

**ANCOM Taxa (7-15 genera per fold):**
- Effective dimensionality reduction (59 → ~9 features)
- Maintained competitive discrimination (AUROC 0.704)
- More interpretable feature space
- Reduced computational requirements

**Interpretation:**
The modest performance advantage of full microbiome (3 AUROC percentage points) suggests that non-differentially abundant taxa contribute meaningful predictive signals, possibly through:
1. Multivariate dysbiosis patterns not captured by univariate differential abundance
2. Synergistic/antagonistic interactions among taxa
3. Ensemble community composition effects (e.g., CST transitions)

This aligns with DREAM Challenge findings that ensemble microbiome features often outperform individual taxa (Golob et al., 2024).

### Biological Interpretation of ANCOM Taxa

**Reproductive health-relevant taxa:**
- **Peptostreptococcus** (100% of folds): Gram-positive anaerobe linked to bacterial vaginosis; consistent PTB association
- **Mycoplasma** (80% of folds): Established role in chorioamnionitis and adverse pregnancy outcomes

**Environmental/contaminant taxa:**
- **Mesorhizobium** (60%): Nitrogen-fixing soil bacteria; likely environmental contamination
- **Methylorubrum** (60%): Environmental methylotroph; unclear PTB relevance
- **Microbacterium** (60%): Ubiquitous environmental actinobacteria

**Implications:**
The high stability of environmental taxa across independent cross-validation folds suggests they represent genuine signals in this dataset rather than random noise. However, their biological significance remains unclear. Possibilities include:
1. True environmental exposures (occupational, dietary, or hygiene-related)
2. Reagent/kit contamination (common issue in low-biomass vaginal samples)
3. Differential carryover of environmental DNA between PTB and term samples

Future studies should implement rigorous negative controls (extraction blanks, PCR blanks, sequencing blanks) to distinguish true biological signals from technical artifacts.

---

## Feature Importance Analysis

Analysis of the best-performing Random Forest model (data-driven + full microbiome, AUROC 0.849) revealed:

**Top 10 features by Gini importance:**

1. **BMI at visit** (1.94) - Clinical
2. **Extreme BMI** (1.71) - Clinical  
3. **Underweight** (1.40) - Clinical
4. **Pre-pregnancy weight** (1.32) - Clinical
5. **Current weight** (1.04) - Clinical
6. **Methylobacterium** (0.98) - Microbiome
7. **PROM (preterm)** (0.96) - Clinical
8. **Lactobacillus** (0.85) - Microbiome
9. **Anaerococcus** (0.79) - Microbiome
10. **Maternal age** (0.73) - Clinical

**Feature type distribution (top 25):**
- Clinical: 10 features (40%)
- Microbiome: 14 features (56%)
- Diversity: 1 feature (4%)

**Interpretation:**

**Anthropometric dominance:**
Five of the top six features relate to maternal body composition (BMI, weight, underweight status). This striking pattern suggests nutritional status plays a critical role in PTB risk within this Mexican cohort. The clustering of highly correlated anthropometric features (despite correlation filtering at |r|<0.95) indicates Random Forest identifies complementary information across these measures—potentially capturing:
- Temporal changes (pre-pregnancy vs. current weight)
- Non-linear relationships (continuous BMI vs. binary extremes)
- Interaction effects with microbiome features

**Microbiome contributions:**
Microbiome features constitute >50% of top 25 features, demonstrating substantial predictive value even when clinical features are available. Key microbiome taxa include:
- **Lactobacillus** (rank 8): Protective role in vaginal health; lower abundance may indicate dysbiosis
- **Anaerococcus** (rank 9): Associated with bacterial vaginosis
- **Gardnerella** (rank 11): Classic BV marker
- **Streptococcus** (rank 18): Some species linked to ascending infection

**Clinical complications:**
PROM (premature rupture of membranes, rank 7) and hemoglobin (rank 12) represent direct PTB risk factors, confirming the model captures established pathophysiological pathways.

**Diversity metrics:**
Shannon diversity (rank 17) shows moderate importance, consistent with literature suggesting that overall community structure (diversity, evenness) contributes to PTB risk beyond individual taxa.

**Methodological note:**
Feature importance was extracted from a Random Forest model trained on the complete dataset using the same features and data transformations as the nested cross-validation models. This ensures feature importance rankings reflect the same feature space used for performance evaluation, maintaining internal consistency.

---

## Performance Variability and Uncertainty

All models exhibited substantial standard deviations (0.10–0.25 AUROC), reflecting:

**Statistical effects:**
- ~1 PTB case per outer test fold (8-9 subjects per fold)
- High leverage of individual cases on fold-specific performance
- Wide confidence intervals due to small effective sample size

**Biological heterogeneity:**
- PTB encompasses multiple etiologies (infection, placental insufficiency, maternal stress, etc.)
- Single cases may represent different pathways, each requiring different predictive features
- Microbiome-PTB associations may vary by gestational age at sampling

**Model instability:**
- Training sets contain only ~24 subjects per fold
- Feature selection variability (especially Approach 3)
- Algorithm sensitivity to bootstrap sample composition (RF) or regularization parameter stability (Elastic Net)

**Clinical interpretation:**
Wide intervals mean these results are **hypothesis-generating, not actionable**. The 95% CI for the best model (0.589-1.000) includes values from poor discrimination to perfect prediction, precluding clinical implementation without external validation.

**Comparison to larger studies:**
Even studies with n=150-200 report AUROC standard deviations of 0.05-0.10 across cross-validation folds (Park et al., 2022; Callahan et al., 2017). Our observed variability (0.10-0.25) is proportionally larger but expected given the 3-5× smaller sample size.

---

## Sensitivity–Specificity Trade-offs

Different model configurations achieved distinct operating points on the sensitivity-specificity curve:

### High-Sensitivity Model

**RF + data-driven + full microbiome** (best overall)
- Sensitivity: 80.0% ± 44.7%
- Specificity: 47.3% ± 42.6%
- AUROC: 0.849 ± 0.130

**Clinical utility:** Suitable for screening applications where missing true PTB cases (false negatives) is more costly than false positives. Could identify high-risk pregnancies for enhanced monitoring, though the extremely wide confidence intervals (sensitivity 35-100%, specificity 5-90%) limit practical utility without further validation.

### Balanced Model

**RF + data-driven + ANCOM taxa**
- Sensitivity: 53.3% ± 38.0%
- Specificity: 76.0% ± 34.4%
- AUROC: 0.782 ± 0.142

**Clinical utility:** Better balanced performance with acceptable trade-offs in both directions. The focused feature set (7-15 taxa) offers simplified implementation compared to full microbiome profiling.

### High-Specificity Models

**EN + DREAM + full microbiome**
- Sensitivity: 33.3% ± 40.8%
- Specificity: 86.0% ± 21.9%
- AUROC: 0.680 ± 0.168

**Clinical utility:** Prioritizes ruling in PTB when positive (high specificity), minimizing false positives. However, the low sensitivity means >66% of true PTB cases would be missed, limiting screening utility.

**Threshold optimization note:**
All thresholds were optimized on inner validation sets using Youden's Index (maximizing sensitivity + specificity - 1). Alternative thresholds could shift the sensitivity-specificity balance to match specific clinical priorities (e.g., threshold lowering to improve sensitivity for screening applications).

---

# Conclusions

This study represents the **first published machine learning model for preterm birth prediction developed specifically in a Mexican pregnancy cohort**, addressing a critical gap in precision medicine equity for Latin American populations.

## Key Findings

**Feasible prediction in small sample:**
Despite severe sample size constraints (n=43 subjects, 14 PTB cases), we achieved discrimination performance (**AUROC 0.849 for best model, range 0.616-0.849**) that compares favorably to larger international studies:
- Exceeds DREAM Challenge late PTB benchmark (AUROC 0.69, n=1268)
- Comparable to Callahan et al. 2017 (AUROC 0.66, n=135)
- Approaches Park et al. 2022 Korean cohort (AUROC 0.84, n=150)
- Matches deep learning approaches (Chakoory et al.: AUROC 0.88, n=561)

**Algorithm performance:**
Random Forest consistently outperformed Elastic Net across all feature combinations (mean AUROC 0.735 vs. 0.685), aligning with DREAM Challenge findings that tree-based ensemble methods excel for microbiome-based prediction.

**Feature selection strategies:**
Data-driven empirical selection achieved highest discrimination (AUROC 0.849), identifying population-specific risk patterns including anthropometric extremes (BMI, weight) and workplace factors not emphasized in literature-based approaches. This suggests Mexican populations may have distinct PTB risk factor profiles requiring locally-tailored models.

**Microbiome contribution:**
Full microbiome profiles (59 genera) slightly outperformed ANCOM-selected taxa (AUROC 0.734 vs. 0.704), indicating that non-differentially abundant taxa contribute meaningful predictive signals through ensemble dysbiosis patterns. However, ANCOM-based dimensionality reduction (59 → 7-15 taxa) maintained competitive performance while enhancing interpretability.

**Methodological rigor:**
Nested cross-validation with independent threshold optimization and within-fold feature selection prevented data leakage, providing unbiased performance estimates. All results must be interpreted within wide confidence intervals (±0.10-0.25 AUROC) reflecting the exploratory nature of this small-sample study.

## Critical Limitations

**Sample size severely limits generalizability:**
- n=43 subjects, only 14 PTB cases
- Wide confidence intervals prevent precise performance estimation
- Overfitting risk remains despite rigorous nested CV framework
- No subgroup analyses possible (e.g., by gestational age, PTB etiology)
- Single-center recruitment limits geographic/socioeconomic diversity

**Case-enriched design affects interpretation:**
- PTB prevalence (11.6%) approaches population baseline but differs from typical cohort studies
- PRAUC values more reliable than in extreme class imbalance scenarios
- Threshold recalibration required for clinical implementation

**No external validation:**
- All results from single hospital in Puebla, Mexico
- Generalization to other Mexican regions, healthcare systems, or Latin American countries unvalidated
- Sequencing platform, 16S region, and bioinformatics pipeline may affect cross-study comparability

**Technical concerns:**
- Presence of environmental taxa (Mesorhizobium, Methylorubrum, Microbacterium) suggests potential contamination
- Lack of negative controls prevents distinguishing true biological signals from technical artifacts
- Low-biomass vaginal samples particularly vulnerable to contamination effects

## Path Forward

**Immediate priorities:**
1. **External validation** in independent Mexican cohorts (target n≥200, ≥20 PTB cases) to assess generalization beyond single center
2. **Multi-site collaboration** pooling data from Mexican hospitals to achieve adequate statistical power for precise performance estimation
3. **Prospective validation** in routine clinical settings to evaluate real-world performance and workflow integration

**Medium-term development:**
1. **Larger cohort study** (target n≥500) enabling:
   - Subgroup analyses by gestational age, PTB etiology, and maternal risk factors
   - Optimization of classification thresholds for specific clinical scenarios (screening vs. diagnosis)
   - Cost-effectiveness analyses comparing microbiome testing to standard of care

2. **Longitudinal sampling protocols** examining:
   - Optimal gestational age windows for sampling (early vs. late pregnancy)
   - Temporal dynamics of microbiome-PTB associations
   - Prediction lead time (how far in advance PTB can be reliably predicted)

3. **Enhanced quality control** including:
   - Rigorous negative controls (extraction blanks, PCR blanks, sequencing blanks)
   - Decontamination algorithms (e.g., decontam R package)
   - Absolute quantification (qPCR) to distinguish low-biomass samples from contamination

**Long-term vision:**
1. **Multi-omic integration** combining:
   - Microbiome (16S rRNA, shotgun metagenomics)
   - Host transcriptomics (cervicovaginal fluid)
   - Metabolomics (vaginal metabolites, serum markers)
   - Ultrasound measurements (cervical length, uterine artery Doppler)

2. **Mechanistic studies** exploring:
   - Causal relationships between specific taxa and PTB outcomes (vs. pure association)
   - Host-microbiome interactions (immune responses to dysbiosis)
   - Intervention targets (probiotics, antibiotics, dietary modifications)

3. **Equity-focused research** ensuring:
   - Representative inclusion of Latin American populations in model development
   - Population-specific model optimization rather than assuming universal applicability
   - Accessibility and affordability of microbiome-based testing in resource-limited settings

4. **Clinical implementation pathways**:
   - Regulatory approval (FDA, COFEPRIS) based on validated performance
   - Clinical decision support tools integrating microbiome data with standard risk factors
   - Randomized trials evaluating whether microbiome-guided interventions improve pregnancy outcomes

## Scientific Contribution

This work provides **foundational evidence that vaginal microbiome-based PTB prediction is feasible in Mexican populations** with performance comparable to or exceeding international benchmarks despite substantially smaller sample size. The identification of population-specific risk factor patterns (anthropometric extremes, workplace factors) highlights the importance of locally-developed models rather than assuming universal applicability of models trained in non-Hispanic populations.

By demonstrating proof-of-concept for machine learning-based PTB prediction in a severely underrepresented population, this study:
1. **Addresses equity gaps** in precision medicine by extending microbiome research to Latin American contexts
2. **Establishes methodological standards** for small-sample microbiome-ML studies through rigorous nested CV and data leakage prevention
3. **Identifies research priorities** for Mexican PTB prediction, including larger validation studies and enhanced quality control
4. **Provides baseline performance estimates** against which future Mexican studies can be benchmarked

## Equity Implications

The absence of published machine learning models for PTB prediction in Mexican populations—despite Mexico's 10% PTB rate and 2 million annual births—exemplifies systemic inequities in precision medicine development. These inequities arise from:

1. **Research funding allocation** prioritizing high-income country populations
2. **Data infrastructure limitations** in middle-income countries
3. **Publication bias** favoring large, well-funded studies over exploratory work in underrepresented populations
4. **Generalization assumptions** that models developed in US/European populations will perform equivalently in Latin American contexts

Our findings challenge this assumption by demonstrating population-specific risk factor patterns, suggesting that **universal models may systematically underperform in populations excluded from training data**. This creates a precision medicine divide where advanced diagnostic/prognostic tools remain inaccessible to the populations that could benefit most.

**Policy recommendations:**
1. Funding agencies should prioritize research in underrepresented populations, including support for exploratory studies with modest sample sizes as foundational steps toward larger validation efforts
2. International collaborations should ensure equitable data sharing, with Mexican researchers retaining primary authority over Mexican cohort data rather than merely contributing data to US-led studies
3. Clinical implementation of microbiome-based testing should require validation in the target population, not just any population, to prevent systematic performance disparities

**Call to action:**
This work demonstrates that with appropriate methodological rigor, meaningful discoveries can emerge from small-sample studies in underrepresented populations. We call for prioritized investment in Latin American pregnancy cohorts to ensure that precision medicine advances benefit all populations equitably, not only those already well-represented in biomedical research.

---

**Acknowledgment:** This research was conducted with limited resources but rigorous methodology, demonstrating that resource constraints need not preclude scientifically valuable contributions. Future studies will benefit from expanded sample sizes, but the foundational work must begin somewhere—this study provides that foundation for Mexican PTB prediction research.


# Session Information

```{r session_info}
sessionInfo()
```

---

**End of Integrated Analysis**

